import logging
from collections.abc import AsyncIterable

from dotenv import load_dotenv

from livekit.agents import (
    Agent,
    AgentSession,
    JobContext,
    MetricsCollectedEvent,
    ModelSettings,
    WorkerOptions,
    cli,
    inference,
    llm,
    metrics,
)
from livekit.plugins import silero

logger = logging.getLogger("two-llm-example")
logger.setLevel(logging.INFO)

load_dotenv()

## This example shows how to flush a fast response in `llm_node` to tts immediately.
## The fast response is generated by a fast LLM for quick response.
## The main response is generated by a main LLM for detailed response.


class FastResponseAgent(Agent):
    def __init__(self) -> None:
        super().__init__(
            instructions="You are a helpful assistant.",
            llm="openai/gpt-4.1",
        )
        self.fast_llm = inference.LLM(model="openai/gpt-4.1-mini")
        self.fast_llm_prompt = llm.ChatMessage(
            role="system",
            content=[
                "Generate a short instant response to the user's message with 5 to 10 words.",
                "Do not answer the questions directly. For example, let me think about that, "
                "wait a moment, that's a good question, etc.",
            ],
        )

    async def llm_node(
        self,
        chat_ctx: llm.ChatContext,
        tools: list[llm.FunctionTool],
        model_settings: ModelSettings,
    ) -> AsyncIterable[llm.ChatChunk | llm.FlushSentinel]:
        # truncate the chat ctx with a fast response prompt
        fast_chat_ctx = chat_ctx.copy(
            exclude_function_call=True, exclude_instructions=True
        ).truncate(max_items=3)
        fast_chat_ctx.items.insert(0, self.fast_llm_prompt)

        quick_response = ""
        async with self.fast_llm.chat(chat_ctx=fast_chat_ctx) as stream:
            async for chunk in stream:
                yield chunk
                if chunk.delta and chunk.delta.content:
                    quick_response += chunk.delta.content

        if quick_response:
            yield "\n"
            logger.info(f"quick response: {quick_response}")

            # flush the quick response to tts immediately
            # NOTE: this will close the current tts_node and start a new one
            yield llm.FlushSentinel()

            # (Optional) add the quick response to the chat ctx for the main llm
            assert isinstance(self.llm, llm.LLM)
            chat_ctx.add_message(role="assistant", content=quick_response)

        # generate the response with the main llm
        async for chunk in Agent.default.llm_node(
            agent=self,
            chat_ctx=chat_ctx,
            tools=tools,
            model_settings=model_settings,
        ):
            yield chunk


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    session = AgentSession(
        vad=silero.VAD.load(),
        stt="assemblyai/universal-streaming",
        tts="elevenlabs",
    )

    @session.on("metrics_collected")
    def _on_metrics_collected(ev: MetricsCollectedEvent):
        metrics.log_metrics(ev.metrics)

    await session.start(agent=FastResponseAgent(), room=ctx.room)


if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
