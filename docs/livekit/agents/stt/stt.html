<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>livekit.agents.stt.stt API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>livekit.agents.stt.stt</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="livekit.agents.stt.stt.RecognitionUsage"><code class="flex name class">
<span>class <span class="ident">RecognitionUsage</span></span>
<span>(</span><span>audio_duration: float)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class RecognitionUsage:
    audio_duration: float</code></pre>
</details>
<div class="desc"><p>RecognitionUsage(audio_duration: 'float')</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="livekit.agents.stt.stt.RecognitionUsage.audio_duration"><code class="name">var <span class="ident">audio_duration</span> : float</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
</dl>
</dd>
<dt id="livekit.agents.stt.stt.RecognizeStream"><code class="flex name class">
<span>class <span class="ident">RecognizeStream</span></span>
<span>(</span><span>*,<br>stt: <a title="livekit.agents.stt.stt.STT" href="#livekit.agents.stt.stt.STT">STT</a>,<br>conn_options: APIConnectOptions,<br>sample_rate: NotGivenOr[int] = NOT_GIVEN)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RecognizeStream(ABC):
    class _FlushSentinel:
        &#34;&#34;&#34;Sentinel to mark when it was flushed&#34;&#34;&#34;

        pass

    def __init__(
        self,
        *,
        stt: STT,
        conn_options: APIConnectOptions,
        sample_rate: NotGivenOr[int] = NOT_GIVEN,
    ):
        &#34;&#34;&#34;
        Args:
        sample_rate : int or None, optional
            The desired sample rate for the audio input.
            If specified, the audio input will be automatically resampled to match
            the given sample rate before being processed for Speech-to-Text.
            If not provided (None), the input will retain its original sample rate.
        &#34;&#34;&#34;
        self._stt = stt
        self._conn_options = conn_options
        self._input_ch = aio.Chan[Union[rtc.AudioFrame, RecognizeStream._FlushSentinel]]()
        self._event_ch = aio.Chan[SpeechEvent]()

        self._event_aiter, monitor_aiter = aio.itertools.tee(self._event_ch, 2)
        self._metrics_task = asyncio.create_task(
            self._metrics_monitor_task(monitor_aiter), name=&#34;STT._metrics_task&#34;
        )

        self._task = asyncio.create_task(self._main_task())
        self._task.add_done_callback(lambda _: self._event_ch.close())

        self._needed_sr = sample_rate if is_given(sample_rate) else None
        self._pushed_sr = 0
        self._resampler: rtc.AudioResampler | None = None

    @abstractmethod
    async def _run(self) -&gt; None: ...

    async def _main_task(self) -&gt; None:
        max_retries = self._conn_options.max_retry
        num_retries = 0

        while num_retries &lt;= max_retries:
            try:
                return await self._run()
            except APIError as e:
                if max_retries == 0:
                    self._emit_error(e, recoverable=False)
                    raise
                elif num_retries == max_retries:
                    self._emit_error(e, recoverable=False)
                    raise APIConnectionError(
                        f&#34;failed to recognize speech after {num_retries} attempts&#34;,
                    ) from e
                else:
                    self._emit_error(e, recoverable=True)

                    retry_interval = self._conn_options._interval_for_retry(num_retries)
                    logger.warning(
                        f&#34;failed to recognize speech, retrying in {retry_interval}s&#34;,
                        exc_info=e,
                        extra={
                            &#34;tts&#34;: self._stt._label,
                            &#34;attempt&#34;: num_retries,
                            &#34;streamed&#34;: True,
                        },
                    )
                    await asyncio.sleep(retry_interval)

                num_retries += 1

    def _emit_error(self, api_error: APIError, recoverable: bool):
        self._stt.emit(
            &#34;error&#34;,
            STTError(
                timestamp=time.time(),
                label=self._stt._label,
                error=api_error,
                recoverable=recoverable,
            ),
        )

    async def _metrics_monitor_task(self, event_aiter: AsyncIterable[SpeechEvent]) -&gt; None:
        &#34;&#34;&#34;Task used to collect metrics&#34;&#34;&#34;

        async for ev in event_aiter:
            if ev.type == SpeechEventType.RECOGNITION_USAGE:
                assert ev.recognition_usage is not None, (
                    &#34;recognition_usage must be provided for RECOGNITION_USAGE event&#34;
                )

                stt_metrics = STTMetrics(
                    request_id=ev.request_id,
                    timestamp=time.time(),
                    duration=0.0,
                    label=self._stt._label,
                    audio_duration=ev.recognition_usage.audio_duration,
                    streamed=True,
                )

                self._stt.emit(&#34;metrics_collected&#34;, stt_metrics)

    def push_frame(self, frame: rtc.AudioFrame) -&gt; None:
        &#34;&#34;&#34;Push audio to be recognized&#34;&#34;&#34;
        self._check_input_not_ended()
        self._check_not_closed()

        if self._pushed_sr and self._pushed_sr != frame.sample_rate:
            raise ValueError(&#34;the sample rate of the input frames must be consistent&#34;)

        self._pushed_sr = frame.sample_rate

        if self._needed_sr and self._needed_sr != frame.sample_rate:
            if not self._resampler:
                self._resampler = rtc.AudioResampler(
                    frame.sample_rate,
                    self._needed_sr,
                    quality=rtc.AudioResamplerQuality.HIGH,
                )

        if self._resampler:
            frames = self._resampler.push(frame)
            for frame in frames:
                self._input_ch.send_nowait(frame)
        else:
            self._input_ch.send_nowait(frame)

    def flush(self) -&gt; None:
        &#34;&#34;&#34;Mark the end of the current segment&#34;&#34;&#34;
        self._check_input_not_ended()
        self._check_not_closed()

        if self._resampler:
            for frame in self._resampler.flush():
                self._input_ch.send_nowait(frame)

        self._input_ch.send_nowait(self._FlushSentinel())

    def end_input(self) -&gt; None:
        &#34;&#34;&#34;Mark the end of input, no more audio will be pushed&#34;&#34;&#34;
        self.flush()
        self._input_ch.close()

    async def aclose(self) -&gt; None:
        &#34;&#34;&#34;Close ths stream immediately&#34;&#34;&#34;
        self._input_ch.close()
        await aio.cancel_and_wait(self._task)

        if self._metrics_task is not None:
            await self._metrics_task

    async def __anext__(self) -&gt; SpeechEvent:
        try:
            val = await self._event_aiter.__anext__()
        except StopAsyncIteration:
            if not self._task.cancelled() and (exc := self._task.exception()):
                raise exc  # noqa: B904

            raise StopAsyncIteration from None

        return val

    def __aiter__(self) -&gt; AsyncIterator[SpeechEvent]:
        return self

    def _check_not_closed(self) -&gt; None:
        if self._event_ch.closed:
            cls = type(self)
            raise RuntimeError(f&#34;{cls.__module__}.{cls.__name__} is closed&#34;)

    def _check_input_not_ended(self) -&gt; None:
        if self._input_ch.closed:
            cls = type(self)
            raise RuntimeError(f&#34;{cls.__module__}.{cls.__name__} input ended&#34;)

    async def __aenter__(self) -&gt; RecognizeStream:
        return self

    async def __aexit__(
        self,
        exc_type: type[BaseException] | None,
        exc: BaseException | None,
        exc_tb: TracebackType | None,
    ) -&gt; None:
        await self.aclose()</code></pre>
</details>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Args:
sample_rate : int or None, optional
The desired sample rate for the audio input.
If specified, the audio input will be automatically resampled to match
the given sample rate before being processed for Speech-to-Text.
If not provided (None), the input will retain its original sample rate.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="livekit.agents.stt.fallback_adapter.FallbackRecognizeStream" href="fallback_adapter.html#livekit.agents.stt.fallback_adapter.FallbackRecognizeStream">FallbackRecognizeStream</a></li>
<li><a title="livekit.agents.stt.stream_adapter.StreamAdapterWrapper" href="stream_adapter.html#livekit.agents.stt.stream_adapter.StreamAdapterWrapper">StreamAdapterWrapper</a></li>
<li><a title="livekit.plugins.assemblyai.stt.SpeechStream" href="../../plugins/assemblyai/stt.html#livekit.plugins.assemblyai.stt.SpeechStream">SpeechStream</a></li>
<li><a title="livekit.plugins.aws.stt.SpeechStream" href="../../plugins/aws/stt.html#livekit.plugins.aws.stt.SpeechStream">SpeechStream</a></li>
<li>livekit.plugins.azure.stt.SpeechStream</li>
<li>livekit.plugins.deepgram.stt.SpeechStream</li>
<li>livekit.plugins.gladia.stt.SpeechStream</li>
<li>livekit.plugins.google.stt.SpeechStream</li>
<li>livekit.plugins.openai.stt.SpeechStream</li>
<li><a title="livekit.plugins.speechmatics.stt.SpeechStream" href="../../plugins/speechmatics/stt.html#livekit.plugins.speechmatics.stt.SpeechStream">SpeechStream</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="livekit.agents.stt.stt.RecognizeStream.aclose"><code class="name flex">
<span>async def <span class="ident">aclose</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def aclose(self) -&gt; None:
    &#34;&#34;&#34;Close ths stream immediately&#34;&#34;&#34;
    self._input_ch.close()
    await aio.cancel_and_wait(self._task)

    if self._metrics_task is not None:
        await self._metrics_task</code></pre>
</details>
<div class="desc"><p>Close ths stream immediately</p></div>
</dd>
<dt id="livekit.agents.stt.stt.RecognizeStream.end_input"><code class="name flex">
<span>def <span class="ident">end_input</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def end_input(self) -&gt; None:
    &#34;&#34;&#34;Mark the end of input, no more audio will be pushed&#34;&#34;&#34;
    self.flush()
    self._input_ch.close()</code></pre>
</details>
<div class="desc"><p>Mark the end of input, no more audio will be pushed</p></div>
</dd>
<dt id="livekit.agents.stt.stt.RecognizeStream.flush"><code class="name flex">
<span>def <span class="ident">flush</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flush(self) -&gt; None:
    &#34;&#34;&#34;Mark the end of the current segment&#34;&#34;&#34;
    self._check_input_not_ended()
    self._check_not_closed()

    if self._resampler:
        for frame in self._resampler.flush():
            self._input_ch.send_nowait(frame)

    self._input_ch.send_nowait(self._FlushSentinel())</code></pre>
</details>
<div class="desc"><p>Mark the end of the current segment</p></div>
</dd>
<dt id="livekit.agents.stt.stt.RecognizeStream.push_frame"><code class="name flex">
<span>def <span class="ident">push_frame</span></span>(<span>self, frame: rtc.AudioFrame) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def push_frame(self, frame: rtc.AudioFrame) -&gt; None:
    &#34;&#34;&#34;Push audio to be recognized&#34;&#34;&#34;
    self._check_input_not_ended()
    self._check_not_closed()

    if self._pushed_sr and self._pushed_sr != frame.sample_rate:
        raise ValueError(&#34;the sample rate of the input frames must be consistent&#34;)

    self._pushed_sr = frame.sample_rate

    if self._needed_sr and self._needed_sr != frame.sample_rate:
        if not self._resampler:
            self._resampler = rtc.AudioResampler(
                frame.sample_rate,
                self._needed_sr,
                quality=rtc.AudioResamplerQuality.HIGH,
            )

    if self._resampler:
        frames = self._resampler.push(frame)
        for frame in frames:
            self._input_ch.send_nowait(frame)
    else:
        self._input_ch.send_nowait(frame)</code></pre>
</details>
<div class="desc"><p>Push audio to be recognized</p></div>
</dd>
</dl>
</dd>
<dt id="livekit.agents.stt.stt.RecognizeStream"><code class="flex name class">
<span>class <span class="ident">SpeechStream</span></span>
<span>(</span><span>*,<br>stt: <a title="livekit.agents.stt.stt.STT" href="#livekit.agents.stt.stt.STT">STT</a>,<br>conn_options: APIConnectOptions,<br>sample_rate: NotGivenOr[int] = NOT_GIVEN)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RecognizeStream(ABC):
    class _FlushSentinel:
        &#34;&#34;&#34;Sentinel to mark when it was flushed&#34;&#34;&#34;

        pass

    def __init__(
        self,
        *,
        stt: STT,
        conn_options: APIConnectOptions,
        sample_rate: NotGivenOr[int] = NOT_GIVEN,
    ):
        &#34;&#34;&#34;
        Args:
        sample_rate : int or None, optional
            The desired sample rate for the audio input.
            If specified, the audio input will be automatically resampled to match
            the given sample rate before being processed for Speech-to-Text.
            If not provided (None), the input will retain its original sample rate.
        &#34;&#34;&#34;
        self._stt = stt
        self._conn_options = conn_options
        self._input_ch = aio.Chan[Union[rtc.AudioFrame, RecognizeStream._FlushSentinel]]()
        self._event_ch = aio.Chan[SpeechEvent]()

        self._event_aiter, monitor_aiter = aio.itertools.tee(self._event_ch, 2)
        self._metrics_task = asyncio.create_task(
            self._metrics_monitor_task(monitor_aiter), name=&#34;STT._metrics_task&#34;
        )

        self._task = asyncio.create_task(self._main_task())
        self._task.add_done_callback(lambda _: self._event_ch.close())

        self._needed_sr = sample_rate if is_given(sample_rate) else None
        self._pushed_sr = 0
        self._resampler: rtc.AudioResampler | None = None

    @abstractmethod
    async def _run(self) -&gt; None: ...

    async def _main_task(self) -&gt; None:
        max_retries = self._conn_options.max_retry
        num_retries = 0

        while num_retries &lt;= max_retries:
            try:
                return await self._run()
            except APIError as e:
                if max_retries == 0:
                    self._emit_error(e, recoverable=False)
                    raise
                elif num_retries == max_retries:
                    self._emit_error(e, recoverable=False)
                    raise APIConnectionError(
                        f&#34;failed to recognize speech after {num_retries} attempts&#34;,
                    ) from e
                else:
                    self._emit_error(e, recoverable=True)

                    retry_interval = self._conn_options._interval_for_retry(num_retries)
                    logger.warning(
                        f&#34;failed to recognize speech, retrying in {retry_interval}s&#34;,
                        exc_info=e,
                        extra={
                            &#34;tts&#34;: self._stt._label,
                            &#34;attempt&#34;: num_retries,
                            &#34;streamed&#34;: True,
                        },
                    )
                    await asyncio.sleep(retry_interval)

                num_retries += 1

    def _emit_error(self, api_error: APIError, recoverable: bool):
        self._stt.emit(
            &#34;error&#34;,
            STTError(
                timestamp=time.time(),
                label=self._stt._label,
                error=api_error,
                recoverable=recoverable,
            ),
        )

    async def _metrics_monitor_task(self, event_aiter: AsyncIterable[SpeechEvent]) -&gt; None:
        &#34;&#34;&#34;Task used to collect metrics&#34;&#34;&#34;

        async for ev in event_aiter:
            if ev.type == SpeechEventType.RECOGNITION_USAGE:
                assert ev.recognition_usage is not None, (
                    &#34;recognition_usage must be provided for RECOGNITION_USAGE event&#34;
                )

                stt_metrics = STTMetrics(
                    request_id=ev.request_id,
                    timestamp=time.time(),
                    duration=0.0,
                    label=self._stt._label,
                    audio_duration=ev.recognition_usage.audio_duration,
                    streamed=True,
                )

                self._stt.emit(&#34;metrics_collected&#34;, stt_metrics)

    def push_frame(self, frame: rtc.AudioFrame) -&gt; None:
        &#34;&#34;&#34;Push audio to be recognized&#34;&#34;&#34;
        self._check_input_not_ended()
        self._check_not_closed()

        if self._pushed_sr and self._pushed_sr != frame.sample_rate:
            raise ValueError(&#34;the sample rate of the input frames must be consistent&#34;)

        self._pushed_sr = frame.sample_rate

        if self._needed_sr and self._needed_sr != frame.sample_rate:
            if not self._resampler:
                self._resampler = rtc.AudioResampler(
                    frame.sample_rate,
                    self._needed_sr,
                    quality=rtc.AudioResamplerQuality.HIGH,
                )

        if self._resampler:
            frames = self._resampler.push(frame)
            for frame in frames:
                self._input_ch.send_nowait(frame)
        else:
            self._input_ch.send_nowait(frame)

    def flush(self) -&gt; None:
        &#34;&#34;&#34;Mark the end of the current segment&#34;&#34;&#34;
        self._check_input_not_ended()
        self._check_not_closed()

        if self._resampler:
            for frame in self._resampler.flush():
                self._input_ch.send_nowait(frame)

        self._input_ch.send_nowait(self._FlushSentinel())

    def end_input(self) -&gt; None:
        &#34;&#34;&#34;Mark the end of input, no more audio will be pushed&#34;&#34;&#34;
        self.flush()
        self._input_ch.close()

    async def aclose(self) -&gt; None:
        &#34;&#34;&#34;Close ths stream immediately&#34;&#34;&#34;
        self._input_ch.close()
        await aio.cancel_and_wait(self._task)

        if self._metrics_task is not None:
            await self._metrics_task

    async def __anext__(self) -&gt; SpeechEvent:
        try:
            val = await self._event_aiter.__anext__()
        except StopAsyncIteration:
            if not self._task.cancelled() and (exc := self._task.exception()):
                raise exc  # noqa: B904

            raise StopAsyncIteration from None

        return val

    def __aiter__(self) -&gt; AsyncIterator[SpeechEvent]:
        return self

    def _check_not_closed(self) -&gt; None:
        if self._event_ch.closed:
            cls = type(self)
            raise RuntimeError(f&#34;{cls.__module__}.{cls.__name__} is closed&#34;)

    def _check_input_not_ended(self) -&gt; None:
        if self._input_ch.closed:
            cls = type(self)
            raise RuntimeError(f&#34;{cls.__module__}.{cls.__name__} input ended&#34;)

    async def __aenter__(self) -&gt; RecognizeStream:
        return self

    async def __aexit__(
        self,
        exc_type: type[BaseException] | None,
        exc: BaseException | None,
        exc_tb: TracebackType | None,
    ) -&gt; None:
        await self.aclose()</code></pre>
</details>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Args:
sample_rate : int or None, optional
The desired sample rate for the audio input.
If specified, the audio input will be automatically resampled to match
the given sample rate before being processed for Speech-to-Text.
If not provided (None), the input will retain its original sample rate.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="livekit.agents.stt.fallback_adapter.FallbackRecognizeStream" href="fallback_adapter.html#livekit.agents.stt.fallback_adapter.FallbackRecognizeStream">FallbackRecognizeStream</a></li>
<li><a title="livekit.agents.stt.stream_adapter.StreamAdapterWrapper" href="stream_adapter.html#livekit.agents.stt.stream_adapter.StreamAdapterWrapper">StreamAdapterWrapper</a></li>
<li><a title="livekit.plugins.assemblyai.stt.SpeechStream" href="../../plugins/assemblyai/stt.html#livekit.plugins.assemblyai.stt.SpeechStream">SpeechStream</a></li>
<li><a title="livekit.plugins.aws.stt.SpeechStream" href="../../plugins/aws/stt.html#livekit.plugins.aws.stt.SpeechStream">SpeechStream</a></li>
<li>livekit.plugins.azure.stt.SpeechStream</li>
<li>livekit.plugins.deepgram.stt.SpeechStream</li>
<li>livekit.plugins.gladia.stt.SpeechStream</li>
<li>livekit.plugins.google.stt.SpeechStream</li>
<li>livekit.plugins.openai.stt.SpeechStream</li>
<li><a title="livekit.plugins.speechmatics.stt.SpeechStream" href="../../plugins/speechmatics/stt.html#livekit.plugins.speechmatics.stt.SpeechStream">SpeechStream</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="livekit.agents.stt.stt.RecognizeStream.aclose"><code class="name flex">
<span>async def <span class="ident">aclose</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def aclose(self) -&gt; None:
    &#34;&#34;&#34;Close ths stream immediately&#34;&#34;&#34;
    self._input_ch.close()
    await aio.cancel_and_wait(self._task)

    if self._metrics_task is not None:
        await self._metrics_task</code></pre>
</details>
<div class="desc"><p>Close ths stream immediately</p></div>
</dd>
<dt id="livekit.agents.stt.stt.RecognizeStream.end_input"><code class="name flex">
<span>def <span class="ident">end_input</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def end_input(self) -&gt; None:
    &#34;&#34;&#34;Mark the end of input, no more audio will be pushed&#34;&#34;&#34;
    self.flush()
    self._input_ch.close()</code></pre>
</details>
<div class="desc"><p>Mark the end of input, no more audio will be pushed</p></div>
</dd>
<dt id="livekit.agents.stt.stt.RecognizeStream.flush"><code class="name flex">
<span>def <span class="ident">flush</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flush(self) -&gt; None:
    &#34;&#34;&#34;Mark the end of the current segment&#34;&#34;&#34;
    self._check_input_not_ended()
    self._check_not_closed()

    if self._resampler:
        for frame in self._resampler.flush():
            self._input_ch.send_nowait(frame)

    self._input_ch.send_nowait(self._FlushSentinel())</code></pre>
</details>
<div class="desc"><p>Mark the end of the current segment</p></div>
</dd>
<dt id="livekit.agents.stt.stt.RecognizeStream.push_frame"><code class="name flex">
<span>def <span class="ident">push_frame</span></span>(<span>self, frame: rtc.AudioFrame) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def push_frame(self, frame: rtc.AudioFrame) -&gt; None:
    &#34;&#34;&#34;Push audio to be recognized&#34;&#34;&#34;
    self._check_input_not_ended()
    self._check_not_closed()

    if self._pushed_sr and self._pushed_sr != frame.sample_rate:
        raise ValueError(&#34;the sample rate of the input frames must be consistent&#34;)

    self._pushed_sr = frame.sample_rate

    if self._needed_sr and self._needed_sr != frame.sample_rate:
        if not self._resampler:
            self._resampler = rtc.AudioResampler(
                frame.sample_rate,
                self._needed_sr,
                quality=rtc.AudioResamplerQuality.HIGH,
            )

    if self._resampler:
        frames = self._resampler.push(frame)
        for frame in frames:
            self._input_ch.send_nowait(frame)
    else:
        self._input_ch.send_nowait(frame)</code></pre>
</details>
<div class="desc"><p>Push audio to be recognized</p></div>
</dd>
</dl>
</dd>
<dt id="livekit.agents.stt.stt.STT"><code class="flex name class">
<span>class <span class="ident">STT</span></span>
<span>(</span><span>*,<br>capabilities: <a title="livekit.agents.stt.stt.STTCapabilities" href="#livekit.agents.stt.stt.STTCapabilities">STTCapabilities</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class STT(
    ABC,
    rtc.EventEmitter[Union[Literal[&#34;metrics_collected&#34;, &#34;error&#34;], TEvent]],
    Generic[TEvent],
):
    def __init__(self, *, capabilities: STTCapabilities) -&gt; None:
        super().__init__()
        self._capabilities = capabilities
        self._label = f&#34;{type(self).__module__}.{type(self).__name__}&#34;

    @property
    def label(self) -&gt; str:
        return self._label

    @property
    def capabilities(self) -&gt; STTCapabilities:
        return self._capabilities

    @abstractmethod
    async def _recognize_impl(
        self,
        buffer: AudioBuffer,
        *,
        language: NotGivenOr[str] = NOT_GIVEN,
        conn_options: APIConnectOptions,
    ) -&gt; SpeechEvent: ...

    async def recognize(
        self,
        buffer: AudioBuffer,
        *,
        language: NotGivenOr[str | None] = NOT_GIVEN,
        conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
    ) -&gt; SpeechEvent:
        for i in range(conn_options.max_retry + 1):
            try:
                start_time = time.perf_counter()
                event = await self._recognize_impl(
                    buffer, language=language, conn_options=conn_options
                )
                duration = time.perf_counter() - start_time
                stt_metrics = STTMetrics(
                    request_id=event.request_id,
                    timestamp=time.time(),
                    duration=duration,
                    label=self._label,
                    audio_duration=calculate_audio_duration(buffer),
                    streamed=False,
                )
                self.emit(&#34;metrics_collected&#34;, stt_metrics)
                return event

            except APIError as e:
                retry_interval = conn_options._interval_for_retry(i)
                if conn_options.max_retry == 0:
                    raise
                elif i == conn_options.max_retry:
                    raise APIConnectionError(
                        f&#34;failed to recognize speech after {conn_options.max_retry + 1} attempts&#34;,
                    ) from e
                else:
                    logger.warning(
                        f&#34;failed to recognize speech, retrying in {retry_interval}s&#34;,
                        exc_info=e,
                        extra={
                            &#34;tts&#34;: self._label,
                            &#34;attempt&#34;: i + 1,
                            &#34;streamed&#34;: False,
                        },
                    )

                await asyncio.sleep(retry_interval)

        raise RuntimeError(&#34;unreachable&#34;)

    def stream(
        self,
        *,
        language: NotGivenOr[str | None] = NOT_GIVEN,
        conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
    ) -&gt; RecognizeStream:
        raise NotImplementedError(
            &#34;streaming is not supported by this STT, please use a different STT or use a StreamAdapter&#34;  # noqa: E501
        )

    async def aclose(self) -&gt; None:
        &#34;&#34;&#34;Close the STT, and every stream/requests associated with it&#34;&#34;&#34;
        ...

    async def __aenter__(self) -&gt; STT:
        return self

    async def __aexit__(
        self,
        exc_type: type[BaseException] | None,
        exc: BaseException | None,
        exc_tb: TracebackType | None,
    ) -&gt; None:
        await self.aclose()</code></pre>
</details>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
<li><a title="livekit.rtc.event_emitter.EventEmitter" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter">EventEmitter</a></li>
<li>typing.Generic</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="livekit.agents.stt.fallback_adapter.FallbackAdapter" href="fallback_adapter.html#livekit.agents.stt.fallback_adapter.FallbackAdapter">FallbackAdapter</a></li>
<li><a title="livekit.agents.stt.stream_adapter.StreamAdapter" href="stream_adapter.html#livekit.agents.stt.stream_adapter.StreamAdapter">StreamAdapter</a></li>
<li><a title="livekit.plugins.assemblyai.stt.STT" href="../../plugins/assemblyai/stt.html#livekit.plugins.assemblyai.stt.STT">STT</a></li>
<li><a title="livekit.plugins.aws.stt.STT" href="../../plugins/aws/stt.html#livekit.plugins.aws.stt.STT">STT</a></li>
<li>livekit.plugins.azure.stt.STT</li>
<li>livekit.plugins.clova.stt.STT</li>
<li>livekit.plugins.deepgram.stt.STT</li>
<li><a title="livekit.plugins.fal.stt.WizperSTT" href="../../plugins/fal/stt.html#livekit.plugins.fal.stt.WizperSTT">WizperSTT</a></li>
<li>livekit.plugins.gladia.stt.STT</li>
<li>livekit.plugins.google.stt.STT</li>
<li>livekit.plugins.openai.stt.STT</li>
<li><a title="livekit.plugins.speechmatics.stt.STT" href="../../plugins/speechmatics/stt.html#livekit.plugins.speechmatics.stt.STT">STT</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="livekit.agents.stt.stt.STT.capabilities"><code class="name">prop <span class="ident">capabilities</span> : <a title="livekit.agents.stt.stt.STTCapabilities" href="#livekit.agents.stt.stt.STTCapabilities">STTCapabilities</a></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def capabilities(self) -&gt; STTCapabilities:
    return self._capabilities</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.agents.stt.stt.STT.label"><code class="name">prop <span class="ident">label</span> : str</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def label(self) -&gt; str:
    return self._label</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="livekit.agents.stt.stt.STT.aclose"><code class="name flex">
<span>async def <span class="ident">aclose</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def aclose(self) -&gt; None:
    &#34;&#34;&#34;Close the STT, and every stream/requests associated with it&#34;&#34;&#34;
    ...</code></pre>
</details>
<div class="desc"><p>Close the STT, and every stream/requests associated with it</p></div>
</dd>
<dt id="livekit.agents.stt.stt.STT.recognize"><code class="name flex">
<span>async def <span class="ident">recognize</span></span>(<span>self,<br>buffer: AudioBuffer,<br>*,<br>language: NotGivenOr[str | None] = NOT_GIVEN,<br>conn_options: APIConnectOptions = APIConnectOptions(max_retry=3, retry_interval=2.0, timeout=10.0)) ‑> <a title="livekit.agents.stt.stt.SpeechEvent" href="#livekit.agents.stt.stt.SpeechEvent">SpeechEvent</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def recognize(
    self,
    buffer: AudioBuffer,
    *,
    language: NotGivenOr[str | None] = NOT_GIVEN,
    conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
) -&gt; SpeechEvent:
    for i in range(conn_options.max_retry + 1):
        try:
            start_time = time.perf_counter()
            event = await self._recognize_impl(
                buffer, language=language, conn_options=conn_options
            )
            duration = time.perf_counter() - start_time
            stt_metrics = STTMetrics(
                request_id=event.request_id,
                timestamp=time.time(),
                duration=duration,
                label=self._label,
                audio_duration=calculate_audio_duration(buffer),
                streamed=False,
            )
            self.emit(&#34;metrics_collected&#34;, stt_metrics)
            return event

        except APIError as e:
            retry_interval = conn_options._interval_for_retry(i)
            if conn_options.max_retry == 0:
                raise
            elif i == conn_options.max_retry:
                raise APIConnectionError(
                    f&#34;failed to recognize speech after {conn_options.max_retry + 1} attempts&#34;,
                ) from e
            else:
                logger.warning(
                    f&#34;failed to recognize speech, retrying in {retry_interval}s&#34;,
                    exc_info=e,
                    extra={
                        &#34;tts&#34;: self._label,
                        &#34;attempt&#34;: i + 1,
                        &#34;streamed&#34;: False,
                    },
                )

            await asyncio.sleep(retry_interval)

    raise RuntimeError(&#34;unreachable&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.agents.stt.stt.STT.stream"><code class="name flex">
<span>def <span class="ident">stream</span></span>(<span>self,<br>*,<br>language: NotGivenOr[str | None] = NOT_GIVEN,<br>conn_options: APIConnectOptions = APIConnectOptions(max_retry=3, retry_interval=2.0, timeout=10.0)) ‑> <a title="livekit.agents.stt.stt.RecognizeStream" href="#livekit.agents.stt.stt.RecognizeStream">RecognizeStream</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stream(
    self,
    *,
    language: NotGivenOr[str | None] = NOT_GIVEN,
    conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
) -&gt; RecognizeStream:
    raise NotImplementedError(
        &#34;streaming is not supported by this STT, please use a different STT or use a StreamAdapter&#34;  # noqa: E501
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="livekit.rtc.event_emitter.EventEmitter" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter">EventEmitter</a></b></code>:
<ul class="hlist">
<li><code><a title="livekit.rtc.event_emitter.EventEmitter.emit" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.emit">emit</a></code></li>
<li><code><a title="livekit.rtc.event_emitter.EventEmitter.off" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.off">off</a></code></li>
<li><code><a title="livekit.rtc.event_emitter.EventEmitter.on" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.on">on</a></code></li>
<li><code><a title="livekit.rtc.event_emitter.EventEmitter.once" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.once">once</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="livekit.agents.stt.stt.STTCapabilities"><code class="flex name class">
<span>class <span class="ident">STTCapabilities</span></span>
<span>(</span><span>streaming: bool, interim_results: bool)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class STTCapabilities:
    streaming: bool
    interim_results: bool</code></pre>
</details>
<div class="desc"><p>STTCapabilities(streaming: 'bool', interim_results: 'bool')</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="livekit.agents.stt.stt.STTCapabilities.interim_results"><code class="name">var <span class="ident">interim_results</span> : bool</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="livekit.agents.stt.stt.STTCapabilities.streaming"><code class="name">var <span class="ident">streaming</span> : bool</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
</dl>
</dd>
<dt id="livekit.agents.stt.stt.STTError"><code class="flex name class">
<span>class <span class="ident">STTError</span></span>
<span>(</span><span>**data: Any)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class STTError(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    type: Literal[&#34;stt_error&#34;] = &#34;stt_error&#34;
    timestamp: float
    label: str
    error: APIError = Field(..., exclude=True)
    recoverable: bool</code></pre>
</details>
<div class="desc"><p>Usage docs: <a href="https://docs.pydantic.dev/2.10/concepts/models/">https://docs.pydantic.dev/2.10/concepts/models/</a></p>
<p>A base class for creating Pydantic models.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>__class_vars__</code></strong></dt>
<dd>The names of the class variables defined on the model.</dd>
<dt><strong><code>__private_attributes__</code></strong></dt>
<dd>Metadata about the private attributes of the model.</dd>
<dt><strong><code>__signature__</code></strong></dt>
<dd>The synthesized <code>__init__</code> [<code>Signature</code>][inspect.Signature] of the model.</dd>
<dt><strong><code>__pydantic_complete__</code></strong></dt>
<dd>Whether model building is completed, or if there are still undefined fields.</dd>
<dt><strong><code>__pydantic_core_schema__</code></strong></dt>
<dd>The core schema of the model.</dd>
<dt><strong><code>__pydantic_custom_init__</code></strong></dt>
<dd>Whether the model has a custom <code>__init__</code> function.</dd>
<dt><strong><code>__pydantic_decorators__</code></strong></dt>
<dd>Metadata containing the decorators defined on the model.
This replaces <code>Model.__validators__</code> and <code>Model.__root_validators__</code> from Pydantic V1.</dd>
<dt><strong><code>__pydantic_generic_metadata__</code></strong></dt>
<dd>Metadata for generic models; contains data used for a similar purpose to
<strong>args</strong>, <strong>origin</strong>, <strong>parameters</strong> in typing-module generics. May eventually be replaced by these.</dd>
<dt><strong><code>__pydantic_parent_namespace__</code></strong></dt>
<dd>Parent namespace of the model, used for automatic rebuilding of models.</dd>
<dt><strong><code>__pydantic_post_init__</code></strong></dt>
<dd>The name of the post-init method for the model, if defined.</dd>
<dt><strong><code>__pydantic_root_model__</code></strong></dt>
<dd>Whether the model is a [<code>RootModel</code>][pydantic.root_model.RootModel].</dd>
<dt><strong><code>__pydantic_serializer__</code></strong></dt>
<dd>The <code>pydantic-core</code> <code>SchemaSerializer</code> used to dump instances of the model.</dd>
<dt><strong><code>__pydantic_validator__</code></strong></dt>
<dd>The <code>pydantic-core</code> <code>SchemaValidator</code> used to validate instances of the model.</dd>
<dt><strong><code>__pydantic_fields__</code></strong></dt>
<dd>A dictionary of field names and their corresponding [<code>FieldInfo</code>][pydantic.fields.FieldInfo] objects.</dd>
<dt><strong><code>__pydantic_computed_fields__</code></strong></dt>
<dd>A dictionary of computed field names and their corresponding [<code>ComputedFieldInfo</code>][pydantic.fields.ComputedFieldInfo] objects.</dd>
<dt><strong><code>__pydantic_extra__</code></strong></dt>
<dd>A dictionary containing extra values, if [<code>extra</code>][pydantic.config.ConfigDict.extra]
is set to <code>'allow'</code>.</dd>
<dt><strong><code>__pydantic_fields_set__</code></strong></dt>
<dd>The names of fields explicitly set during instantiation.</dd>
<dt><strong><code>__pydantic_private__</code></strong></dt>
<dd>Values of private attributes set on the model instance.</dd>
</dl>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises [<code>ValidationError</code>][pydantic_core.ValidationError] if the input data cannot be
validated to form a valid model.</p>
<p><code>self</code> is explicitly positional-only to allow <code>self</code> as a field name.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pydantic.main.BaseModel</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="livekit.agents.stt.stt.STTError.error"><code class="name">var <span class="ident">error</span> : livekit.agents._exceptions.APIError</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="livekit.agents.stt.stt.STTError.label"><code class="name">var <span class="ident">label</span> : str</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="livekit.agents.stt.stt.STTError.model_config"><code class="name">var <span class="ident">model_config</span></code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="livekit.agents.stt.stt.STTError.recoverable"><code class="name">var <span class="ident">recoverable</span> : bool</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="livekit.agents.stt.stt.STTError.timestamp"><code class="name">var <span class="ident">timestamp</span> : float</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="livekit.agents.stt.stt.STTError.type"><code class="name">var <span class="ident">type</span> : Literal['stt_error']</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
</dl>
</dd>
<dt id="livekit.agents.stt.stt.SpeechData"><code class="flex name class">
<span>class <span class="ident">SpeechData</span></span>
<span>(</span><span>language: str,<br>text: str,<br>start_time: float = 0.0,<br>end_time: float = 0.0,<br>confidence: float = 0.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class SpeechData:
    language: str
    text: str
    start_time: float = 0.0
    end_time: float = 0.0
    confidence: float = 0.0  # [0, 1]</code></pre>
</details>
<div class="desc"><p>SpeechData(language: 'str', text: 'str', start_time: 'float' = 0.0, end_time: 'float' = 0.0, confidence: 'float' = 0.0)</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="livekit.agents.stt.stt.SpeechData.confidence"><code class="name">var <span class="ident">confidence</span> : float</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="livekit.agents.stt.stt.SpeechData.end_time"><code class="name">var <span class="ident">end_time</span> : float</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="livekit.agents.stt.stt.SpeechData.language"><code class="name">var <span class="ident">language</span> : str</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="livekit.agents.stt.stt.SpeechData.start_time"><code class="name">var <span class="ident">start_time</span> : float</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="livekit.agents.stt.stt.SpeechData.text"><code class="name">var <span class="ident">text</span> : str</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
</dl>
</dd>
<dt id="livekit.agents.stt.stt.SpeechEvent"><code class="flex name class">
<span>class <span class="ident">SpeechEvent</span></span>
<span>(</span><span>type: <a title="livekit.agents.stt.stt.SpeechEventType" href="#livekit.agents.stt.stt.SpeechEventType">SpeechEventType</a>,<br>request_id: str = '',<br>alternatives: list[<a title="livekit.agents.stt.stt.SpeechData" href="#livekit.agents.stt.stt.SpeechData">SpeechData</a>] = &lt;factory&gt;,<br>recognition_usage: <a title="livekit.agents.stt.stt.RecognitionUsage" href="#livekit.agents.stt.stt.RecognitionUsage">RecognitionUsage</a> | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class SpeechEvent:
    type: SpeechEventType
    request_id: str = &#34;&#34;
    alternatives: list[SpeechData] = field(default_factory=list)
    recognition_usage: RecognitionUsage | None = None</code></pre>
</details>
<div class="desc"><p>SpeechEvent(type: 'SpeechEventType', request_id: 'str' = '', alternatives: 'list[SpeechData]' = <factory>, recognition_usage: 'RecognitionUsage | None' = None)</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="livekit.agents.stt.stt.SpeechEvent.alternatives"><code class="name">var <span class="ident">alternatives</span> : list[<a title="livekit.agents.stt.stt.SpeechData" href="#livekit.agents.stt.stt.SpeechData">SpeechData</a>]</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="livekit.agents.stt.stt.SpeechEvent.recognition_usage"><code class="name">var <span class="ident">recognition_usage</span> : <a title="livekit.agents.stt.stt.RecognitionUsage" href="#livekit.agents.stt.stt.RecognitionUsage">RecognitionUsage</a> | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="livekit.agents.stt.stt.SpeechEvent.request_id"><code class="name">var <span class="ident">request_id</span> : str</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="livekit.agents.stt.stt.SpeechEvent.type"><code class="name">var <span class="ident">type</span> : <a title="livekit.agents.stt.stt.SpeechEventType" href="#livekit.agents.stt.stt.SpeechEventType">SpeechEventType</a></code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
</dl>
</dd>
<dt id="livekit.agents.stt.stt.SpeechEventType"><code class="flex name class">
<span>class <span class="ident">SpeechEventType</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@unique
class SpeechEventType(str, Enum):
    START_OF_SPEECH = &#34;start_of_speech&#34;
    &#34;&#34;&#34;indicate the start of speech
    if the STT doesn&#39;t support this event, this will be emitted as the same time as the first INTERIM_TRANSCRIPT&#34;&#34;&#34;  # noqa: E501
    INTERIM_TRANSCRIPT = &#34;interim_transcript&#34;
    &#34;&#34;&#34;interim transcript, useful for real-time transcription&#34;&#34;&#34;
    FINAL_TRANSCRIPT = &#34;final_transcript&#34;
    &#34;&#34;&#34;final transcript, emitted when the STT is confident enough that a certain
    portion of speech will not change&#34;&#34;&#34;
    RECOGNITION_USAGE = &#34;recognition_usage&#34;
    &#34;&#34;&#34;usage event, emitted periodically to indicate usage metrics&#34;&#34;&#34;
    END_OF_SPEECH = &#34;end_of_speech&#34;
    &#34;&#34;&#34;indicate the end of speech, emitted when the user stops speaking&#34;&#34;&#34;</code></pre>
</details>
<div class="desc"><p>str(object='') -&gt; str
str(bytes_or_buffer[, encoding[, errors]]) -&gt; str</p>
<p>Create a new string object from the given object. If encoding or
errors is specified, then the object must expose a data buffer
that will be decoded using the given encoding and error handler.
Otherwise, returns the result of object.<strong>str</strong>() (if defined)
or repr(object).
encoding defaults to 'utf-8'.
errors defaults to 'strict'.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.str</li>
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="livekit.agents.stt.stt.SpeechEventType.END_OF_SPEECH"><code class="name">var <span class="ident">END_OF_SPEECH</span></code></dt>
<dd>
<div class="desc"><p>indicate the end of speech, emitted when the user stops speaking</p></div>
</dd>
<dt id="livekit.agents.stt.stt.SpeechEventType.FINAL_TRANSCRIPT"><code class="name">var <span class="ident">FINAL_TRANSCRIPT</span></code></dt>
<dd>
<div class="desc"><p>final transcript, emitted when the STT is confident enough that a certain
portion of speech will not change</p></div>
</dd>
<dt id="livekit.agents.stt.stt.SpeechEventType.INTERIM_TRANSCRIPT"><code class="name">var <span class="ident">INTERIM_TRANSCRIPT</span></code></dt>
<dd>
<div class="desc"><p>interim transcript, useful for real-time transcription</p></div>
</dd>
<dt id="livekit.agents.stt.stt.SpeechEventType.RECOGNITION_USAGE"><code class="name">var <span class="ident">RECOGNITION_USAGE</span></code></dt>
<dd>
<div class="desc"><p>usage event, emitted periodically to indicate usage metrics</p></div>
</dd>
<dt id="livekit.agents.stt.stt.SpeechEventType.START_OF_SPEECH"><code class="name">var <span class="ident">START_OF_SPEECH</span></code></dt>
<dd>
<div class="desc"><p>indicate the start of speech
if the STT doesn't support this event, this will be emitted as the same time as the first INTERIM_TRANSCRIPT</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="livekit.agents.stt" href="index.html">livekit.agents.stt</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="livekit.agents.stt.stt.RecognitionUsage" href="#livekit.agents.stt.stt.RecognitionUsage">RecognitionUsage</a></code></h4>
<ul class="">
<li><code><a title="livekit.agents.stt.stt.RecognitionUsage.audio_duration" href="#livekit.agents.stt.stt.RecognitionUsage.audio_duration">audio_duration</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.agents.stt.stt.RecognizeStream" href="#livekit.agents.stt.stt.RecognizeStream">RecognizeStream</a></code></h4>
<ul class="">
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.aclose" href="#livekit.agents.stt.stt.RecognizeStream.aclose">aclose</a></code></li>
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.end_input" href="#livekit.agents.stt.stt.RecognizeStream.end_input">end_input</a></code></li>
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.flush" href="#livekit.agents.stt.stt.RecognizeStream.flush">flush</a></code></li>
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.push_frame" href="#livekit.agents.stt.stt.RecognizeStream.push_frame">push_frame</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.agents.stt.stt.RecognizeStream" href="#livekit.agents.stt.stt.RecognizeStream">RecognizeStream</a></code></h4>
<ul class="">
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.aclose" href="#livekit.agents.stt.stt.RecognizeStream.aclose">aclose</a></code></li>
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.end_input" href="#livekit.agents.stt.stt.RecognizeStream.end_input">end_input</a></code></li>
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.flush" href="#livekit.agents.stt.stt.RecognizeStream.flush">flush</a></code></li>
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.push_frame" href="#livekit.agents.stt.stt.RecognizeStream.push_frame">push_frame</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.agents.stt.stt.STT" href="#livekit.agents.stt.stt.STT">STT</a></code></h4>
<ul class="">
<li><code><a title="livekit.agents.stt.stt.STT.aclose" href="#livekit.agents.stt.stt.STT.aclose">aclose</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STT.capabilities" href="#livekit.agents.stt.stt.STT.capabilities">capabilities</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STT.label" href="#livekit.agents.stt.stt.STT.label">label</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STT.recognize" href="#livekit.agents.stt.stt.STT.recognize">recognize</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STT.stream" href="#livekit.agents.stt.stt.STT.stream">stream</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.agents.stt.stt.STTCapabilities" href="#livekit.agents.stt.stt.STTCapabilities">STTCapabilities</a></code></h4>
<ul class="">
<li><code><a title="livekit.agents.stt.stt.STTCapabilities.interim_results" href="#livekit.agents.stt.stt.STTCapabilities.interim_results">interim_results</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STTCapabilities.streaming" href="#livekit.agents.stt.stt.STTCapabilities.streaming">streaming</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.agents.stt.stt.STTError" href="#livekit.agents.stt.stt.STTError">STTError</a></code></h4>
<ul class="two-column">
<li><code><a title="livekit.agents.stt.stt.STTError.error" href="#livekit.agents.stt.stt.STTError.error">error</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STTError.label" href="#livekit.agents.stt.stt.STTError.label">label</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STTError.model_config" href="#livekit.agents.stt.stt.STTError.model_config">model_config</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STTError.recoverable" href="#livekit.agents.stt.stt.STTError.recoverable">recoverable</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STTError.timestamp" href="#livekit.agents.stt.stt.STTError.timestamp">timestamp</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STTError.type" href="#livekit.agents.stt.stt.STTError.type">type</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.agents.stt.stt.SpeechData" href="#livekit.agents.stt.stt.SpeechData">SpeechData</a></code></h4>
<ul class="">
<li><code><a title="livekit.agents.stt.stt.SpeechData.confidence" href="#livekit.agents.stt.stt.SpeechData.confidence">confidence</a></code></li>
<li><code><a title="livekit.agents.stt.stt.SpeechData.end_time" href="#livekit.agents.stt.stt.SpeechData.end_time">end_time</a></code></li>
<li><code><a title="livekit.agents.stt.stt.SpeechData.language" href="#livekit.agents.stt.stt.SpeechData.language">language</a></code></li>
<li><code><a title="livekit.agents.stt.stt.SpeechData.start_time" href="#livekit.agents.stt.stt.SpeechData.start_time">start_time</a></code></li>
<li><code><a title="livekit.agents.stt.stt.SpeechData.text" href="#livekit.agents.stt.stt.SpeechData.text">text</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.agents.stt.stt.SpeechEvent" href="#livekit.agents.stt.stt.SpeechEvent">SpeechEvent</a></code></h4>
<ul class="">
<li><code><a title="livekit.agents.stt.stt.SpeechEvent.alternatives" href="#livekit.agents.stt.stt.SpeechEvent.alternatives">alternatives</a></code></li>
<li><code><a title="livekit.agents.stt.stt.SpeechEvent.recognition_usage" href="#livekit.agents.stt.stt.SpeechEvent.recognition_usage">recognition_usage</a></code></li>
<li><code><a title="livekit.agents.stt.stt.SpeechEvent.request_id" href="#livekit.agents.stt.stt.SpeechEvent.request_id">request_id</a></code></li>
<li><code><a title="livekit.agents.stt.stt.SpeechEvent.type" href="#livekit.agents.stt.stt.SpeechEvent.type">type</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.agents.stt.stt.SpeechEventType" href="#livekit.agents.stt.stt.SpeechEventType">SpeechEventType</a></code></h4>
<ul class="">
<li><code><a title="livekit.agents.stt.stt.SpeechEventType.END_OF_SPEECH" href="#livekit.agents.stt.stt.SpeechEventType.END_OF_SPEECH">END_OF_SPEECH</a></code></li>
<li><code><a title="livekit.agents.stt.stt.SpeechEventType.FINAL_TRANSCRIPT" href="#livekit.agents.stt.stt.SpeechEventType.FINAL_TRANSCRIPT">FINAL_TRANSCRIPT</a></code></li>
<li><code><a title="livekit.agents.stt.stt.SpeechEventType.INTERIM_TRANSCRIPT" href="#livekit.agents.stt.stt.SpeechEventType.INTERIM_TRANSCRIPT">INTERIM_TRANSCRIPT</a></code></li>
<li><code><a title="livekit.agents.stt.stt.SpeechEventType.RECOGNITION_USAGE" href="#livekit.agents.stt.stt.SpeechEventType.RECOGNITION_USAGE">RECOGNITION_USAGE</a></code></li>
<li><code><a title="livekit.agents.stt.stt.SpeechEventType.START_OF_SPEECH" href="#livekit.agents.stt.stt.SpeechEventType.START_OF_SPEECH">START_OF_SPEECH</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
