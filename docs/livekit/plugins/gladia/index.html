<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>livekit.plugins.gladia API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>livekit.plugins.gladia</code></h1>
</header>
<section id="section-intro">
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="livekit.plugins.gladia.models" href="models.html">livekit.plugins.gladia.models</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="livekit.plugins.gladia.AudioEnergyFilter"><code class="flex name class">
<span>class <span class="ident">AudioEnergyFilter</span></span>
<span>(</span><span>*, min_silence: float = 1.5, rms_threshold: float = 1.6e-05)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AudioEnergyFilter:
    class State(Enum):
        START = 0
        SPEAKING = 1
        SILENCE = 2
        END = 3

    def __init__(self, *, min_silence: float = 1.5, rms_threshold: float = MAGIC_NUMBER_THRESHOLD):
        self._cooldown_seconds = min_silence
        self._cooldown = min_silence
        self._state = self.State.SILENCE
        self._rms_threshold = rms_threshold

    def update(self, frame: rtc.AudioFrame) -&gt; State:
        arr = np.frombuffer(frame.data, dtype=np.int16)
        float_arr = arr.astype(np.float32) / 32768.0
        rms = np.mean(np.square(float_arr))

        if rms &gt; self._rms_threshold:
            self._cooldown = self._cooldown_seconds
            if self._state in (self.State.SILENCE, self.State.END):
                self._state = self.State.START
            else:
                self._state = self.State.SPEAKING
        else:
            if self._cooldown &lt;= 0:
                if self._state in (self.State.SPEAKING, self.State.START):
                    self._state = self.State.END
                elif self._state == self.State.END:
                    self._state = self.State.SILENCE
            else:
                self._cooldown -= frame.duration
                self._state = self.State.SPEAKING

        return self._state</code></pre>
</details>
<div class="desc"></div>
<h3>Class variables</h3>
<dl>
<dt id="livekit.plugins.gladia.AudioEnergyFilter.State"><code class="name">var <span class="ident">State</span></code></dt>
<dd>
<div class="desc"><p>Create a collection of name/value pairs.</p>
<p>Example enumeration:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; class Color(Enum):
...     RED = 1
...     BLUE = 2
...     GREEN = 3
</code></pre>
<p>Access them by:</p>
<ul>
<li>attribute access:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color.RED
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li>value lookup:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color(1)
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li>name lookup:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color['RED']
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<p>Enumerations can be iterated over, and know how many members they have:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; len(Color)
3
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; list(Color)
[&lt;Color.RED: 1&gt;, &lt;Color.BLUE: 2&gt;, &lt;Color.GREEN: 3&gt;]
</code></pre>
<p>Methods can be added to enumerations, and members can have their own
attributes &ndash; see the documentation for details.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="livekit.plugins.gladia.AudioEnergyFilter.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, frame: rtc.AudioFrame) ‑> State</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self, frame: rtc.AudioFrame) -&gt; State:
    arr = np.frombuffer(frame.data, dtype=np.int16)
    float_arr = arr.astype(np.float32) / 32768.0
    rms = np.mean(np.square(float_arr))

    if rms &gt; self._rms_threshold:
        self._cooldown = self._cooldown_seconds
        if self._state in (self.State.SILENCE, self.State.END):
            self._state = self.State.START
        else:
            self._state = self.State.SPEAKING
    else:
        if self._cooldown &lt;= 0:
            if self._state in (self.State.SPEAKING, self.State.START):
                self._state = self.State.END
            elif self._state == self.State.END:
                self._state = self.State.SILENCE
        else:
            self._cooldown -= frame.duration
            self._state = self.State.SPEAKING

    return self._state</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="livekit.plugins.gladia.STT"><code class="flex name class">
<span>class <span class="ident">STT</span></span>
<span>(</span><span>*,<br>interim_results: bool = True,<br>languages: list[str] | None = None,<br>code_switching: bool = True,<br>sample_rate: int = 16000,<br>bit_depth: Literal[8, 16, 24, 32] = 16,<br>channels: int = 1,<br>encoding: "Literal['wav/pcm', 'wav/alaw', 'wav/ulaw']" = 'wav/pcm',<br>api_key: str | None = None,<br>http_session: aiohttp.ClientSession | None = None,<br>base_url: str = 'https://api.gladia.io/v2/live',<br>energy_filter: <a title="livekit.plugins.gladia.AudioEnergyFilter" href="#livekit.plugins.gladia.AudioEnergyFilter">AudioEnergyFilter</a> | bool = False,<br>translation_enabled: bool = False,<br>translation_target_languages: list[str] | None = None,<br>translation_model: str = 'base',<br>translation_match_original_utterances: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class STT(stt.STT):
    def __init__(
        self,
        *,
        interim_results: bool = True,
        languages: list[str] | None = None,
        code_switching: bool = True,
        sample_rate: int = 16000,
        bit_depth: Literal[8, 16, 24, 32] = 16,
        channels: int = 1,
        encoding: Literal[&#34;wav/pcm&#34;, &#34;wav/alaw&#34;, &#34;wav/ulaw&#34;] = &#34;wav/pcm&#34;,
        api_key: str | None = None,
        http_session: aiohttp.ClientSession | None = None,
        base_url: str = BASE_URL,
        energy_filter: AudioEnergyFilter | bool = False,
        translation_enabled: bool = False,
        translation_target_languages: list[str] | None = None,
        translation_model: str = &#34;base&#34;,
        translation_match_original_utterances: bool = True,
    ) -&gt; None:
        &#34;&#34;&#34;Create a new instance of Gladia STT.

        Args:
            interim_results: Whether to return interim (non-final) transcription results.
                            Defaults to True.
            languages: List of language codes to use for recognition. Defaults to None
                    (auto-detect).
            code_switching: Whether to allow switching between languages during recognition.
                            Defaults to True.
            sample_rate: The sample rate of the audio in Hz. Defaults to 16000.
            bit_depth: The bit depth of the audio. Defaults to 16.
            channels: The number of audio channels. Defaults to 1.
            encoding: The encoding of the audio. Defaults to &#34;wav/pcm&#34;.
            api_key: Your Gladia API key. If not provided, will look for GLADIA_API_KEY
                        environment variable.
            http_session: Optional aiohttp ClientSession to use for requests.
            base_url: The base URL for Gladia API. Defaults to &#34;https://api.gladia.io/v2/live&#34;.
            energy_filter: Audio energy filter configuration for voice activity detection.
                         Can be a boolean or AudioEnergyFilter instance. Defaults to False.
            translation_enabled: Whether to enable translation. Defaults to False.
            translation_target_languages: List of target languages for translation.
                                        Required if translation_enabled is True.
            translation_model: Translation model to use. Defaults to &#34;base&#34;.
            translation_match_original_utterances: Whether to match original utterances with
                                                    translations. Defaults to True.

        Raises:
            ValueError: If no API key is provided or found in environment variables.
        &#34;&#34;&#34;
        super().__init__(
            capabilities=stt.STTCapabilities(streaming=True, interim_results=interim_results)
        )
        self._base_url = base_url

        api_key = api_key or os.environ.get(&#34;GLADIA_API_KEY&#34;)
        if api_key is None:
            raise ValueError(&#34;Gladia API key is required&#34;)

        self._api_key = api_key

        language_config = LanguageConfiguration(languages=languages, code_switching=code_switching)

        translation_config = TranslationConfiguration(
            enabled=translation_enabled,
            target_languages=translation_target_languages or [],
            model=translation_model,
            match_original_utterances=translation_match_original_utterances,
        )

        if translation_enabled and not translation_target_languages:
            raise ValueError(
                &#34;translation_target_languages is required when translation_enabled is True&#34;
            )

        self._opts = STTOptions(
            language_config=language_config,
            interim_results=interim_results,
            sample_rate=sample_rate,
            bit_depth=bit_depth,
            channels=channels,
            encoding=encoding,
            translation_config=translation_config,
            energy_filter=energy_filter,
        )
        self._session = http_session
        self._streams = weakref.WeakSet()

    def _ensure_session(self) -&gt; aiohttp.ClientSession:
        if not self._session:
            self._session = utils.http_context.http_session()

        return self._session

    async def _recognize_impl(
        self,
        buffer: AudioBuffer,
        *,
        language: list[str] | None = None,
        conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
    ) -&gt; stt.SpeechEvent:
        &#34;&#34;&#34;Implement synchronous speech recognition for Gladia using the live endpoint.&#34;&#34;&#34;
        config = self._sanitize_options(languages=language)

        streaming_config = {
            &#34;encoding&#34;: config.encoding,
            &#34;sample_rate&#34;: config.sample_rate,
            &#34;bit_depth&#34;: config.bit_depth,
            &#34;channels&#34;: config.channels,
            &#34;language_config&#34;: {
                &#34;languages&#34;: config.language_config.languages or [],
                &#34;code_switching&#34;: config.language_config.code_switching,
            },
            &#34;realtime_processing&#34;: {
                &#34;words_accurate_timestamps&#34;: False,
                &#34;custom_vocabulary&#34;: False,
                &#34;custom_vocabulary_config&#34;: {
                    &#34;vocabulary&#34;: [
                        &#34;Gladia&#34;,
                        {&#34;value&#34;: &#34;Gladia&#34;, &#34;intensity&#34;: 0.5},
                    ],
                    &#34;default_intensity&#34;: 0.5,
                },
                &#34;custom_spelling&#34;: False,
                &#34;custom_spelling_config&#34;: {
                    &#34;spelling_dictionary&#34;: {
                        &#34;SQL&#34;: [&#34;Sequel&#34;],
                    }
                },
            },
        }

        # Add translation configuration if enabled
        if config.translation_config.enabled:
            streaming_config[&#34;realtime_processing&#34;][&#34;translation&#34;] = True
            streaming_config[&#34;realtime_processing&#34;][&#34;translation_config&#34;] = {
                &#34;target_languages&#34;: config.translation_config.target_languages,
                &#34;model&#34;: config.translation_config.model,
                &#34;match_original_utterances&#34;: config.translation_config.match_original_utterances,
            }

        try:
            # Initialize a session with Gladia
            session_response = await self._init_live_session(streaming_config, conn_options)
            session_id = session_response[&#34;id&#34;]
            session_url = session_response[&#34;url&#34;]

            # Connect to the WebSocket
            async with self._ensure_session().ws_connect(
                session_url,
                timeout=aiohttp.ClientTimeout(
                    total=30,  # Keep a reasonable total timeout
                    sock_connect=conn_options.timeout,
                ),
            ) as ws:
                # Combine audio frames to get a single frame with all raw PCM data
                combined_frame = rtc.combine_audio_frames(buffer)
                # Get the raw bytes from the combined frame
                pcm_data = combined_frame.data.tobytes()

                bytes_per_second = config.sample_rate * config.channels * (config.bit_depth // 8)
                chunk_size = (bytes_per_second * 150) // 1000
                chunk_size = max(chunk_size, 1024)

                # Send raw PCM audio data in chunks
                for i in range(0, len(pcm_data), chunk_size):
                    chunk = pcm_data[i : i + chunk_size]
                    chunk_b64 = base64.b64encode(chunk).decode(&#34;utf-8&#34;)
                    await ws.send_str(
                        json.dumps({&#34;type&#34;: &#34;audio_chunk&#34;, &#34;data&#34;: {&#34;chunk&#34;: chunk_b64}})
                    )

                # Tell Gladia we&#39;re done sending audio
                await ws.send_str(json.dumps({&#34;type&#34;: &#34;stop_recording&#34;}))

                # Wait for final transcript
                utterances = []

                # Receive messages until we get the post_final_transcript message
                try:
                    # Set a timeout for waiting for the final results after sending stop_recording
                    receive_timeout = conn_options.timeout * 5
                    async for msg in ws.iter(timeout=receive_timeout):
                        if msg.type == aiohttp.WSMsgType.TEXT:
                            data = json.loads(msg.data)
                            # Collect final utterances
                            if data[&#34;type&#34;] == &#34;transcript&#34; and data[&#34;data&#34;][&#34;is_final&#34;]:
                                utterance = data[&#34;data&#34;][&#34;utterance&#34;]
                                utterances.append(utterance)
                            # Check for translation as the final result if enabled
                            elif (
                                data[&#34;type&#34;] == &#34;translation&#34; and config.translation_config.enabled
                            ):
                                pass
                            elif data[&#34;type&#34;] == &#34;post_final_transcript&#34;:
                                break
                            elif data[&#34;type&#34;] == &#34;error&#34;:
                                raise APIConnectionError(
                                    f&#34;Gladia WebSocket error: {data.get(&#39;data&#39;)}&#34;
                                ) from None

                        elif msg.type == aiohttp.WSMsgType.ERROR:
                            logger.error(f&#34;Gladia WebSocket connection error: {ws.exception()}&#34;)
                            raise ws.exception() or APIConnectionError(
                                &#34;Gladia WebSocket connection error&#34;
                            )
                        elif msg.type in (
                            aiohttp.WSMsgType.CLOSE,
                            aiohttp.WSMsgType.CLOSED,
                            aiohttp.WSMsgType.CLOSING,
                        ):
                            logger.warning(
                                &#34;Gladia WebSocket closed unexpectedly during result receiving: &#34;
                                f&#34;type={msg.type}&#34;
                            )
                            break

                except asyncio.TimeoutError:
                    logger.warning(
                        f&#34;Timeout waiting for Gladia final transcript ({receive_timeout}s)&#34;
                    )
                    if not utterances:
                        raise APITimeoutError(
                            f&#34;Timeout waiting for Gladia final transcript ({receive_timeout}s)&#34;
                        ) from None

                # Create a speech event from the collected final utterances
                return self._create_speech_event(
                    utterances, session_id, config.language_config.languages
                )

        except asyncio.TimeoutError as e:
            # Catch timeout during connection or initial phase
            logger.error(f&#34;Timeout during Gladia connection/initialization: {e}&#34;)
            raise APITimeoutError(&#34;Timeout connecting to or initializing Gladia session&#34;) from e
        except aiohttp.ClientResponseError as e:
            # Error during session initialization POST request
            logger.error(f&#34;Gladia API status error during session init: {e.status} {e.message}&#34;)
            raise APIStatusError(
                message=e.message,
                status_code=e.status,
                request_id=e.headers.get(
                    &#34;X-Request-ID&#34;
                ),  # Check if Gladia provides a request ID header
                body=await e.response.text() if hasattr(e, &#34;response&#34;) else None,
            ) from e
        except aiohttp.ClientError as e:
            # General client errors (connection refused, DNS resolution etc.)
            logger.error(f&#34;Gladia connection error: {e}&#34;)
            raise APIConnectionError(f&#34;Gladia connection error: {e}&#34;) from e
        except Exception as e:
            # Catch-all for other unexpected errors
            logger.exception(
                f&#34;Unexpected error during Gladia synchronous recognition: {e}&#34;
            )  # Use logger.exception to include stack trace
            raise APIConnectionError(f&#34;An unexpected error occurred: {e}&#34;) from e

    async def _init_live_session(self, config: dict, conn_options: APIConnectOptions) -&gt; dict:
        &#34;&#34;&#34;Initialize a live transcription session with Gladia.&#34;&#34;&#34;
        try:
            async with self._ensure_session().post(
                url=self._base_url,
                json=config,
                headers={&#34;X-Gladia-Key&#34;: self._api_key},
                timeout=aiohttp.ClientTimeout(
                    total=30,
                    sock_connect=conn_options.timeout,
                ),
            ) as res:
                # Gladia returns 201 Created when successfully creating a session
                if res.status not in (200, 201):
                    raise APIStatusError(
                        message=f&#34;Failed to initialize Gladia session: {res.status}&#34;,
                        status_code=res.status,
                        request_id=None,
                        body=await res.text(),
                    )
                return await res.json()
        except Exception as e:
            logger.exception(f&#34;Failed to initialize Gladia session: {e}&#34;)
            raise APIConnectionError(f&#34;Failed to initialize Gladia session: {str(e)}&#34;) from e

    def _create_speech_event(
        self, utterances: list[dict], session_id: str, languages: list[str] | None
    ) -&gt; stt.SpeechEvent:
        &#34;&#34;&#34;Create a SpeechEvent from Gladia&#39;s transcript data.&#34;&#34;&#34;
        alternatives = []

        # Process each utterance into a SpeechData object
        for utterance in utterances:
            text = utterance.get(&#34;text&#34;, &#34;&#34;).strip()
            if text:
                alternatives.append(
                    stt.SpeechData(
                        language=utterance.get(&#34;language&#34;, languages[0] if languages else &#34;en&#34;),
                        start_time=utterance.get(&#34;start&#34;, 0),
                        end_time=utterance.get(&#34;end&#34;, 0),
                        confidence=utterance.get(&#34;confidence&#34;, 1.0),
                        text=text,
                    )
                )

        if not alternatives:
            alternatives.append(
                stt.SpeechData(
                    language=languages[0] if languages and len(languages) &gt; 0 else &#34;en&#34;,
                    start_time=0,
                    end_time=0,
                    confidence=1.0,
                    text=&#34;&#34;,
                )
            )

        return stt.SpeechEvent(
            type=stt.SpeechEventType.FINAL_TRANSCRIPT,
            request_id=session_id,
            alternatives=alternatives,
        )

    def stream(
        self,
        *,
        language: list[str] | None = None,
        conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
    ) -&gt; SpeechStream:
        config = self._sanitize_options(languages=language)
        stream = SpeechStream(
            stt=self,
            conn_options=conn_options,
            opts=config,
            api_key=self._api_key,
            http_session=self._ensure_session(),
            base_url=self._base_url,
        )
        self._streams.add(stream)
        return stream

    def update_options(
        self,
        *,
        languages: list[str] | None = None,
        code_switching: bool | None = None,
        interim_results: bool | None = None,
        sample_rate: int | None = None,
        bit_depth: Literal[8, 16, 24, 32] | None = None,
        channels: int | None = None,
        encoding: Literal[&#34;wav/pcm&#34;, &#34;wav/alaw&#34;, &#34;wav/ulaw&#34;] | None = None,
        translation_enabled: bool | None = None,
        translation_target_languages: list[str] | None = None,
        translation_model: str | None = None,
        translation_match_original_utterances: bool | None = None,
    ):
        if languages is not None or code_switching is not None:
            language_config = dataclasses.replace(
                self._opts.language_config,
                languages=languages
                if languages is not None
                else self._opts.language_config.languages,
                code_switching=code_switching
                if code_switching is not None
                else self._opts.language_config.code_switching,
            )
            self._opts.language_config = language_config

        if (
            translation_enabled is not None
            or translation_target_languages is not None
            or translation_model is not None
            or translation_match_original_utterances is not None
        ):
            translation_config = dataclasses.replace(
                self._opts.translation_config,
                enabled=translation_enabled
                if translation_enabled is not None
                else self._opts.translation_config.enabled,
                target_languages=translation_target_languages
                if translation_target_languages is not None
                else self._opts.translation_config.target_languages,
                model=translation_model
                if translation_model is not None
                else self._opts.translation_config.model,
                match_original_utterances=translation_match_original_utterances
                if translation_match_original_utterances is not None
                else self._opts.translation_config.match_original_utterances,
            )
            self._opts.translation_config = translation_config

        if interim_results is not None:
            self._opts.interim_results = interim_results
        if sample_rate is not None:
            self._opts.sample_rate = sample_rate
        if bit_depth is not None:
            self._opts.bit_depth = bit_depth
        if channels is not None:
            self._opts.channels = channels
        if encoding is not None:
            self._opts.encoding = encoding

        for stream in self._streams:
            stream.update_options(
                languages=languages,
                code_switching=code_switching,
                interim_results=interim_results,
                sample_rate=sample_rate,
                bit_depth=bit_depth,
                channels=channels,
                encoding=encoding,
                translation_enabled=translation_enabled,
                translation_target_languages=translation_target_languages,
                translation_model=translation_model,
                translation_match_original_utterances=translation_match_original_utterances,
            )

    def _sanitize_options(self, *, languages: list[str] | None = None) -&gt; STTOptions:
        config = dataclasses.replace(self._opts)
        if languages is not None:
            language_config = dataclasses.replace(
                config.language_config,
                languages=languages,
            )
            config.language_config = language_config
        return config</code></pre>
</details>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Create a new instance of Gladia STT.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>interim_results</code></strong></dt>
<dd>Whether to return interim (non-final) transcription results.
Defaults to True.</dd>
<dt><strong><code>languages</code></strong></dt>
<dd>List of language codes to use for recognition. Defaults to None
(auto-detect).</dd>
<dt><strong><code>code_switching</code></strong></dt>
<dd>Whether to allow switching between languages during recognition.
Defaults to True.</dd>
<dt><strong><code>sample_rate</code></strong></dt>
<dd>The sample rate of the audio in Hz. Defaults to 16000.</dd>
<dt><strong><code>bit_depth</code></strong></dt>
<dd>The bit depth of the audio. Defaults to 16.</dd>
<dt><strong><code>channels</code></strong></dt>
<dd>The number of audio channels. Defaults to 1.</dd>
<dt><strong><code>encoding</code></strong></dt>
<dd>The encoding of the audio. Defaults to "wav/pcm".</dd>
<dt><strong><code>api_key</code></strong></dt>
<dd>Your Gladia API key. If not provided, will look for GLADIA_API_KEY
environment variable.</dd>
<dt><strong><code>http_session</code></strong></dt>
<dd>Optional aiohttp ClientSession to use for requests.</dd>
<dt><strong><code>base_url</code></strong></dt>
<dd>The base URL for Gladia API. Defaults to "https://api.gladia.io/v2/live".</dd>
<dt><strong><code>energy_filter</code></strong></dt>
<dd>Audio energy filter configuration for voice activity detection.
Can be a boolean or AudioEnergyFilter instance. Defaults to False.</dd>
<dt><strong><code>translation_enabled</code></strong></dt>
<dd>Whether to enable translation. Defaults to False.</dd>
<dt><strong><code>translation_target_languages</code></strong></dt>
<dd>List of target languages for translation.
Required if translation_enabled is True.</dd>
<dt><strong><code>translation_model</code></strong></dt>
<dd>Translation model to use. Defaults to "base".</dd>
<dt><strong><code>translation_match_original_utterances</code></strong></dt>
<dd>Whether to match original utterances with
translations. Defaults to True.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If no API key is provided or found in environment variables.</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="livekit.agents.stt.stt.STT" href="../../agents/stt/stt.html#livekit.agents.stt.stt.STT">STT</a></li>
<li>abc.ABC</li>
<li><a title="livekit.rtc.event_emitter.EventEmitter" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter">EventEmitter</a></li>
<li>typing.Generic</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="livekit.plugins.gladia.STT.stream"><code class="name flex">
<span>def <span class="ident">stream</span></span>(<span>self,<br>*,<br>language: list[str] | None = None,<br>conn_options: APIConnectOptions = APIConnectOptions(max_retry=3, retry_interval=2.0, timeout=10.0)) ‑> livekit.plugins.gladia.stt.SpeechStream</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stream(
    self,
    *,
    language: list[str] | None = None,
    conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
) -&gt; SpeechStream:
    config = self._sanitize_options(languages=language)
    stream = SpeechStream(
        stt=self,
        conn_options=conn_options,
        opts=config,
        api_key=self._api_key,
        http_session=self._ensure_session(),
        base_url=self._base_url,
    )
    self._streams.add(stream)
    return stream</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.gladia.STT.update_options"><code class="name flex">
<span>def <span class="ident">update_options</span></span>(<span>self,<br>*,<br>languages: list[str] | None = None,<br>code_switching: bool | None = None,<br>interim_results: bool | None = None,<br>sample_rate: int | None = None,<br>bit_depth: Literal[8, 16, 24, 32] | None = None,<br>channels: int | None = None,<br>encoding: "Literal['wav/pcm', 'wav/alaw', 'wav/ulaw'] | None" = None,<br>translation_enabled: bool | None = None,<br>translation_target_languages: list[str] | None = None,<br>translation_model: str | None = None,<br>translation_match_original_utterances: bool | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_options(
    self,
    *,
    languages: list[str] | None = None,
    code_switching: bool | None = None,
    interim_results: bool | None = None,
    sample_rate: int | None = None,
    bit_depth: Literal[8, 16, 24, 32] | None = None,
    channels: int | None = None,
    encoding: Literal[&#34;wav/pcm&#34;, &#34;wav/alaw&#34;, &#34;wav/ulaw&#34;] | None = None,
    translation_enabled: bool | None = None,
    translation_target_languages: list[str] | None = None,
    translation_model: str | None = None,
    translation_match_original_utterances: bool | None = None,
):
    if languages is not None or code_switching is not None:
        language_config = dataclasses.replace(
            self._opts.language_config,
            languages=languages
            if languages is not None
            else self._opts.language_config.languages,
            code_switching=code_switching
            if code_switching is not None
            else self._opts.language_config.code_switching,
        )
        self._opts.language_config = language_config

    if (
        translation_enabled is not None
        or translation_target_languages is not None
        or translation_model is not None
        or translation_match_original_utterances is not None
    ):
        translation_config = dataclasses.replace(
            self._opts.translation_config,
            enabled=translation_enabled
            if translation_enabled is not None
            else self._opts.translation_config.enabled,
            target_languages=translation_target_languages
            if translation_target_languages is not None
            else self._opts.translation_config.target_languages,
            model=translation_model
            if translation_model is not None
            else self._opts.translation_config.model,
            match_original_utterances=translation_match_original_utterances
            if translation_match_original_utterances is not None
            else self._opts.translation_config.match_original_utterances,
        )
        self._opts.translation_config = translation_config

    if interim_results is not None:
        self._opts.interim_results = interim_results
    if sample_rate is not None:
        self._opts.sample_rate = sample_rate
    if bit_depth is not None:
        self._opts.bit_depth = bit_depth
    if channels is not None:
        self._opts.channels = channels
    if encoding is not None:
        self._opts.encoding = encoding

    for stream in self._streams:
        stream.update_options(
            languages=languages,
            code_switching=code_switching,
            interim_results=interim_results,
            sample_rate=sample_rate,
            bit_depth=bit_depth,
            channels=channels,
            encoding=encoding,
            translation_enabled=translation_enabled,
            translation_target_languages=translation_target_languages,
            translation_model=translation_model,
            translation_match_original_utterances=translation_match_original_utterances,
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="livekit.agents.stt.stt.STT" href="../../agents/stt/stt.html#livekit.agents.stt.stt.STT">STT</a></b></code>:
<ul class="hlist">
<li><code><a title="livekit.agents.stt.stt.STT.aclose" href="../../agents/stt/stt.html#livekit.agents.stt.stt.STT.aclose">aclose</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STT.emit" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.emit">emit</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STT.off" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.off">off</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STT.on" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.on">on</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STT.once" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.once">once</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="livekit.plugins.gladia.SpeechStream"><code class="flex name class">
<span>class <span class="ident">SpeechStream</span></span>
<span>(</span><span>*,<br>stt: <a title="livekit.plugins.gladia.STT" href="#livekit.plugins.gladia.STT">STT</a>,<br>opts: STTOptions,<br>conn_options: APIConnectOptions,<br>api_key: str,<br>http_session: aiohttp.ClientSession,<br>base_url: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SpeechStream(stt.SpeechStream):
    def __init__(
        self,
        *,
        stt: STT,
        opts: STTOptions,
        conn_options: APIConnectOptions,
        api_key: str,
        http_session: aiohttp.ClientSession,
        base_url: str,
    ) -&gt; None:
        super().__init__(stt=stt, conn_options=conn_options, sample_rate=opts.sample_rate)

        self._opts = opts
        self._api_key = api_key
        self._session = http_session
        self._base_url = base_url
        self._speaking = False
        self._audio_duration_collector = PeriodicCollector(
            callback=self._on_audio_duration_report,
            duration=5.0,
        )

        self._audio_energy_filter: AudioEnergyFilter | None = None
        if opts.energy_filter:
            if isinstance(opts.energy_filter, AudioEnergyFilter):
                self._audio_energy_filter = opts.energy_filter
            else:
                self._audio_energy_filter = AudioEnergyFilter()

        self._pushed_audio_duration = 0.0
        self._request_id = &#34;&#34;
        self._reconnect_event = asyncio.Event()
        self._ws: aiohttp.ClientWebSocketResponse | None = None

    def update_options(
        self,
        *,
        languages: list[str] | None = None,
        code_switching: bool | None = None,
        interim_results: bool | None = None,
        sample_rate: int | None = None,
        bit_depth: Literal[8, 16, 24, 32] | None = None,
        channels: int | None = None,
        encoding: Literal[&#34;wav/pcm&#34;, &#34;wav/alaw&#34;, &#34;wav/ulaw&#34;] | None = None,
        translation_enabled: bool | None = None,
        translation_target_languages: list[str] | None = None,
        translation_model: str | None = None,
        translation_match_original_utterances: bool | None = None,
    ):
        if languages is not None or code_switching is not None:
            language_config = dataclasses.replace(
                self._opts.language_config,
                languages=languages
                if languages is not None
                else self._opts.language_config.languages,
                code_switching=code_switching
                if code_switching is not None
                else self._opts.language_config.code_switching,
            )
            self._opts.language_config = language_config

        if (
            translation_enabled is not None
            or translation_target_languages is not None
            or translation_model is not None
            or translation_match_original_utterances is not None
        ):
            translation_config = dataclasses.replace(
                self._opts.translation_config,
                enabled=translation_enabled
                if translation_enabled is not None
                else self._opts.translation_config.enabled,
                target_languages=translation_target_languages
                if translation_target_languages is not None
                else self._opts.translation_config.target_languages,
                model=translation_model
                if translation_model is not None
                else self._opts.translation_config.model,
                match_original_utterances=translation_match_original_utterances
                if translation_match_original_utterances is not None
                else self._opts.translation_config.match_original_utterances,
            )
            self._opts.translation_config = translation_config

        if interim_results is not None:
            self._opts.interim_results = interim_results
        if sample_rate is not None:
            self._opts.sample_rate = sample_rate
        if bit_depth is not None:
            self._opts.bit_depth = bit_depth
        if channels is not None:
            self._opts.channels = channels
        if encoding is not None:
            self._opts.encoding = encoding

        self._reconnect_event.set()

    async def _run(self) -&gt; None:
        backoff_time = 1.0
        max_backoff = 30.0

        while True:
            try:
                # Initialize the Gladia session
                session_info = await self._init_live_session()
                session_url = session_info[&#34;url&#34;]
                self._request_id = session_info[&#34;id&#34;]

                # Reset backoff on success
                backoff_time = 1.0

                # Connect to the WebSocket
                async with self._session.ws_connect(session_url) as ws:
                    self._ws = ws
                    logger.info(f&#34;Connected to Gladia session {self._request_id}&#34;)

                    send_task = asyncio.create_task(self._send_audio_task())
                    recv_task = asyncio.create_task(self._recv_messages_task())

                    wait_reconnect_task = asyncio.create_task(self._reconnect_event.wait())

                    try:
                        done, _ = await asyncio.wait(
                            [send_task, recv_task, wait_reconnect_task],
                            return_when=asyncio.FIRST_COMPLETED,
                        )

                        for task in done:
                            if task != wait_reconnect_task:
                                task.result()

                        if wait_reconnect_task not in done:
                            break

                        self._reconnect_event.clear()
                        logger.info(&#34;Reconnecting Gladia session due to options change&#34;)
                    finally:
                        await utils.aio.gracefully_cancel(send_task, recv_task, wait_reconnect_task)
                        self._ws = None
            except APIStatusError as e:
                if e.status_code == 429:
                    logger.warning(
                        f&#34;Rate limited by Gladia API. Backing off for {backoff_time} seconds.&#34;
                    )
                    await asyncio.sleep(backoff_time)
                    backoff_time = min(backoff_time * 2, max_backoff)
                else:
                    logger.exception(f&#34;Error in speech stream: {e}&#34;)
                    await asyncio.sleep(backoff_time)
            except Exception as e:
                logger.exception(f&#34;Error in speech stream: {e}&#34;)
                # Wait a bit before reconnecting to avoid rapid reconnection attempts
                await asyncio.sleep(backoff_time)

    async def _init_live_session(self) -&gt; dict:
        &#34;&#34;&#34;Initialize a live session with Gladia.&#34;&#34;&#34;
        streaming_config = {
            &#34;encoding&#34;: self._opts.encoding,
            &#34;sample_rate&#34;: self._opts.sample_rate,
            &#34;bit_depth&#34;: self._opts.bit_depth,
            &#34;channels&#34;: self._opts.channels,
            &#34;language_config&#34;: {
                &#34;languages&#34;: self._opts.language_config.languages or [],
                &#34;code_switching&#34;: self._opts.language_config.code_switching,
            },
            &#34;realtime_processing&#34;: {},
        }

        # Add translation configuration if enabled
        if self._opts.translation_config.enabled:
            streaming_config[&#34;realtime_processing&#34;][&#34;translation&#34;] = True
            streaming_config[&#34;realtime_processing&#34;][&#34;translation_config&#34;] = {
                &#34;target_languages&#34;: self._opts.translation_config.target_languages,
                &#34;model&#34;: self._opts.translation_config.model,
                &#34;match_original_utterances&#34;: (
                    self._opts.translation_config.match_original_utterances
                ),
            }

        try:
            async with self._session.post(
                url=self._base_url,
                json=streaming_config,
                headers={&#34;X-Gladia-Key&#34;: self._api_key},
                timeout=aiohttp.ClientTimeout(
                    total=30,
                    sock_connect=self._conn_options.timeout,
                ),
            ) as res:
                # Gladia returns 201 Created when successfully creating a session
                if res.status not in (200, 201):
                    raise APIStatusError(
                        message=f&#34;Failed to initialize Gladia session: {res.status}&#34;,
                        status_code=res.status,
                        request_id=None,
                        body=await res.text(),
                    )
                return await res.json()
        except Exception as e:
            logger.exception(f&#34;Failed to initialize Gladia session: {e}&#34;)
            raise APIConnectionError(f&#34;Failed to initialize Gladia session: {str(e)}&#34;) from e

    async def _send_audio_task(self):
        &#34;&#34;&#34;Send audio data to Gladia WebSocket.&#34;&#34;&#34;
        if not self._ws:
            return

        # We&#39;ll aim to send audio chunks every ~100ms
        samples_100ms = self._opts.sample_rate // 10
        audio_bstream = utils.audio.AudioByteStream(
            sample_rate=self._opts.sample_rate,
            num_channels=self._opts.channels,
            samples_per_channel=samples_100ms,
        )

        has_ended = False
        last_frame: rtc.AudioFrame | None = None

        async for data in self._input_ch:
            if not self._ws:
                break

            frames: list[rtc.AudioFrame] = []
            if isinstance(data, rtc.AudioFrame):
                state = self._check_energy_state(data)
                if state in (
                    AudioEnergyFilter.State.START,
                    AudioEnergyFilter.State.SPEAKING,
                ):
                    if last_frame:
                        frames.extend(audio_bstream.write(last_frame.data.tobytes()))
                        last_frame = None
                    frames.extend(audio_bstream.write(data.data.tobytes()))
                elif state == AudioEnergyFilter.State.END:
                    frames = audio_bstream.flush()
                    has_ended = True
                elif state == AudioEnergyFilter.State.SILENCE:
                    last_frame = data
            elif isinstance(data, self._FlushSentinel):
                frames = audio_bstream.flush()
                has_ended = True

            for frame in frames:
                self._audio_duration_collector.push(frame.duration)
                # Encode the audio data as base64
                chunk_b64 = base64.b64encode(frame.data.tobytes()).decode(&#34;utf-8&#34;)
                message = json.dumps({&#34;type&#34;: &#34;audio_chunk&#34;, &#34;data&#34;: {&#34;chunk&#34;: chunk_b64}})
                await self._ws.send_str(message)

                if has_ended:
                    self._audio_duration_collector.flush()
                    await self._ws.send_str(json.dumps({&#34;type&#34;: &#34;stop_recording&#34;}))
                    has_ended = False

        # Tell Gladia we&#39;re done sending audio when the stream ends
        if self._ws:
            await self._ws.send_str(json.dumps({&#34;type&#34;: &#34;stop_recording&#34;}))

    async def _recv_messages_task(self):
        &#34;&#34;&#34;Receive and process messages from Gladia WebSocket.&#34;&#34;&#34;
        if not self._ws:
            return

        async for msg in self._ws:
            if msg.type == aiohttp.WSMsgType.TEXT:
                try:
                    data = json.loads(msg.data)
                    self._process_gladia_message(data)
                except Exception as e:
                    logger.exception(f&#34;Error processing Gladia message: {e}&#34;)
            elif msg.type in (
                aiohttp.WSMsgType.CLOSED,
                aiohttp.WSMsgType.CLOSE,
                aiohttp.WSMsgType.CLOSING,
            ):
                break
            else:
                logger.warning(f&#34;Unexpected message type from Gladia: {msg.type}&#34;)

    def _process_gladia_message(self, data: dict):
        &#34;&#34;&#34;Process messages from Gladia WebSocket.&#34;&#34;&#34;
        if data[&#34;type&#34;] == &#34;transcript&#34;:
            is_final = data[&#34;data&#34;][&#34;is_final&#34;]
            utterance = data[&#34;data&#34;][&#34;utterance&#34;]
            text = utterance.get(&#34;text&#34;, &#34;&#34;).strip()

            if not self._speaking and text:
                self._speaking = True
                self._event_ch.send_nowait(
                    stt.SpeechEvent(
                        type=stt.SpeechEventType.START_OF_SPEECH, request_id=self._request_id
                    )
                )

            if text:
                language = utterance.get(
                    &#34;language&#34;,
                    self._opts.language_config.languages[0]
                    if self._opts.language_config.languages
                    else &#34;en&#34;,
                )

                speech_data = stt.SpeechData(
                    language=language,
                    start_time=utterance.get(&#34;start&#34;, 0),
                    end_time=utterance.get(&#34;end&#34;, 0),
                    confidence=utterance.get(&#34;confidence&#34;, 1.0),
                    text=text,
                )

                if is_final:
                    # Only emit FINAL_TRANSCRIPT for the *original* language
                    # if translation is NOT enabled.
                    if not self._opts.translation_config.enabled:
                        event = stt.SpeechEvent(
                            type=stt.SpeechEventType.FINAL_TRANSCRIPT,
                            request_id=self._request_id,
                            alternatives=[speech_data],
                        )
                        self._event_ch.send_nowait(event)

                        # End of speech after final original transcript only if not translating
                        if self._speaking:
                            self._speaking = False
                            self._event_ch.send_nowait(
                                stt.SpeechEvent(
                                    type=stt.SpeechEventType.END_OF_SPEECH,
                                    request_id=self._request_id,
                                )
                            )
                    # If translation *is* enabled, we suppress this final event
                    # and wait for the &#39;translation&#39; message to emit the final event.
                elif self._opts.interim_results:
                    # Always send INTERIM_TRANSCRIPT for the original language if enabled
                    event = stt.SpeechEvent(
                        type=stt.SpeechEventType.INTERIM_TRANSCRIPT,
                        request_id=self._request_id,
                        alternatives=[speech_data],
                    )
                    self._event_ch.send_nowait(event)

        elif data[&#34;type&#34;] == &#34;translation&#34;:
            # Process translation messages according to Gladia&#39;s documentation:
            # https://docs.gladia.io/reference/realtime-messages/translation
            if self._opts.translation_config.enabled and &#34;data&#34; in data:
                translation_data = data[&#34;data&#34;]

                # Extract translated utterance
                translated_utterance = translation_data.get(&#34;translated_utterance&#34;, {})
                if not translated_utterance:
                    logger.warning(
                        f&#34;No translated_utterance in translation message: {translation_data}&#34;
                    )
                    return

                # Get language information
                target_language = translation_data.get(&#34;target_language&#34;, &#34;&#34;)
                language = translated_utterance.get(&#34;language&#34;, target_language)

                # Get the translated text
                translated_text = translated_utterance.get(&#34;text&#34;, &#34;&#34;).strip()

                if translated_text and language:
                    # Create speech data for the translation
                    speech_data = stt.SpeechData(
                        language=language,  # Use the target language
                        start_time=translated_utterance.get(&#34;start&#34;, 0),
                        end_time=translated_utterance.get(&#34;end&#34;, 0),
                        confidence=translated_utterance.get(&#34;confidence&#34;, 1.0),
                        text=translated_text,  # Use the translated text
                    )

                    # Emit FINAL_TRANSCRIPT containing the TRANSLATION
                    event = stt.SpeechEvent(
                        type=stt.SpeechEventType.FINAL_TRANSCRIPT,
                        request_id=self._request_id,
                        alternatives=[speech_data],  # Now contains translated data
                    )
                    self._event_ch.send_nowait(event)

                    # Emit END_OF_SPEECH after the final *translated* transcript
                    if self._speaking:
                        self._speaking = False
                        self._event_ch.send_nowait(
                            stt.SpeechEvent(
                                type=stt.SpeechEventType.END_OF_SPEECH, request_id=self._request_id
                            )
                        )

        elif data[&#34;type&#34;] == &#34;post_final_transcript&#34;:
            # This is sent at the end of a session
            # We now tie END_OF_SPEECH to the emission of the relevant FINAL_TRANSCRIPT
            # (either original if no translation, or translated if translation is enabled).
            # So, we might not strictly need to act on this message anymore for END_OF_SPEECH,
            # but ensure speaking state is reset if somehow missed.
            if self._speaking:
                self._speaking = False

    def _check_energy_state(self, frame: rtc.AudioFrame) -&gt; AudioEnergyFilter.State:
        &#34;&#34;&#34;Check the energy state of an audio frame.&#34;&#34;&#34;
        if self._audio_energy_filter:
            return self._audio_energy_filter.update(frame)
        return AudioEnergyFilter.State.SPEAKING

    def _on_audio_duration_report(self, duration: float) -&gt; None:
        &#34;&#34;&#34;Report the audio duration for usage tracking.&#34;&#34;&#34;
        usage_event = stt.SpeechEvent(
            type=stt.SpeechEventType.RECOGNITION_USAGE,
            request_id=self._request_id,
            alternatives=[],
            recognition_usage=stt.RecognitionUsage(audio_duration=duration),
        )
        self._event_ch.send_nowait(usage_event)</code></pre>
</details>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Args:
sample_rate : int or None, optional
The desired sample rate for the audio input.
If specified, the audio input will be automatically resampled to match
the given sample rate before being processed for Speech-to-Text.
If not provided (None), the input will retain its original sample rate.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="livekit.agents.stt.stt.RecognizeStream" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream">RecognizeStream</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="livekit.plugins.gladia.SpeechStream.update_options"><code class="name flex">
<span>def <span class="ident">update_options</span></span>(<span>self,<br>*,<br>languages: list[str] | None = None,<br>code_switching: bool | None = None,<br>interim_results: bool | None = None,<br>sample_rate: int | None = None,<br>bit_depth: Literal[8, 16, 24, 32] | None = None,<br>channels: int | None = None,<br>encoding: "Literal['wav/pcm', 'wav/alaw', 'wav/ulaw'] | None" = None,<br>translation_enabled: bool | None = None,<br>translation_target_languages: list[str] | None = None,<br>translation_model: str | None = None,<br>translation_match_original_utterances: bool | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_options(
    self,
    *,
    languages: list[str] | None = None,
    code_switching: bool | None = None,
    interim_results: bool | None = None,
    sample_rate: int | None = None,
    bit_depth: Literal[8, 16, 24, 32] | None = None,
    channels: int | None = None,
    encoding: Literal[&#34;wav/pcm&#34;, &#34;wav/alaw&#34;, &#34;wav/ulaw&#34;] | None = None,
    translation_enabled: bool | None = None,
    translation_target_languages: list[str] | None = None,
    translation_model: str | None = None,
    translation_match_original_utterances: bool | None = None,
):
    if languages is not None or code_switching is not None:
        language_config = dataclasses.replace(
            self._opts.language_config,
            languages=languages
            if languages is not None
            else self._opts.language_config.languages,
            code_switching=code_switching
            if code_switching is not None
            else self._opts.language_config.code_switching,
        )
        self._opts.language_config = language_config

    if (
        translation_enabled is not None
        or translation_target_languages is not None
        or translation_model is not None
        or translation_match_original_utterances is not None
    ):
        translation_config = dataclasses.replace(
            self._opts.translation_config,
            enabled=translation_enabled
            if translation_enabled is not None
            else self._opts.translation_config.enabled,
            target_languages=translation_target_languages
            if translation_target_languages is not None
            else self._opts.translation_config.target_languages,
            model=translation_model
            if translation_model is not None
            else self._opts.translation_config.model,
            match_original_utterances=translation_match_original_utterances
            if translation_match_original_utterances is not None
            else self._opts.translation_config.match_original_utterances,
        )
        self._opts.translation_config = translation_config

    if interim_results is not None:
        self._opts.interim_results = interim_results
    if sample_rate is not None:
        self._opts.sample_rate = sample_rate
    if bit_depth is not None:
        self._opts.bit_depth = bit_depth
    if channels is not None:
        self._opts.channels = channels
    if encoding is not None:
        self._opts.encoding = encoding

    self._reconnect_event.set()</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="livekit.agents.stt.stt.RecognizeStream" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream">RecognizeStream</a></b></code>:
<ul class="hlist">
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.aclose" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream.aclose">aclose</a></code></li>
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.end_input" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream.end_input">end_input</a></code></li>
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.flush" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream.flush">flush</a></code></li>
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.push_frame" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream.push_frame">push_frame</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="livekit.plugins" href="../index.html">livekit.plugins</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="livekit.plugins.gladia.models" href="models.html">livekit.plugins.gladia.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="livekit.plugins.gladia.AudioEnergyFilter" href="#livekit.plugins.gladia.AudioEnergyFilter">AudioEnergyFilter</a></code></h4>
<ul class="">
<li><code><a title="livekit.plugins.gladia.AudioEnergyFilter.State" href="#livekit.plugins.gladia.AudioEnergyFilter.State">State</a></code></li>
<li><code><a title="livekit.plugins.gladia.AudioEnergyFilter.update" href="#livekit.plugins.gladia.AudioEnergyFilter.update">update</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.plugins.gladia.STT" href="#livekit.plugins.gladia.STT">STT</a></code></h4>
<ul class="">
<li><code><a title="livekit.plugins.gladia.STT.stream" href="#livekit.plugins.gladia.STT.stream">stream</a></code></li>
<li><code><a title="livekit.plugins.gladia.STT.update_options" href="#livekit.plugins.gladia.STT.update_options">update_options</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.plugins.gladia.SpeechStream" href="#livekit.plugins.gladia.SpeechStream">SpeechStream</a></code></h4>
<ul class="">
<li><code><a title="livekit.plugins.gladia.SpeechStream.update_options" href="#livekit.plugins.gladia.SpeechStream.update_options">update_options</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
