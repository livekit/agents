<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>livekit.plugins.google API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>livekit.plugins.google</code></h1>
</header>
<section id="section-intro">
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="livekit.plugins.google.beta" href="beta/index.html">livekit.plugins.google.beta</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="livekit.plugins.google.LLM"><code class="flex name class">
<span>class <span class="ident">LLM</span></span>
<span>(</span><span>*,<br>model: ChatModels | str = 'gemini-2.0-flash-001',<br>api_key: NotGivenOr[str] = NOT_GIVEN,<br>vertexai: NotGivenOr[bool] = False,<br>project: NotGivenOr[str] = NOT_GIVEN,<br>location: NotGivenOr[str] = NOT_GIVEN,<br>temperature: NotGivenOr[float] = NOT_GIVEN,<br>max_output_tokens: NotGivenOr[int] = NOT_GIVEN,<br>top_p: NotGivenOr[float] = NOT_GIVEN,<br>top_k: NotGivenOr[float] = NOT_GIVEN,<br>presence_penalty: NotGivenOr[float] = NOT_GIVEN,<br>frequency_penalty: NotGivenOr[float] = NOT_GIVEN,<br>tool_choice: NotGivenOr[ToolChoice] = NOT_GIVEN,<br>thinking_config: NotGivenOr[types.ThinkingConfigOrDict] = NOT_GIVEN)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LLM(llm.LLM):
    def __init__(
        self,
        *,
        model: ChatModels | str = &#34;gemini-2.0-flash-001&#34;,
        api_key: NotGivenOr[str] = NOT_GIVEN,
        vertexai: NotGivenOr[bool] = False,
        project: NotGivenOr[str] = NOT_GIVEN,
        location: NotGivenOr[str] = NOT_GIVEN,
        temperature: NotGivenOr[float] = NOT_GIVEN,
        max_output_tokens: NotGivenOr[int] = NOT_GIVEN,
        top_p: NotGivenOr[float] = NOT_GIVEN,
        top_k: NotGivenOr[float] = NOT_GIVEN,
        presence_penalty: NotGivenOr[float] = NOT_GIVEN,
        frequency_penalty: NotGivenOr[float] = NOT_GIVEN,
        tool_choice: NotGivenOr[ToolChoice] = NOT_GIVEN,
        thinking_config: NotGivenOr[types.ThinkingConfigOrDict] = NOT_GIVEN,
    ) -&gt; None:
        &#34;&#34;&#34;
        Create a new instance of Google GenAI LLM.

        Environment Requirements:
        - For VertexAI: Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to the path of the service account key file.
        The Google Cloud project and location can be set via `project` and `location` arguments or the environment variables
        `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION`. By default, the project is inferred from the service account key file,
        and the location defaults to &#34;us-central1&#34;.
        - For Google Gemini API: Set the `api_key` argument or the `GOOGLE_API_KEY` environment variable.

        Args:
            model (ChatModels | str, optional): The model name to use. Defaults to &#34;gemini-2.0-flash-001&#34;.
            api_key (str, optional): The API key for Google Gemini. If not provided, it attempts to read from the `GOOGLE_API_KEY` environment variable.
            vertexai (bool, optional): Whether to use VertexAI. Defaults to False.
            project (str, optional): The Google Cloud project to use (only for VertexAI). Defaults to None.
            location (str, optional): The location to use for VertexAI API requests. Defaults value is &#34;us-central1&#34;.
            temperature (float, optional): Sampling temperature for response generation. Defaults to 0.8.
            max_output_tokens (int, optional): Maximum number of tokens to generate in the output. Defaults to None.
            top_p (float, optional): The nucleus sampling probability for response generation. Defaults to None.
            top_k (int, optional): The top-k sampling value for response generation. Defaults to None.
            presence_penalty (float, optional): Penalizes the model for generating previously mentioned concepts. Defaults to None.
            frequency_penalty (float, optional): Penalizes the model for repeating words. Defaults to None.
            tool_choice (ToolChoice, optional): Specifies whether to use tools during response generation. Defaults to &#34;auto&#34;.
            thinking_config (ThinkingConfigOrDict, optional): The thinking configuration for response generation. Defaults to None.
        &#34;&#34;&#34;  # noqa: E501
        super().__init__()
        gcp_project = project if is_given(project) else os.environ.get(&#34;GOOGLE_CLOUD_PROJECT&#34;)
        gcp_location = location if is_given(location) else os.environ.get(&#34;GOOGLE_CLOUD_LOCATION&#34;)
        gemini_api_key = api_key if is_given(api_key) else os.environ.get(&#34;GOOGLE_API_KEY&#34;)
        _gac = os.environ.get(&#34;GOOGLE_APPLICATION_CREDENTIALS&#34;)
        if _gac is None:
            logger.warning(
                &#34;`GOOGLE_APPLICATION_CREDENTIALS` environment variable is not set. please set it to the path of the service account key file. Otherwise, use any of the other Google Cloud auth methods.&#34;  # noqa: E501
            )

        if is_given(vertexai) and vertexai:
            if not gcp_project:
                _, gcp_project = default_async(
                    scopes=[&#34;https://www.googleapis.com/auth/cloud-platform&#34;]
                )
            gemini_api_key = None  # VertexAI does not require an API key

        else:
            gcp_project = None
            gcp_location = None
            if not gemini_api_key:
                raise ValueError(
                    &#34;API key is required for Google API either via api_key or GOOGLE_API_KEY environment variable&#34;  # noqa: E501
                )

        # Validate thinking_config
        if is_given(thinking_config):
            _thinking_budget = None
            if isinstance(thinking_config, dict):
                _thinking_budget = thinking_config.get(&#34;thinking_budget&#34;)
            elif isinstance(thinking_config, types.ThinkingConfig):
                _thinking_budget = thinking_config.thinking_budget

            if _thinking_budget is not None:
                if not isinstance(_thinking_budget, int):
                    raise ValueError(&#34;thinking_budget inside thinking_config must be an integer&#34;)
                if not (0 &lt;= _thinking_budget &lt;= 24576):
                    raise ValueError(
                        &#34;thinking_budget inside thinking_config must be between 0 and 24576&#34;
                    )

        self._opts = _LLMOptions(
            model=model,
            temperature=temperature,
            tool_choice=tool_choice,
            vertexai=vertexai,
            project=project,
            location=location,
            max_output_tokens=max_output_tokens,
            top_p=top_p,
            top_k=top_k,
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            thinking_config=thinking_config,
        )
        self._client = genai.Client(
            api_key=gemini_api_key,
            vertexai=is_given(vertexai) and vertexai,
            project=gcp_project,
            location=gcp_location,
        )

    def chat(
        self,
        *,
        chat_ctx: llm.ChatContext,
        tools: list[FunctionTool] | None = None,
        conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
        parallel_tool_calls: NotGivenOr[bool] = NOT_GIVEN,
        tool_choice: NotGivenOr[ToolChoice] = NOT_GIVEN,
        response_format: NotGivenOr[
            types.SchemaUnion | type[llm_utils.ResponseFormatT]
        ] = NOT_GIVEN,
        extra_kwargs: NotGivenOr[dict[str, Any]] = NOT_GIVEN,
    ) -&gt; LLMStream:
        extra = {}

        if is_given(extra_kwargs):
            extra.update(extra_kwargs)

        tool_choice = tool_choice if is_given(tool_choice) else self._opts.tool_choice
        if is_given(tool_choice):
            gemini_tool_choice: types.ToolConfig
            if isinstance(tool_choice, dict) and tool_choice.get(&#34;type&#34;) == &#34;function&#34;:
                gemini_tool_choice = types.ToolConfig(
                    function_calling_config=types.FunctionCallingConfig(
                        mode=&#34;ANY&#34;,
                        allowed_function_names=[tool_choice[&#34;function&#34;][&#34;name&#34;]],
                    )
                )
                extra[&#34;tool_config&#34;] = gemini_tool_choice
            elif tool_choice == &#34;required&#34;:
                gemini_tool_choice = types.ToolConfig(
                    function_calling_config=types.FunctionCallingConfig(
                        mode=&#34;ANY&#34;,
                        allowed_function_names=[get_function_info(fnc).name for fnc in tools]
                        if tools
                        else None,
                    )
                )
                extra[&#34;tool_config&#34;] = gemini_tool_choice
            elif tool_choice == &#34;auto&#34;:
                gemini_tool_choice = types.ToolConfig(
                    function_calling_config=types.FunctionCallingConfig(
                        mode=&#34;AUTO&#34;,
                    )
                )
                extra[&#34;tool_config&#34;] = gemini_tool_choice
            elif tool_choice == &#34;none&#34;:
                gemini_tool_choice = types.ToolConfig(
                    function_calling_config=types.FunctionCallingConfig(
                        mode=&#34;NONE&#34;,
                    )
                )
                extra[&#34;tool_config&#34;] = gemini_tool_choice

        if is_given(response_format):
            extra[&#34;response_schema&#34;] = to_response_format(response_format)
            extra[&#34;response_mime_type&#34;] = &#34;application/json&#34;

        if is_given(self._opts.temperature):
            extra[&#34;temperature&#34;] = self._opts.temperature
        if is_given(self._opts.max_output_tokens):
            extra[&#34;max_output_tokens&#34;] = self._opts.max_output_tokens
        if is_given(self._opts.top_p):
            extra[&#34;top_p&#34;] = self._opts.top_p
        if is_given(self._opts.top_k):
            extra[&#34;top_k&#34;] = self._opts.top_k
        if is_given(self._opts.presence_penalty):
            extra[&#34;presence_penalty&#34;] = self._opts.presence_penalty
        if is_given(self._opts.frequency_penalty):
            extra[&#34;frequency_penalty&#34;] = self._opts.frequency_penalty

        # Add thinking config if thinking_budget is provided
        if is_given(self._opts.thinking_config):
            extra[&#34;thinking_config&#34;] = self._opts.thinking_config

        return LLMStream(
            self,
            client=self._client,
            model=self._opts.model,
            chat_ctx=chat_ctx,
            tools=tools,
            conn_options=conn_options,
            extra_kwargs=extra,
        )</code></pre>
</details>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Create a new instance of Google GenAI LLM.</p>
<p>Environment Requirements:
- For VertexAI: Set the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable to the path of the service account key file.
The Google Cloud project and location can be set via <code>project</code> and <code>location</code> arguments or the environment variables
<code>GOOGLE_CLOUD_PROJECT</code> and <code>GOOGLE_CLOUD_LOCATION</code>. By default, the project is inferred from the service account key file,
and the location defaults to "us-central1".
- For Google Gemini API: Set the <code>api_key</code> argument or the <code>GOOGLE_API_KEY</code> environment variable.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>ChatModels | str</code>, optional</dt>
<dd>The model name to use. Defaults to "gemini-2.0-flash-001".</dd>
<dt><strong><code>api_key</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The API key for Google Gemini. If not provided, it attempts to read from the <code>GOOGLE_API_KEY</code> environment variable.</dd>
<dt><strong><code>vertexai</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use VertexAI. Defaults to False.</dd>
<dt><strong><code>project</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The Google Cloud project to use (only for VertexAI). Defaults to None.</dd>
<dt><strong><code>location</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The location to use for VertexAI API requests. Defaults value is "us-central1".</dd>
<dt><strong><code>temperature</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Sampling temperature for response generation. Defaults to 0.8.</dd>
<dt><strong><code>max_output_tokens</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum number of tokens to generate in the output. Defaults to None.</dd>
<dt><strong><code>top_p</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The nucleus sampling probability for response generation. Defaults to None.</dd>
<dt><strong><code>top_k</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The top-k sampling value for response generation. Defaults to None.</dd>
<dt><strong><code>presence_penalty</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Penalizes the model for generating previously mentioned concepts. Defaults to None.</dd>
<dt><strong><code>frequency_penalty</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Penalizes the model for repeating words. Defaults to None.</dd>
<dt><strong><code>tool_choice</code></strong> :&ensp;<code>ToolChoice</code>, optional</dt>
<dd>Specifies whether to use tools during response generation. Defaults to "auto".</dd>
<dt><strong><code>thinking_config</code></strong> :&ensp;<code>ThinkingConfigOrDict</code>, optional</dt>
<dd>The thinking configuration for response generation. Defaults to None.</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="livekit.agents.llm.llm.LLM" href="../../agents/llm/llm.html#livekit.agents.llm.llm.LLM">LLM</a></li>
<li>abc.ABC</li>
<li><a title="livekit.rtc.event_emitter.EventEmitter" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter">EventEmitter</a></li>
<li>typing.Generic</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="livekit.plugins.google.LLM.chat"><code class="name flex">
<span>def <span class="ident">chat</span></span>(<span>self,<br>*,<br>chat_ctx: llm.ChatContext,<br>tools: list[FunctionTool] | None = None,<br>conn_options: APIConnectOptions = APIConnectOptions(max_retry=3, retry_interval=2.0, timeout=10.0),<br>parallel_tool_calls: NotGivenOr[bool] = NOT_GIVEN,<br>tool_choice: NotGivenOr[ToolChoice] = NOT_GIVEN,<br>response_format: NotGivenOr[types.SchemaUnion | type[llm_utils.ResponseFormatT]] = NOT_GIVEN,<br>extra_kwargs: NotGivenOr[dict[str, Any]] = NOT_GIVEN) ‑> livekit.plugins.google.llm.LLMStream</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def chat(
    self,
    *,
    chat_ctx: llm.ChatContext,
    tools: list[FunctionTool] | None = None,
    conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
    parallel_tool_calls: NotGivenOr[bool] = NOT_GIVEN,
    tool_choice: NotGivenOr[ToolChoice] = NOT_GIVEN,
    response_format: NotGivenOr[
        types.SchemaUnion | type[llm_utils.ResponseFormatT]
    ] = NOT_GIVEN,
    extra_kwargs: NotGivenOr[dict[str, Any]] = NOT_GIVEN,
) -&gt; LLMStream:
    extra = {}

    if is_given(extra_kwargs):
        extra.update(extra_kwargs)

    tool_choice = tool_choice if is_given(tool_choice) else self._opts.tool_choice
    if is_given(tool_choice):
        gemini_tool_choice: types.ToolConfig
        if isinstance(tool_choice, dict) and tool_choice.get(&#34;type&#34;) == &#34;function&#34;:
            gemini_tool_choice = types.ToolConfig(
                function_calling_config=types.FunctionCallingConfig(
                    mode=&#34;ANY&#34;,
                    allowed_function_names=[tool_choice[&#34;function&#34;][&#34;name&#34;]],
                )
            )
            extra[&#34;tool_config&#34;] = gemini_tool_choice
        elif tool_choice == &#34;required&#34;:
            gemini_tool_choice = types.ToolConfig(
                function_calling_config=types.FunctionCallingConfig(
                    mode=&#34;ANY&#34;,
                    allowed_function_names=[get_function_info(fnc).name for fnc in tools]
                    if tools
                    else None,
                )
            )
            extra[&#34;tool_config&#34;] = gemini_tool_choice
        elif tool_choice == &#34;auto&#34;:
            gemini_tool_choice = types.ToolConfig(
                function_calling_config=types.FunctionCallingConfig(
                    mode=&#34;AUTO&#34;,
                )
            )
            extra[&#34;tool_config&#34;] = gemini_tool_choice
        elif tool_choice == &#34;none&#34;:
            gemini_tool_choice = types.ToolConfig(
                function_calling_config=types.FunctionCallingConfig(
                    mode=&#34;NONE&#34;,
                )
            )
            extra[&#34;tool_config&#34;] = gemini_tool_choice

    if is_given(response_format):
        extra[&#34;response_schema&#34;] = to_response_format(response_format)
        extra[&#34;response_mime_type&#34;] = &#34;application/json&#34;

    if is_given(self._opts.temperature):
        extra[&#34;temperature&#34;] = self._opts.temperature
    if is_given(self._opts.max_output_tokens):
        extra[&#34;max_output_tokens&#34;] = self._opts.max_output_tokens
    if is_given(self._opts.top_p):
        extra[&#34;top_p&#34;] = self._opts.top_p
    if is_given(self._opts.top_k):
        extra[&#34;top_k&#34;] = self._opts.top_k
    if is_given(self._opts.presence_penalty):
        extra[&#34;presence_penalty&#34;] = self._opts.presence_penalty
    if is_given(self._opts.frequency_penalty):
        extra[&#34;frequency_penalty&#34;] = self._opts.frequency_penalty

    # Add thinking config if thinking_budget is provided
    if is_given(self._opts.thinking_config):
        extra[&#34;thinking_config&#34;] = self._opts.thinking_config

    return LLMStream(
        self,
        client=self._client,
        model=self._opts.model,
        chat_ctx=chat_ctx,
        tools=tools,
        conn_options=conn_options,
        extra_kwargs=extra,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="livekit.agents.llm.llm.LLM" href="../../agents/llm/llm.html#livekit.agents.llm.llm.LLM">LLM</a></b></code>:
<ul class="hlist">
<li><code><a title="livekit.agents.llm.llm.LLM.emit" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.emit">emit</a></code></li>
<li><code><a title="livekit.agents.llm.llm.LLM.off" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.off">off</a></code></li>
<li><code><a title="livekit.agents.llm.llm.LLM.on" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.on">on</a></code></li>
<li><code><a title="livekit.agents.llm.llm.LLM.once" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.once">once</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="livekit.plugins.google.STT"><code class="flex name class">
<span>class <span class="ident">STT</span></span>
<span>(</span><span>*,<br>languages: LanguageCode = 'en-US',<br>detect_language: bool = True,<br>interim_results: bool = True,<br>punctuate: bool = True,<br>spoken_punctuation: bool = False,<br>model: SpeechModels | str = 'latest_long',<br>location: str = 'global',<br>sample_rate: int = 16000,<br>credentials_info: NotGivenOr[dict] = NOT_GIVEN,<br>credentials_file: NotGivenOr[str] = NOT_GIVEN,<br>keywords: NotGivenOr[list[tuple[str, float]]] = NOT_GIVEN)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class STT(stt.STT):
    def __init__(
        self,
        *,
        languages: LanguageCode = &#34;en-US&#34;,  # Google STT can accept multiple languages
        detect_language: bool = True,
        interim_results: bool = True,
        punctuate: bool = True,
        spoken_punctuation: bool = False,
        model: SpeechModels | str = &#34;latest_long&#34;,
        location: str = &#34;global&#34;,
        sample_rate: int = 16000,
        credentials_info: NotGivenOr[dict] = NOT_GIVEN,
        credentials_file: NotGivenOr[str] = NOT_GIVEN,
        keywords: NotGivenOr[list[tuple[str, float]]] = NOT_GIVEN,
    ):
        &#34;&#34;&#34;
        Create a new instance of Google STT.

        Credentials must be provided, either by using the ``credentials_info`` dict, or reading
        from the file specified in ``credentials_file`` or via Application Default Credentials as
        described in https://cloud.google.com/docs/authentication/application-default-credentials

        args:
            languages(LanguageCode): list of language codes to recognize (default: &#34;en-US&#34;)
            detect_language(bool): whether to detect the language of the audio (default: True)
            interim_results(bool): whether to return interim results (default: True)
            punctuate(bool): whether to punctuate the audio (default: True)
            spoken_punctuation(bool): whether to use spoken punctuation (default: False)
            model(SpeechModels): the model to use for recognition default: &#34;latest_long&#34;
            location(str): the location to use for recognition default: &#34;global&#34;
            sample_rate(int): the sample rate of the audio default: 16000
            credentials_info(dict): the credentials info to use for recognition (default: None)
            credentials_file(str): the credentials file to use for recognition (default: None)
            keywords(List[tuple[str, float]]): list of keywords to recognize (default: None)
        &#34;&#34;&#34;
        super().__init__(capabilities=stt.STTCapabilities(streaming=True, interim_results=True))

        self._location = location
        self._credentials_info = credentials_info
        self._credentials_file = credentials_file

        if not is_given(credentials_file) and not is_given(credentials_info):
            try:
                gauth_default()
            except DefaultCredentialsError:
                raise ValueError(
                    &#34;Application default credentials must be available &#34;
                    &#34;when using Google STT without explicitly passing &#34;
                    &#34;credentials through credentials_info or credentials_file.&#34;
                ) from None

        if isinstance(languages, str):
            languages = [languages]

        self._config = STTOptions(
            languages=languages,
            detect_language=detect_language,
            interim_results=interim_results,
            punctuate=punctuate,
            spoken_punctuation=spoken_punctuation,
            model=model,
            sample_rate=sample_rate,
            keywords=keywords,
        )
        self._streams = weakref.WeakSet[SpeechStream]()
        self._pool = utils.ConnectionPool[SpeechAsyncClient](
            max_session_duration=_max_session_duration,
            connect_cb=self._create_client,
        )

    async def _create_client(self) -&gt; SpeechAsyncClient:
        # Add support for passing a specific location that matches recognizer
        # see: https://cloud.google.com/speech-to-text/v2/docs/speech-to-text-supported-languages
        client_options = None
        client: SpeechAsyncClient | None = None
        if self._location != &#34;global&#34;:
            client_options = ClientOptions(api_endpoint=f&#34;{self._location}-speech.googleapis.com&#34;)
        if is_given(self._credentials_info):
            client = SpeechAsyncClient.from_service_account_info(
                self._credentials_info, client_options=client_options
            )
        elif is_given(self._credentials_file):
            client = SpeechAsyncClient.from_service_account_file(
                self._credentials_file, client_options=client_options
            )
        else:
            client = SpeechAsyncClient(client_options=client_options)
        assert client is not None
        return client

    def _get_recognizer(self, client: SpeechAsyncClient) -&gt; str:
        # TODO(theomonnom): should we use recognizers?
        # recognizers may improve latency https://cloud.google.com/speech-to-text/v2/docs/recognizers#understand_recognizers

        # TODO(theomonnom): find a better way to access the project_id
        try:
            project_id = client.transport._credentials.project_id  # type: ignore
        except AttributeError:
            from google.auth import default as ga_default

            _, project_id = ga_default()
        return f&#34;projects/{project_id}/locations/{self._location}/recognizers/_&#34;

    def _sanitize_options(self, *, language: NotGivenOr[str] = NOT_GIVEN) -&gt; STTOptions:
        config = dataclasses.replace(self._config)

        if is_given(language):
            config.languages = [language]

        if not isinstance(config.languages, list):
            config.languages = [config.languages]
        elif not config.detect_language:
            if len(config.languages) &gt; 1:
                logger.warning(&#34;multiple languages provided, but language detection is disabled&#34;)
            config.languages = [config.languages[0]]

        return config

    async def _recognize_impl(
        self,
        buffer: utils.AudioBuffer,
        *,
        language: NotGivenOr[SpeechLanguages | str] = NOT_GIVEN,
        conn_options: APIConnectOptions,
    ) -&gt; stt.SpeechEvent:
        config = self._sanitize_options(language=language)
        frame = rtc.combine_audio_frames(buffer)

        config = cloud_speech.RecognitionConfig(
            explicit_decoding_config=cloud_speech.ExplicitDecodingConfig(
                encoding=cloud_speech.ExplicitDecodingConfig.AudioEncoding.LINEAR16,
                sample_rate_hertz=frame.sample_rate,
                audio_channel_count=frame.num_channels,
            ),
            adaptation=config.build_adaptation(),
            features=cloud_speech.RecognitionFeatures(
                enable_automatic_punctuation=config.punctuate,
                enable_spoken_punctuation=config.spoken_punctuation,
                enable_word_time_offsets=True,
            ),
            model=config.model,
            language_codes=config.languages,
        )

        try:
            async with self._pool.connection() as client:
                raw = await client.recognize(
                    cloud_speech.RecognizeRequest(
                        recognizer=self._get_recognizer(client),
                        config=config,
                        content=frame.data.tobytes(),
                    ),
                    timeout=conn_options.timeout,
                )

                return _recognize_response_to_speech_event(raw)
        except DeadlineExceeded:
            raise APITimeoutError() from None
        except GoogleAPICallError as e:
            raise APIStatusError(e.message, status_code=e.code or -1) from None
        except Exception as e:
            raise APIConnectionError() from e

    def stream(
        self,
        *,
        language: NotGivenOr[SpeechLanguages | str] = NOT_GIVEN,
        conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
    ) -&gt; SpeechStream:
        config = self._sanitize_options(language=language)
        stream = SpeechStream(
            stt=self,
            pool=self._pool,
            recognizer_cb=self._get_recognizer,
            config=config,
            conn_options=conn_options,
        )
        self._streams.add(stream)
        return stream

    def update_options(
        self,
        *,
        languages: NotGivenOr[LanguageCode] = NOT_GIVEN,
        detect_language: NotGivenOr[bool] = NOT_GIVEN,
        interim_results: NotGivenOr[bool] = NOT_GIVEN,
        punctuate: NotGivenOr[bool] = NOT_GIVEN,
        spoken_punctuation: NotGivenOr[bool] = NOT_GIVEN,
        model: NotGivenOr[SpeechModels] = NOT_GIVEN,
        location: NotGivenOr[str] = NOT_GIVEN,
        keywords: NotGivenOr[list[tuple[str, float]]] = NOT_GIVEN,
    ):
        if is_given(languages):
            if isinstance(languages, str):
                languages = [languages]
            self._config.languages = languages
        if is_given(detect_language):
            self._config.detect_language = detect_language
        if is_given(interim_results):
            self._config.interim_results = interim_results
        if is_given(punctuate):
            self._config.punctuate = punctuate
        if is_given(spoken_punctuation):
            self._config.spoken_punctuation = spoken_punctuation
        if is_given(model):
            self._config.model = model
        if is_given(location):
            self._location = location
            # if location is changed, fetch a new client and recognizer as per the new location
            self._pool.invalidate()
        if is_given(keywords):
            self._config.keywords = keywords

        for stream in self._streams:
            stream.update_options(
                languages=languages,
                detect_language=detect_language,
                interim_results=interim_results,
                punctuate=punctuate,
                spoken_punctuation=spoken_punctuation,
                model=model,
                keywords=keywords,
            )

    async def aclose(self) -&gt; None:
        await self._pool.aclose()
        await super().aclose()</code></pre>
</details>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Create a new instance of Google STT.</p>
<p>Credentials must be provided, either by using the <code>credentials_info</code> dict, or reading
from the file specified in <code>credentials_file</code> or via Application Default Credentials as
described in <a href="https://cloud.google.com/docs/authentication/application-default-credentials">https://cloud.google.com/docs/authentication/application-default-credentials</a></p>
<p>args:
languages(LanguageCode): list of language codes to recognize (default: "en-US")
detect_language(bool): whether to detect the language of the audio (default: True)
interim_results(bool): whether to return interim results (default: True)
punctuate(bool): whether to punctuate the audio (default: True)
spoken_punctuation(bool): whether to use spoken punctuation (default: False)
model(SpeechModels): the model to use for recognition default: "latest_long"
location(str): the location to use for recognition default: "global"
sample_rate(int): the sample rate of the audio default: 16000
credentials_info(dict): the credentials info to use for recognition (default: None)
credentials_file(str): the credentials file to use for recognition (default: None)
keywords(List[tuple[str, float]]): list of keywords to recognize (default: None)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="livekit.agents.stt.stt.STT" href="../../agents/stt/stt.html#livekit.agents.stt.stt.STT">STT</a></li>
<li>abc.ABC</li>
<li><a title="livekit.rtc.event_emitter.EventEmitter" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter">EventEmitter</a></li>
<li>typing.Generic</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="livekit.plugins.google.STT.stream"><code class="name flex">
<span>def <span class="ident">stream</span></span>(<span>self,<br>*,<br>language: NotGivenOr[SpeechLanguages | str] = NOT_GIVEN,<br>conn_options: APIConnectOptions = APIConnectOptions(max_retry=3, retry_interval=2.0, timeout=10.0)) ‑> livekit.plugins.google.stt.SpeechStream</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stream(
    self,
    *,
    language: NotGivenOr[SpeechLanguages | str] = NOT_GIVEN,
    conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
) -&gt; SpeechStream:
    config = self._sanitize_options(language=language)
    stream = SpeechStream(
        stt=self,
        pool=self._pool,
        recognizer_cb=self._get_recognizer,
        config=config,
        conn_options=conn_options,
    )
    self._streams.add(stream)
    return stream</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.google.STT.update_options"><code class="name flex">
<span>def <span class="ident">update_options</span></span>(<span>self,<br>*,<br>languages: NotGivenOr[LanguageCode] = NOT_GIVEN,<br>detect_language: NotGivenOr[bool] = NOT_GIVEN,<br>interim_results: NotGivenOr[bool] = NOT_GIVEN,<br>punctuate: NotGivenOr[bool] = NOT_GIVEN,<br>spoken_punctuation: NotGivenOr[bool] = NOT_GIVEN,<br>model: NotGivenOr[SpeechModels] = NOT_GIVEN,<br>location: NotGivenOr[str] = NOT_GIVEN,<br>keywords: NotGivenOr[list[tuple[str, float]]] = NOT_GIVEN)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_options(
    self,
    *,
    languages: NotGivenOr[LanguageCode] = NOT_GIVEN,
    detect_language: NotGivenOr[bool] = NOT_GIVEN,
    interim_results: NotGivenOr[bool] = NOT_GIVEN,
    punctuate: NotGivenOr[bool] = NOT_GIVEN,
    spoken_punctuation: NotGivenOr[bool] = NOT_GIVEN,
    model: NotGivenOr[SpeechModels] = NOT_GIVEN,
    location: NotGivenOr[str] = NOT_GIVEN,
    keywords: NotGivenOr[list[tuple[str, float]]] = NOT_GIVEN,
):
    if is_given(languages):
        if isinstance(languages, str):
            languages = [languages]
        self._config.languages = languages
    if is_given(detect_language):
        self._config.detect_language = detect_language
    if is_given(interim_results):
        self._config.interim_results = interim_results
    if is_given(punctuate):
        self._config.punctuate = punctuate
    if is_given(spoken_punctuation):
        self._config.spoken_punctuation = spoken_punctuation
    if is_given(model):
        self._config.model = model
    if is_given(location):
        self._location = location
        # if location is changed, fetch a new client and recognizer as per the new location
        self._pool.invalidate()
    if is_given(keywords):
        self._config.keywords = keywords

    for stream in self._streams:
        stream.update_options(
            languages=languages,
            detect_language=detect_language,
            interim_results=interim_results,
            punctuate=punctuate,
            spoken_punctuation=spoken_punctuation,
            model=model,
            keywords=keywords,
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="livekit.agents.stt.stt.STT" href="../../agents/stt/stt.html#livekit.agents.stt.stt.STT">STT</a></b></code>:
<ul class="hlist">
<li><code><a title="livekit.agents.stt.stt.STT.aclose" href="../../agents/stt/stt.html#livekit.agents.stt.stt.STT.aclose">aclose</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STT.emit" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.emit">emit</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STT.off" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.off">off</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STT.on" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.on">on</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STT.once" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.once">once</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="livekit.plugins.google.SpeechStream"><code class="flex name class">
<span>class <span class="ident">SpeechStream</span></span>
<span>(</span><span>*,<br>stt: <a title="livekit.plugins.google.STT" href="#livekit.plugins.google.STT">STT</a>,<br>conn_options: APIConnectOptions,<br>pool: utils.ConnectionPool[SpeechAsyncClient],<br>recognizer_cb: Callable[[SpeechAsyncClient], str],<br>config: STTOptions)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SpeechStream(stt.SpeechStream):
    def __init__(
        self,
        *,
        stt: STT,
        conn_options: APIConnectOptions,
        pool: utils.ConnectionPool[SpeechAsyncClient],
        recognizer_cb: Callable[[SpeechAsyncClient], str],
        config: STTOptions,
    ) -&gt; None:
        super().__init__(stt=stt, conn_options=conn_options, sample_rate=config.sample_rate)

        self._pool = pool
        self._recognizer_cb = recognizer_cb
        self._config = config
        self._reconnect_event = asyncio.Event()
        self._session_connected_at: float = 0

    def update_options(
        self,
        *,
        languages: NotGivenOr[LanguageCode] = NOT_GIVEN,
        detect_language: NotGivenOr[bool] = NOT_GIVEN,
        interim_results: NotGivenOr[bool] = NOT_GIVEN,
        punctuate: NotGivenOr[bool] = NOT_GIVEN,
        spoken_punctuation: NotGivenOr[bool] = NOT_GIVEN,
        model: NotGivenOr[SpeechModels] = NOT_GIVEN,
        keywords: NotGivenOr[list[tuple[str, float]]] = NOT_GIVEN,
    ):
        if is_given(languages):
            if isinstance(languages, str):
                languages = [languages]
            self._config.languages = languages
        if is_given(detect_language):
            self._config.detect_language = detect_language
        if is_given(interim_results):
            self._config.interim_results = interim_results
        if is_given(punctuate):
            self._config.punctuate = punctuate
        if is_given(spoken_punctuation):
            self._config.spoken_punctuation = spoken_punctuation
        if is_given(model):
            self._config.model = model
        if is_given(keywords):
            self._config.keywords = keywords

        self._reconnect_event.set()

    async def _run(self) -&gt; None:
        # google requires a async generator when calling streaming_recognize
        # this function basically convert the queue into a async generator
        async def input_generator(client: SpeechAsyncClient, should_stop: asyncio.Event):
            try:
                # first request should contain the config
                yield cloud_speech.StreamingRecognizeRequest(
                    recognizer=self._recognizer_cb(client),
                    streaming_config=self._streaming_config,
                )

                async for frame in self._input_ch:
                    # when the stream is aborted due to reconnect, this input_generator
                    # needs to stop consuming frames
                    # when the generator stops, the previous gRPC stream will close
                    if should_stop.is_set():
                        return

                    if isinstance(frame, rtc.AudioFrame):
                        yield cloud_speech.StreamingRecognizeRequest(audio=frame.data.tobytes())

            except Exception:
                logger.exception(&#34;an error occurred while streaming input to google STT&#34;)

        async def process_stream(client: SpeechAsyncClient, stream):
            has_started = False
            async for resp in stream:
                if (
                    resp.speech_event_type
                    == cloud_speech.StreamingRecognizeResponse.SpeechEventType.SPEECH_ACTIVITY_BEGIN
                ):
                    self._event_ch.send_nowait(
                        stt.SpeechEvent(type=stt.SpeechEventType.START_OF_SPEECH)
                    )
                    has_started = True

                if (
                    resp.speech_event_type
                    == cloud_speech.StreamingRecognizeResponse.SpeechEventType.SPEECH_EVENT_TYPE_UNSPECIFIED  # noqa: E501
                ):
                    result = resp.results[0]
                    speech_data = _streaming_recognize_response_to_speech_data(resp)
                    if speech_data is None:
                        continue

                    if not result.is_final:
                        self._event_ch.send_nowait(
                            stt.SpeechEvent(
                                type=stt.SpeechEventType.INTERIM_TRANSCRIPT,
                                alternatives=[speech_data],
                            )
                        )
                    else:
                        self._event_ch.send_nowait(
                            stt.SpeechEvent(
                                type=stt.SpeechEventType.FINAL_TRANSCRIPT,
                                alternatives=[speech_data],
                            )
                        )
                        if time.time() - self._session_connected_at &gt; _max_session_duration:
                            logger.debug(
                                &#34;Google STT maximum connection time reached. Reconnecting...&#34;
                            )
                            self._pool.remove(client)
                            if has_started:
                                self._event_ch.send_nowait(
                                    stt.SpeechEvent(type=stt.SpeechEventType.END_OF_SPEECH)
                                )
                                has_started = False
                            self._reconnect_event.set()
                            return

                if (
                    resp.speech_event_type
                    == cloud_speech.StreamingRecognizeResponse.SpeechEventType.SPEECH_ACTIVITY_END
                ):
                    self._event_ch.send_nowait(
                        stt.SpeechEvent(type=stt.SpeechEventType.END_OF_SPEECH)
                    )
                    has_started = False

        while True:
            try:
                async with self._pool.connection() as client:
                    self._streaming_config = cloud_speech.StreamingRecognitionConfig(
                        config=cloud_speech.RecognitionConfig(
                            explicit_decoding_config=cloud_speech.ExplicitDecodingConfig(
                                encoding=cloud_speech.ExplicitDecodingConfig.AudioEncoding.LINEAR16,
                                sample_rate_hertz=self._config.sample_rate,
                                audio_channel_count=1,
                            ),
                            adaptation=self._config.build_adaptation(),
                            language_codes=self._config.languages,
                            model=self._config.model,
                            features=cloud_speech.RecognitionFeatures(
                                enable_automatic_punctuation=self._config.punctuate,
                                enable_word_time_offsets=True,
                            ),
                        ),
                        streaming_features=cloud_speech.StreamingRecognitionFeatures(
                            interim_results=self._config.interim_results,
                        ),
                    )

                    should_stop = asyncio.Event()
                    stream = await client.streaming_recognize(
                        requests=input_generator(client, should_stop),
                    )
                    self._session_connected_at = time.time()

                    process_stream_task = asyncio.create_task(process_stream(client, stream))
                    wait_reconnect_task = asyncio.create_task(self._reconnect_event.wait())

                    try:
                        done, _ = await asyncio.wait(
                            [process_stream_task, wait_reconnect_task],
                            return_when=asyncio.FIRST_COMPLETED,
                        )
                        for task in done:
                            if task != wait_reconnect_task:
                                task.result()
                        if wait_reconnect_task not in done:
                            break
                        self._reconnect_event.clear()
                    finally:
                        await utils.aio.gracefully_cancel(process_stream_task, wait_reconnect_task)
                        should_stop.set()
            except DeadlineExceeded:
                raise APITimeoutError() from None
            except GoogleAPICallError as e:
                raise APIStatusError(e.message, status_code=e.code or -1) from None
            except Exception as e:
                raise APIConnectionError() from e</code></pre>
</details>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Args:
sample_rate : int or None, optional
The desired sample rate for the audio input.
If specified, the audio input will be automatically resampled to match
the given sample rate before being processed for Speech-to-Text.
If not provided (None), the input will retain its original sample rate.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="livekit.agents.stt.stt.RecognizeStream" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream">RecognizeStream</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="livekit.plugins.google.SpeechStream.update_options"><code class="name flex">
<span>def <span class="ident">update_options</span></span>(<span>self,<br>*,<br>languages: NotGivenOr[LanguageCode] = NOT_GIVEN,<br>detect_language: NotGivenOr[bool] = NOT_GIVEN,<br>interim_results: NotGivenOr[bool] = NOT_GIVEN,<br>punctuate: NotGivenOr[bool] = NOT_GIVEN,<br>spoken_punctuation: NotGivenOr[bool] = NOT_GIVEN,<br>model: NotGivenOr[SpeechModels] = NOT_GIVEN,<br>keywords: NotGivenOr[list[tuple[str, float]]] = NOT_GIVEN)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_options(
    self,
    *,
    languages: NotGivenOr[LanguageCode] = NOT_GIVEN,
    detect_language: NotGivenOr[bool] = NOT_GIVEN,
    interim_results: NotGivenOr[bool] = NOT_GIVEN,
    punctuate: NotGivenOr[bool] = NOT_GIVEN,
    spoken_punctuation: NotGivenOr[bool] = NOT_GIVEN,
    model: NotGivenOr[SpeechModels] = NOT_GIVEN,
    keywords: NotGivenOr[list[tuple[str, float]]] = NOT_GIVEN,
):
    if is_given(languages):
        if isinstance(languages, str):
            languages = [languages]
        self._config.languages = languages
    if is_given(detect_language):
        self._config.detect_language = detect_language
    if is_given(interim_results):
        self._config.interim_results = interim_results
    if is_given(punctuate):
        self._config.punctuate = punctuate
    if is_given(spoken_punctuation):
        self._config.spoken_punctuation = spoken_punctuation
    if is_given(model):
        self._config.model = model
    if is_given(keywords):
        self._config.keywords = keywords

    self._reconnect_event.set()</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="livekit.agents.stt.stt.RecognizeStream" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream">RecognizeStream</a></b></code>:
<ul class="hlist">
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.aclose" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream.aclose">aclose</a></code></li>
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.end_input" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream.end_input">end_input</a></code></li>
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.flush" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream.flush">flush</a></code></li>
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.push_frame" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream.push_frame">push_frame</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="livekit.plugins.google.TTS"><code class="flex name class">
<span>class <span class="ident">TTS</span></span>
<span>(</span><span>*,<br>language: NotGivenOr[SpeechLanguages | str] = NOT_GIVEN,<br>gender: NotGivenOr[Gender | str] = NOT_GIVEN,<br>voice_name: NotGivenOr[str] = NOT_GIVEN,<br>sample_rate: int = 24000,<br>pitch: int = 0,<br>effects_profile_id: str = '',<br>speaking_rate: float = 1.0,<br>location: str = 'global',<br>audio_encoding: texttospeech.AudioEncoding = 7,<br>credentials_info: NotGivenOr[dict] = NOT_GIVEN,<br>credentials_file: NotGivenOr[str] = NOT_GIVEN)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TTS(tts.TTS):
    def __init__(
        self,
        *,
        language: NotGivenOr[SpeechLanguages | str] = NOT_GIVEN,
        gender: NotGivenOr[Gender | str] = NOT_GIVEN,
        voice_name: NotGivenOr[str] = NOT_GIVEN,
        sample_rate: int = 24000,
        pitch: int = 0,
        effects_profile_id: str = &#34;&#34;,
        speaking_rate: float = 1.0,
        location: str = &#34;global&#34;,
        audio_encoding: texttospeech.AudioEncoding = texttospeech.AudioEncoding.PCM,
        credentials_info: NotGivenOr[dict] = NOT_GIVEN,
        credentials_file: NotGivenOr[str] = NOT_GIVEN,
    ) -&gt; None:
        &#34;&#34;&#34;
        Create a new instance of Google TTS.

        Credentials must be provided, either by using the ``credentials_info`` dict, or reading
        from the file specified in ``credentials_file`` or the ``GOOGLE_APPLICATION_CREDENTIALS``
        environmental variable.

        Args:
            language (SpeechLanguages | str, optional): Language code (e.g., &#34;en-US&#34;). Default is &#34;en-US&#34;.
            gender (Gender | str, optional): Voice gender (&#34;male&#34;, &#34;female&#34;, &#34;neutral&#34;). Default is &#34;neutral&#34;.
            voice_name (str, optional): Specific voice name. Default is an empty string.
            sample_rate (int, optional): Audio sample rate in Hz. Default is 24000.
            location (str, optional): Location for the TTS client. Default is &#34;global&#34;.
            pitch (float, optional): Speaking pitch, ranging from -20.0 to 20.0 semitones relative to the original pitch. Default is 0.
            effects_profile_id (str): Optional identifier for selecting audio effects profiles to apply to the synthesized speech.
            speaking_rate (float, optional): Speed of speech. Default is 1.0.
            credentials_info (dict, optional): Dictionary containing Google Cloud credentials. Default is None.
            credentials_file (str, optional): Path to the Google Cloud credentials JSON file. Default is None.
        &#34;&#34;&#34;  # noqa: E501

        super().__init__(
            capabilities=tts.TTSCapabilities(
                streaming=False,
            ),
            sample_rate=sample_rate,
            num_channels=1,
        )

        self._client: texttospeech.TextToSpeechAsyncClient | None = None
        self._credentials_info = credentials_info
        self._credentials_file = credentials_file
        self._location = location

        lang = language if is_given(language) else &#34;en-US&#34;
        ssml_gender = _gender_from_str(&#34;neutral&#34; if not is_given(gender) else gender)
        name = &#34;&#34; if not is_given(voice_name) else voice_name

        voice_params = texttospeech.VoiceSelectionParams(
            name=name,
            language_code=lang,
            ssml_gender=ssml_gender,
        )

        self._opts = _TTSOptions(
            voice=voice_params,
            audio_config=texttospeech.AudioConfig(
                audio_encoding=audio_encoding,
                sample_rate_hertz=sample_rate,
                pitch=pitch,
                effects_profile_id=effects_profile_id,
                speaking_rate=speaking_rate,
            ),
        )

    def update_options(
        self,
        *,
        language: NotGivenOr[SpeechLanguages | str] = NOT_GIVEN,
        gender: NotGivenOr[Gender | str] = NOT_GIVEN,
        voice_name: NotGivenOr[str] = NOT_GIVEN,
        speaking_rate: NotGivenOr[float] = NOT_GIVEN,
    ) -&gt; None:
        &#34;&#34;&#34;
        Update the TTS options.

        Args:
            language (SpeechLanguages | str, optional): Language code (e.g., &#34;en-US&#34;).
            gender (Gender | str, optional): Voice gender (&#34;male&#34;, &#34;female&#34;, &#34;neutral&#34;).
            voice_name (str, optional): Specific voice name.
            speaking_rate (float, optional): Speed of speech.
        &#34;&#34;&#34;  # noqa: E501
        params = {}
        if is_given(language):
            params[&#34;language_code&#34;] = str(language)
        if is_given(gender):
            params[&#34;ssml_gender&#34;] = _gender_from_str(str(gender))
        if is_given(voice_name):
            params[&#34;name&#34;] = voice_name

        if params:
            self._opts.voice = texttospeech.VoiceSelectionParams(**params)

        if is_given(speaking_rate):
            self._opts.audio_config.speaking_rate = speaking_rate

    def _ensure_client(self) -&gt; texttospeech.TextToSpeechAsyncClient:
        api_endpoint = &#34;texttospeech.googleapis.com&#34;
        if self._location != &#34;global&#34;:
            api_endpoint = f&#34;{self._location}-texttospeech.googleapis.com&#34;

        if self._client is None:
            if self._credentials_info:
                self._client = texttospeech.TextToSpeechAsyncClient.from_service_account_info(
                    self._credentials_info, client_options=ClientOptions(api_endpoint=api_endpoint)
                )

            elif self._credentials_file:
                self._client = texttospeech.TextToSpeechAsyncClient.from_service_account_file(
                    self._credentials_file, client_options=ClientOptions(api_endpoint=api_endpoint)
                )
            else:
                self._client = texttospeech.TextToSpeechAsyncClient(
                    client_options=ClientOptions(api_endpoint=api_endpoint)
                )

        assert self._client is not None
        return self._client

    def synthesize(
        self,
        text: str,
        *,
        conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
    ) -&gt; ChunkedStream:
        return ChunkedStream(
            tts=self,
            input_text=text,
            conn_options=conn_options,
            opts=self._opts,
            client=self._ensure_client(),
        )</code></pre>
</details>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Create a new instance of Google TTS.</p>
<p>Credentials must be provided, either by using the <code>credentials_info</code> dict, or reading
from the file specified in <code>credentials_file</code> or the <code>GOOGLE_APPLICATION_CREDENTIALS</code>
environmental variable.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>language</code></strong> :&ensp;<code>SpeechLanguages | str</code>, optional</dt>
<dd>Language code (e.g., "en-US"). Default is "en-US".</dd>
<dt><strong><code>gender</code></strong> :&ensp;<code>Gender | str</code>, optional</dt>
<dd>Voice gender ("male", "female", "neutral"). Default is "neutral".</dd>
<dt><strong><code>voice_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Specific voice name. Default is an empty string.</dd>
<dt><strong><code>sample_rate</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Audio sample rate in Hz. Default is 24000.</dd>
<dt><strong><code>location</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Location for the TTS client. Default is "global".</dd>
<dt><strong><code>pitch</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Speaking pitch, ranging from -20.0 to 20.0 semitones relative to the original pitch. Default is 0.</dd>
<dt><strong><code>effects_profile_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Optional identifier for selecting audio effects profiles to apply to the synthesized speech.</dd>
<dt><strong><code>speaking_rate</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Speed of speech. Default is 1.0.</dd>
<dt><strong><code>credentials_info</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Dictionary containing Google Cloud credentials. Default is None.</dd>
<dt><strong><code>credentials_file</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Path to the Google Cloud credentials JSON file. Default is None.</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="livekit.agents.tts.tts.TTS" href="../../agents/tts/tts.html#livekit.agents.tts.tts.TTS">TTS</a></li>
<li>abc.ABC</li>
<li><a title="livekit.rtc.event_emitter.EventEmitter" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter">EventEmitter</a></li>
<li>typing.Generic</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="livekit.plugins.google.TTS.synthesize"><code class="name flex">
<span>def <span class="ident">synthesize</span></span>(<span>self,<br>text: str,<br>*,<br>conn_options: APIConnectOptions = APIConnectOptions(max_retry=3, retry_interval=2.0, timeout=10.0)) ‑> livekit.plugins.google.tts.ChunkedStream</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def synthesize(
    self,
    text: str,
    *,
    conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
) -&gt; ChunkedStream:
    return ChunkedStream(
        tts=self,
        input_text=text,
        conn_options=conn_options,
        opts=self._opts,
        client=self._ensure_client(),
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.google.TTS.update_options"><code class="name flex">
<span>def <span class="ident">update_options</span></span>(<span>self,<br>*,<br>language: NotGivenOr[SpeechLanguages | str] = NOT_GIVEN,<br>gender: NotGivenOr[Gender | str] = NOT_GIVEN,<br>voice_name: NotGivenOr[str] = NOT_GIVEN,<br>speaking_rate: NotGivenOr[float] = NOT_GIVEN) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_options(
    self,
    *,
    language: NotGivenOr[SpeechLanguages | str] = NOT_GIVEN,
    gender: NotGivenOr[Gender | str] = NOT_GIVEN,
    voice_name: NotGivenOr[str] = NOT_GIVEN,
    speaking_rate: NotGivenOr[float] = NOT_GIVEN,
) -&gt; None:
    &#34;&#34;&#34;
    Update the TTS options.

    Args:
        language (SpeechLanguages | str, optional): Language code (e.g., &#34;en-US&#34;).
        gender (Gender | str, optional): Voice gender (&#34;male&#34;, &#34;female&#34;, &#34;neutral&#34;).
        voice_name (str, optional): Specific voice name.
        speaking_rate (float, optional): Speed of speech.
    &#34;&#34;&#34;  # noqa: E501
    params = {}
    if is_given(language):
        params[&#34;language_code&#34;] = str(language)
    if is_given(gender):
        params[&#34;ssml_gender&#34;] = _gender_from_str(str(gender))
    if is_given(voice_name):
        params[&#34;name&#34;] = voice_name

    if params:
        self._opts.voice = texttospeech.VoiceSelectionParams(**params)

    if is_given(speaking_rate):
        self._opts.audio_config.speaking_rate = speaking_rate</code></pre>
</details>
<div class="desc"><p>Update the TTS options.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>language</code></strong> :&ensp;<code>SpeechLanguages | str</code>, optional</dt>
<dd>Language code (e.g., "en-US").</dd>
<dt><strong><code>gender</code></strong> :&ensp;<code>Gender | str</code>, optional</dt>
<dd>Voice gender ("male", "female", "neutral").</dd>
<dt><strong><code>voice_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Specific voice name.</dd>
<dt><strong><code>speaking_rate</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Speed of speech.</dd>
</dl></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="livekit.agents.tts.tts.TTS" href="../../agents/tts/tts.html#livekit.agents.tts.tts.TTS">TTS</a></b></code>:
<ul class="hlist">
<li><code><a title="livekit.agents.tts.tts.TTS.emit" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.emit">emit</a></code></li>
<li><code><a title="livekit.agents.tts.tts.TTS.off" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.off">off</a></code></li>
<li><code><a title="livekit.agents.tts.tts.TTS.on" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.on">on</a></code></li>
<li><code><a title="livekit.agents.tts.tts.TTS.once" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.once">once</a></code></li>
<li><code><a title="livekit.agents.tts.tts.TTS.prewarm" href="../../agents/tts/tts.html#livekit.agents.tts.tts.TTS.prewarm">prewarm</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="livekit.plugins" href="../index.html">livekit.plugins</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="livekit.plugins.google.beta" href="beta/index.html">livekit.plugins.google.beta</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="livekit.plugins.google.LLM" href="#livekit.plugins.google.LLM">LLM</a></code></h4>
<ul class="">
<li><code><a title="livekit.plugins.google.LLM.chat" href="#livekit.plugins.google.LLM.chat">chat</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.plugins.google.STT" href="#livekit.plugins.google.STT">STT</a></code></h4>
<ul class="">
<li><code><a title="livekit.plugins.google.STT.stream" href="#livekit.plugins.google.STT.stream">stream</a></code></li>
<li><code><a title="livekit.plugins.google.STT.update_options" href="#livekit.plugins.google.STT.update_options">update_options</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.plugins.google.SpeechStream" href="#livekit.plugins.google.SpeechStream">SpeechStream</a></code></h4>
<ul class="">
<li><code><a title="livekit.plugins.google.SpeechStream.update_options" href="#livekit.plugins.google.SpeechStream.update_options">update_options</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.plugins.google.TTS" href="#livekit.plugins.google.TTS">TTS</a></code></h4>
<ul class="">
<li><code><a title="livekit.plugins.google.TTS.synthesize" href="#livekit.plugins.google.TTS.synthesize">synthesize</a></code></li>
<li><code><a title="livekit.plugins.google.TTS.update_options" href="#livekit.plugins.google.TTS.update_options">update_options</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
