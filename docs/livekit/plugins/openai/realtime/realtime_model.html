<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>livekit.plugins.openai.realtime.realtime_model API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>livekit.plugins.openai.realtime.realtime_model</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="livekit.plugins.openai.realtime.realtime_model.process_base_url"><code class="name flex">
<span>def <span class="ident">process_base_url</span></span>(<span>url: str,<br>model: str,<br>is_azure: bool = False,<br>azure_deployment: str | None = None,<br>api_version: str | None = None) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_base_url(
    url: str,
    model: str,
    is_azure: bool = False,
    azure_deployment: str | None = None,
    api_version: str | None = None,
) -&gt; str:
    if url.startswith(&#34;http&#34;):
        url = url.replace(&#34;http&#34;, &#34;ws&#34;, 1)

    parsed_url = urlparse(url)
    query_params = parse_qs(parsed_url.query)

    # ensure &#34;/realtime&#34; is added if the path is empty OR &#34;/v1&#34;
    if not parsed_url.path or parsed_url.path.rstrip(&#34;/&#34;) in [&#34;&#34;, &#34;/v1&#34;, &#34;/openai&#34;]:
        path = parsed_url.path.rstrip(&#34;/&#34;) + &#34;/realtime&#34;
    else:
        path = parsed_url.path

    if is_azure:
        if api_version:
            query_params[&#34;api-version&#34;] = [api_version]
        if azure_deployment:
            query_params[&#34;deployment&#34;] = [azure_deployment]

    else:
        if &#34;model&#34; not in query_params:
            query_params[&#34;model&#34;] = [model]

    new_query = urlencode(query_params, doseq=True)
    new_url = urlunparse((parsed_url.scheme, parsed_url.netloc, path, &#34;&#34;, new_query, &#34;&#34;))

    return new_url</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeModel"><code class="flex name class">
<span>class <span class="ident">RealtimeModel</span></span>
<span>(</span><span>*,<br>model: str = 'gpt-4o-realtime-preview',<br>voice: str = 'alloy',<br>temperature: NotGivenOr[float] = NOT_GIVEN,<br>tool_choice: NotGivenOr[llm.ToolChoice | None] = NOT_GIVEN,<br>base_url: NotGivenOr[str] = NOT_GIVEN,<br>input_audio_transcription: NotGivenOr[InputAudioTranscription | None] = NOT_GIVEN,<br>turn_detection: NotGivenOr[TurnDetection | None] = NOT_GIVEN,<br>api_key: str | None = None,<br>http_session: aiohttp.ClientSession | None = None,<br>azure_deployment: str | None = None,<br>entra_token: str | None = None,<br>api_version: str | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RealtimeModel(llm.RealtimeModel):
    @overload
    def __init__(
        self,
        *,
        model: str = &#34;gpt-4o-realtime-preview&#34;,
        voice: str = &#34;alloy&#34;,
        input_audio_transcription: NotGivenOr[InputAudioTranscription | None] = NOT_GIVEN,
        turn_detection: NotGivenOr[TurnDetection | None] = NOT_GIVEN,
        temperature: NotGivenOr[float] = NOT_GIVEN,
        tool_choice: NotGivenOr[llm.ToolChoice | None] = NOT_GIVEN,
        api_key: str | None = None,
        base_url: str | None = None,
        http_session: aiohttp.ClientSession | None = None,
    ) -&gt; None: ...

    @overload
    def __init__(
        self,
        *,
        azure_deployment: str | None = None,
        entra_token: str | None = None,
        api_key: str | None = None,
        api_version: str | None = None,
        base_url: str | None = None,
        voice: str = &#34;alloy&#34;,
        input_audio_transcription: NotGivenOr[InputAudioTranscription | None] = NOT_GIVEN,
        turn_detection: NotGivenOr[TurnDetection | None] = NOT_GIVEN,
        temperature: NotGivenOr[float] = NOT_GIVEN,
        tool_choice: NotGivenOr[llm.ToolChoice | None] = NOT_GIVEN,
        http_session: aiohttp.ClientSession | None = None,
    ) -&gt; None: ...

    def __init__(
        self,
        *,
        model: str = &#34;gpt-4o-realtime-preview&#34;,
        voice: str = &#34;alloy&#34;,
        temperature: NotGivenOr[float] = NOT_GIVEN,
        tool_choice: NotGivenOr[llm.ToolChoice | None] = NOT_GIVEN,
        base_url: NotGivenOr[str] = NOT_GIVEN,
        input_audio_transcription: NotGivenOr[InputAudioTranscription | None] = NOT_GIVEN,
        turn_detection: NotGivenOr[TurnDetection | None] = NOT_GIVEN,
        api_key: str | None = None,
        http_session: aiohttp.ClientSession | None = None,
        azure_deployment: str | None = None,
        entra_token: str | None = None,
        api_version: str | None = None,
    ) -&gt; None:
        super().__init__(
            capabilities=llm.RealtimeCapabilities(
                message_truncation=True,
                turn_detection=turn_detection is not None,
                user_transcription=input_audio_transcription is not None,
            )
        )

        is_azure = (
            api_version is not None or entra_token is not None or azure_deployment is not None
        )

        api_key = api_key or os.environ.get(&#34;OPENAI_API_KEY&#34;)
        if api_key is None and not is_azure:
            raise ValueError(
                &#34;The api_key client option must be set either by passing api_key &#34;
                &#34;to the client or by setting the OPENAI_API_KEY environment variable&#34;
            )

        if is_given(base_url):
            base_url_val = base_url
        else:
            if is_azure:
                azure_endpoint = os.getenv(&#34;AZURE_OPENAI_ENDPOINT&#34;)
                if azure_endpoint is None:
                    raise ValueError(
                        &#34;Missing Azure endpoint. Please pass base_url &#34;
                        &#34;or set AZURE_OPENAI_ENDPOINT environment variable.&#34;
                    )
                base_url_val = f&#34;{azure_endpoint.rstrip(&#39;/&#39;)}/openai&#34;
            else:
                base_url_val = OPENAI_BASE_URL

        self._opts = _RealtimeOptions(
            model=model,
            voice=voice,
            temperature=temperature if is_given(temperature) else DEFAULT_TEMPERATURE,
            tool_choice=tool_choice or None,
            input_audio_transcription=input_audio_transcription
            if is_given(input_audio_transcription)
            else DEFAULT_INPUT_AUDIO_TRANSCRIPTION,
            turn_detection=turn_detection if is_given(turn_detection) else DEFAULT_TURN_DETECTION,
            api_key=api_key,
            base_url=base_url_val,
            is_azure=is_azure,
            azure_deployment=azure_deployment,
            entra_token=entra_token,
            api_version=api_version,
        )
        self._http_session = http_session
        self._sessions = weakref.WeakSet[RealtimeSession]()

    @classmethod
    def with_azure(
        cls,
        *,
        azure_deployment: str,
        azure_endpoint: str | None = None,
        api_version: str | None = None,
        api_key: str | None = None,
        entra_token: str | None = None,
        base_url: str | None = None,
        voice: str = &#34;alloy&#34;,
        input_audio_transcription: NotGivenOr[InputAudioTranscription | None] = NOT_GIVEN,
        turn_detection: NotGivenOr[TurnDetection | None] = NOT_GIVEN,
        temperature: float = 0.8,
        http_session: aiohttp.ClientSession | None = None,
    ):
        &#34;&#34;&#34;
        Create a RealtimeClient instance configured for Azure OpenAI Service.

        Args:
            azure_deployment (str): The name of your Azure OpenAI deployment.
            azure_endpoint (str or None, optional): The endpoint URL for your Azure OpenAI resource. If None, will attempt to read from the environment variable AZURE_OPENAI_ENDPOINT.
            api_version (str or None, optional): API version to use with Azure OpenAI Service. If None, will attempt to read from the environment variable OPENAI_API_VERSION.
            api_key (str or None, optional): Azure OpenAI API key. If None, will attempt to read from the environment variable AZURE_OPENAI_API_KEY.
            entra_token (str or None, optional): Azure Entra authentication token. Required if not using API key authentication.
            base_url (str or None, optional): Base URL for the API endpoint. If None, constructed from the azure_endpoint.
            voice (api_proto.Voice, optional): Voice setting for audio outputs. Defaults to &#34;alloy&#34;.
            input_audio_transcription (InputTranscriptionOptions, optional): Options for transcribing input audio. Defaults to DEFAULT_INPUT_AUDIO_TRANSCRIPTION.
            turn_detection (ServerVadOptions, optional): Options for server-based voice activity detection (VAD). Defaults to DEFAULT_SERVER_VAD_OPTIONS.
            temperature (float, optional): Sampling temperature for response generation. Defaults to 0.8.
            max_response_output_tokens (int or Literal[&#34;inf&#34;], optional): Maximum number of tokens in the response. Defaults to &#34;inf&#34;.
            http_session (aiohttp.ClientSession or None, optional): Async HTTP session to use for requests. If None, a new session will be created.

        Returns:
            RealtimeClient: An instance of RealtimeClient configured for Azure OpenAI Service.

        Raises:
            ValueError: If required Azure parameters are missing or invalid.
        &#34;&#34;&#34;  # noqa: E501
        api_key = api_key or os.getenv(&#34;AZURE_OPENAI_API_KEY&#34;)
        if api_key is None and entra_token is None:
            raise ValueError(
                &#34;Missing credentials. Please pass one of `api_key`, `entra_token`, &#34;
                &#34;or the `AZURE_OPENAI_API_KEY` environment variable.&#34;
            )

        api_version = api_version or os.getenv(&#34;OPENAI_API_VERSION&#34;)
        if api_version is None:
            raise ValueError(
                &#34;Must provide either the `api_version` argument or the &#34;
                &#34;`OPENAI_API_VERSION` environment variable&#34;
            )

        if base_url is None:
            azure_endpoint = azure_endpoint or os.getenv(&#34;AZURE_OPENAI_ENDPOINT&#34;)
            if azure_endpoint is None:
                raise ValueError(
                    &#34;Missing Azure endpoint. Please pass the `azure_endpoint` &#34;
                    &#34;parameter or set the `AZURE_OPENAI_ENDPOINT` environment variable.&#34;
                )

            base_url = f&#34;{azure_endpoint.rstrip(&#39;/&#39;)}/openai&#34;
        elif azure_endpoint is not None:
            raise ValueError(&#34;base_url and azure_endpoint are mutually exclusive&#34;)

        if not is_given(input_audio_transcription):
            input_audio_transcription = AZURE_DEFAULT_INPUT_AUDIO_TRANSCRIPTION

        if not is_given(turn_detection):
            turn_detection = AZURE_DEFAULT_TURN_DETECTION

        return cls(
            voice=voice,
            input_audio_transcription=input_audio_transcription,
            turn_detection=turn_detection,
            temperature=temperature,
            api_key=api_key,
            http_session=http_session,
            azure_deployment=azure_deployment,
            api_version=api_version,
            entra_token=entra_token,
            base_url=base_url,
        )

    def update_options(
        self,
        *,
        voice: NotGivenOr[str] = NOT_GIVEN,
        temperature: NotGivenOr[float] = NOT_GIVEN,
        turn_detection: NotGivenOr[TurnDetection | None] = NOT_GIVEN,
        tool_choice: NotGivenOr[llm.ToolChoice | None] = NOT_GIVEN,
    ) -&gt; None:
        if is_given(voice):
            self._opts.voice = voice

        if is_given(temperature):
            self._opts.temperature = temperature

        if is_given(turn_detection):
            self._opts.turn_detection = turn_detection

        if is_given(tool_choice):
            self._opts.tool_choice = tool_choice

        for sess in self._sessions:
            sess.update_options(
                voice=voice,
                temperature=temperature,
                turn_detection=turn_detection,
                tool_choice=tool_choice,
            )

    def _ensure_http_session(self) -&gt; aiohttp.ClientSession:
        if not self._http_session:
            self._http_session = utils.http_context.http_session()

        return self._http_session

    def session(self) -&gt; RealtimeSession:
        sess = RealtimeSession(self)
        self._sessions.add(sess)
        return sess

    async def aclose(self) -&gt; None: ...</code></pre>
</details>
<div class="desc"></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="livekit.agents.llm.realtime.RealtimeModel" href="../../../agents/llm/realtime.html#livekit.agents.llm.realtime.RealtimeModel">RealtimeModel</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeModel.with_azure"><code class="name flex">
<span>def <span class="ident">with_azure</span></span>(<span>*,<br>azure_deployment: str,<br>azure_endpoint: str | None = None,<br>api_version: str | None = None,<br>api_key: str | None = None,<br>entra_token: str | None = None,<br>base_url: str | None = None,<br>voice: str = 'alloy',<br>input_audio_transcription: NotGivenOr[InputAudioTranscription | None] = NOT_GIVEN,<br>turn_detection: NotGivenOr[TurnDetection | None] = NOT_GIVEN,<br>temperature: float = 0.8,<br>http_session: aiohttp.ClientSession | None = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a RealtimeClient instance configured for Azure OpenAI Service.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>azure_deployment</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of your Azure OpenAI deployment.</dd>
<dt><strong><code>azure_endpoint</code></strong> :&ensp;<code>str</code> or <code>None</code>, optional</dt>
<dd>The endpoint URL for your Azure OpenAI resource. If None, will attempt to read from the environment variable AZURE_OPENAI_ENDPOINT.</dd>
<dt><strong><code>api_version</code></strong> :&ensp;<code>str</code> or <code>None</code>, optional</dt>
<dd>API version to use with Azure OpenAI Service. If None, will attempt to read from the environment variable OPENAI_API_VERSION.</dd>
<dt><strong><code>api_key</code></strong> :&ensp;<code>str</code> or <code>None</code>, optional</dt>
<dd>Azure OpenAI API key. If None, will attempt to read from the environment variable AZURE_OPENAI_API_KEY.</dd>
<dt><strong><code>entra_token</code></strong> :&ensp;<code>str</code> or <code>None</code>, optional</dt>
<dd>Azure Entra authentication token. Required if not using API key authentication.</dd>
<dt><strong><code>base_url</code></strong> :&ensp;<code>str</code> or <code>None</code>, optional</dt>
<dd>Base URL for the API endpoint. If None, constructed from the azure_endpoint.</dd>
<dt><strong><code>voice</code></strong> :&ensp;<code>api_proto.Voice</code>, optional</dt>
<dd>Voice setting for audio outputs. Defaults to "alloy".</dd>
<dt><strong><code>input_audio_transcription</code></strong> :&ensp;<code>InputTranscriptionOptions</code>, optional</dt>
<dd>Options for transcribing input audio. Defaults to DEFAULT_INPUT_AUDIO_TRANSCRIPTION.</dd>
<dt><strong><code>turn_detection</code></strong> :&ensp;<code>ServerVadOptions</code>, optional</dt>
<dd>Options for server-based voice activity detection (VAD). Defaults to DEFAULT_SERVER_VAD_OPTIONS.</dd>
<dt><strong><code>temperature</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Sampling temperature for response generation. Defaults to 0.8.</dd>
<dt>max_response_output_tokens (int or Literal["inf"], optional): Maximum number of tokens in the response. Defaults to "inf".</dt>
<dt><strong><code>http_session</code></strong> :&ensp;<code>aiohttp.ClientSession</code> or <code>None</code>, optional</dt>
<dd>Async HTTP session to use for requests. If None, a new session will be created.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>RealtimeClient</code></dt>
<dd>An instance of RealtimeClient configured for Azure OpenAI Service.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If required Azure parameters are missing or invalid.</dd>
</dl></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeModel.aclose"><code class="name flex">
<span>async def <span class="ident">aclose</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def aclose(self) -&gt; None: ...</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeModel.session"><code class="name flex">
<span>def <span class="ident">session</span></span>(<span>self) ‑> <a title="livekit.plugins.openai.realtime.realtime_model.RealtimeSession" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeSession">RealtimeSession</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def session(self) -&gt; RealtimeSession:
    sess = RealtimeSession(self)
    self._sessions.add(sess)
    return sess</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeModel.update_options"><code class="name flex">
<span>def <span class="ident">update_options</span></span>(<span>self,<br>*,<br>voice: NotGivenOr[str] = NOT_GIVEN,<br>temperature: NotGivenOr[float] = NOT_GIVEN,<br>turn_detection: NotGivenOr[TurnDetection | None] = NOT_GIVEN,<br>tool_choice: NotGivenOr[llm.ToolChoice | None] = NOT_GIVEN) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_options(
    self,
    *,
    voice: NotGivenOr[str] = NOT_GIVEN,
    temperature: NotGivenOr[float] = NOT_GIVEN,
    turn_detection: NotGivenOr[TurnDetection | None] = NOT_GIVEN,
    tool_choice: NotGivenOr[llm.ToolChoice | None] = NOT_GIVEN,
) -&gt; None:
    if is_given(voice):
        self._opts.voice = voice

    if is_given(temperature):
        self._opts.temperature = temperature

    if is_given(turn_detection):
        self._opts.turn_detection = turn_detection

    if is_given(tool_choice):
        self._opts.tool_choice = tool_choice

    for sess in self._sessions:
        sess.update_options(
            voice=voice,
            temperature=temperature,
            turn_detection=turn_detection,
            tool_choice=tool_choice,
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeSession"><code class="flex name class">
<span>class <span class="ident">RealtimeSession</span></span>
<span>(</span><span>realtime_model: <a title="livekit.plugins.openai.realtime.realtime_model.RealtimeModel" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeModel">RealtimeModel</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RealtimeSession(
    llm.RealtimeSession[Literal[&#34;openai_server_event_received&#34;, &#34;openai_client_event_queued&#34;]]
):
    &#34;&#34;&#34;
    A session for the OpenAI Realtime API.

    This class is used to interact with the OpenAI Realtime API.
    It is responsible for sending events to the OpenAI Realtime API and receiving events from it.

    It exposes two more events:
    - openai_server_event_received: expose the raw server events from the OpenAI Realtime API
    - openai_client_event_queued: expose the raw client events sent to the OpenAI Realtime API
    &#34;&#34;&#34;

    def __init__(self, realtime_model: RealtimeModel) -&gt; None:
        super().__init__(realtime_model)
        self._realtime_model = realtime_model
        self._tools = llm.ToolContext.empty()
        self._msg_ch = utils.aio.Chan[Union[RealtimeClientEvent, dict]]()
        self._input_resampler: rtc.AudioResampler | None = None

        self._main_atask = asyncio.create_task(self._main_task(), name=&#34;RealtimeSession._main_task&#34;)
        self._initial_session_update()

        self._response_created_futures: dict[str, _CreateResponseHandle] = {}
        self._text_mode_recovery_atask: asyncio.Task | None = None
        self._text_mode_recovery_retries: int = 0

        self._item_delete_future: dict[str, asyncio.Future] = {}
        self._item_create_future: dict[str, asyncio.Future] = {}

        self._current_generation: _ResponseGeneration | None = None
        self._remote_chat_ctx = llm.remote_chat_context.RemoteChatContext()

        self._update_chat_ctx_lock = asyncio.Lock()
        self._update_fnc_ctx_lock = asyncio.Lock()

        # 100ms chunks
        self._bstream = utils.audio.AudioByteStream(
            SAMPLE_RATE, NUM_CHANNELS, samples_per_channel=SAMPLE_RATE // 10
        )
        self._pushed_duration_s = 0  # duration of audio pushed to the OpenAI Realtime API

    def send_event(self, event: RealtimeClientEvent | dict) -&gt; None:
        with contextlib.suppress(utils.aio.channel.ChanClosed):
            self._msg_ch.send_nowait(event)

    @utils.log_exceptions(logger=logger)
    async def _main_task(self) -&gt; None:
        headers = {&#34;User-Agent&#34;: &#34;LiveKit Agents&#34;}
        if self._realtime_model._opts.is_azure:
            if self._realtime_model._opts.entra_token:
                headers[&#34;Authorization&#34;] = f&#34;Bearer {self._realtime_model._opts.entra_token}&#34;

            if self._realtime_model._opts.api_key:
                headers[&#34;api-key&#34;] = self._realtime_model._opts.api_key
        else:
            headers[&#34;Authorization&#34;] = f&#34;Bearer {self._realtime_model._opts.api_key}&#34;
            headers[&#34;OpenAI-Beta&#34;] = &#34;realtime=v1&#34;

        url = process_base_url(
            self._realtime_model._opts.base_url,
            self._realtime_model._opts.model,
            is_azure=self._realtime_model._opts.is_azure,
            api_version=self._realtime_model._opts.api_version,
            azure_deployment=self._realtime_model._opts.azure_deployment,
        )

        if _log_oai_events:
            logger.debug(f&#34;connecting to Realtime API: {url}&#34;)

        ws_conn = await self._realtime_model._ensure_http_session().ws_connect(
            url=url, headers=headers
        )

        closing = False

        @utils.log_exceptions(logger=logger)
        async def _send_task() -&gt; None:
            nonlocal closing
            async for msg in self._msg_ch:
                try:
                    if isinstance(msg, BaseModel):
                        msg = msg.model_dump(
                            by_alias=True, exclude_unset=True, exclude_defaults=False
                        )

                    self.emit(&#34;openai_client_event_queued&#34;, msg)
                    await ws_conn.send_str(json.dumps(msg))

                    if _log_oai_events:
                        msg_copy = msg.copy()
                        if msg_copy[&#34;type&#34;] == &#34;input_audio_buffer.append&#34;:
                            msg_copy = {**msg_copy, &#34;audio&#34;: &#34;...&#34;}

                        logger.debug(f&#34;&gt;&gt;&gt; {msg_copy}&#34;)
                except Exception:
                    break

            closing = True
            await ws_conn.close()

        @utils.log_exceptions(logger=logger)
        async def _recv_task() -&gt; None:
            while True:
                msg = await ws_conn.receive()
                if msg.type == aiohttp.WSMsgType.CLOSED:
                    if not closing:
                        error = Exception(&#34;OpenAI S2S connection closed unexpectedly&#34;)
                        self.emit(
                            &#34;error&#34;,
                            llm.RealtimeModelError(
                                timestamp=time.time(),
                                label=self._realtime_model._label,
                                error=APIConnectionError(
                                    message=&#34;OpenAI S2S connection closed unexpectedly&#34;,
                                ),
                                recoverable=False,
                            ),
                        )
                        raise error

                    return
                elif msg.type != aiohttp.WSMsgType.TEXT:
                    continue

                event = json.loads(msg.data)

                # emit the raw json dictionary instead of the BaseModel because different
                # providers can have different event types that are not part of the OpenAI Realtime API  # noqa: E501
                self.emit(&#34;openai_server_event_received&#34;, event)

                try:
                    if _log_oai_events:
                        event_copy = event.copy()
                        if event_copy[&#34;type&#34;] == &#34;response.audio.delta&#34;:
                            event_copy = {**event_copy, &#34;delta&#34;: &#34;...&#34;}

                        logger.debug(f&#34;&lt;&lt;&lt; {event_copy}&#34;)

                    if event[&#34;type&#34;] == &#34;input_audio_buffer.speech_started&#34;:
                        self._handle_input_audio_buffer_speech_started(
                            InputAudioBufferSpeechStartedEvent.construct(**event)
                        )
                    elif event[&#34;type&#34;] == &#34;input_audio_buffer.speech_stopped&#34;:
                        self._handle_input_audio_buffer_speech_stopped(
                            InputAudioBufferSpeechStoppedEvent.construct(**event)
                        )
                    elif event[&#34;type&#34;] == &#34;response.created&#34;:
                        self._handle_response_created(ResponseCreatedEvent.construct(**event))
                    elif event[&#34;type&#34;] == &#34;response.output_item.added&#34;:
                        self._handle_response_output_item_added(
                            ResponseOutputItemAddedEvent.construct(**event)
                        )
                    elif event[&#34;type&#34;] == &#34;conversation.item.created&#34;:
                        self._handle_conversion_item_created(
                            ConversationItemCreatedEvent.construct(**event)
                        )
                    elif event[&#34;type&#34;] == &#34;conversation.item.deleted&#34;:
                        self._handle_conversion_item_deleted(
                            ConversationItemDeletedEvent.construct(**event)
                        )
                    elif event[&#34;type&#34;] == &#34;conversation.item.input_audio_transcription.completed&#34;:
                        self._handle_conversion_item_input_audio_transcription_completed(
                            ConversationItemInputAudioTranscriptionCompletedEvent.construct(**event)
                        )
                    elif event[&#34;type&#34;] == &#34;conversation.item.input_audio_transcription.failed&#34;:
                        self._handle_conversion_item_input_audio_transcription_failed(
                            ConversationItemInputAudioTranscriptionFailedEvent.construct(**event)
                        )
                    elif event[&#34;type&#34;] == &#34;response.content_part.added&#34;:
                        self._handle_response_content_part_added(
                            ResponseContentPartAddedEvent.construct(**event)
                        )
                    elif event[&#34;type&#34;] == &#34;response.content_part.done&#34;:
                        self._handle_response_content_part_done(
                            ResponseContentPartDoneEvent.construct(**event)
                        )
                    elif event[&#34;type&#34;] == &#34;response.audio_transcript.delta&#34;:
                        self._handle_response_audio_transcript_delta(event)
                    elif event[&#34;type&#34;] == &#34;response.audio.delta&#34;:
                        self._handle_response_audio_delta(
                            ResponseAudioDeltaEvent.construct(**event)
                        )
                    elif event[&#34;type&#34;] == &#34;response.audio_transcript.done&#34;:
                        self._handle_response_audio_transcript_done(
                            ResponseAudioTranscriptDoneEvent.construct(**event)
                        )
                    elif event[&#34;type&#34;] == &#34;response.audio.done&#34;:
                        self._handle_response_audio_done(ResponseAudioDoneEvent.construct(**event))
                    elif event[&#34;type&#34;] == &#34;response.output_item.done&#34;:
                        self._handle_response_output_item_done(
                            ResponseOutputItemDoneEvent.construct(**event)
                        )
                    elif event[&#34;type&#34;] == &#34;response.done&#34;:
                        self._handle_response_done(ResponseDoneEvent.construct(**event))
                    elif event[&#34;type&#34;] == &#34;error&#34;:
                        self._handle_error(ErrorEvent.construct(**event))
                except Exception:
                    if event[&#34;type&#34;] == &#34;response.audio.delta&#34;:
                        event[&#34;delta&#34;] = event[&#34;delta&#34;][:10] + &#34;...&#34;
                    logger.exception(&#34;failed to handle event&#34;, extra={&#34;event&#34;: event})

        tasks = [
            asyncio.create_task(_recv_task(), name=&#34;_recv_task&#34;),
            asyncio.create_task(_send_task(), name=&#34;_send_task&#34;),
        ]
        try:
            await asyncio.gather(*tasks)
        finally:
            await utils.aio.cancel_and_wait(*tasks)
            await ws_conn.close()

    def _initial_session_update(self) -&gt; None:
        input_audio_transcription = self._realtime_model._opts.input_audio_transcription
        input_audio_transcription = (
            session_update_event.SessionInputAudioTranscription.model_validate(
                input_audio_transcription.model_dump(
                    by_alias=True,
                    exclude_unset=True,
                    exclude_defaults=True,
                )
            )
            if input_audio_transcription
            else None
        )

        turn_detection = self._realtime_model._opts.turn_detection
        turn_detection = (
            session_update_event.SessionTurnDetection.model_validate(
                turn_detection.model_dump(
                    by_alias=True,
                    exclude_unset=True,
                    exclude_defaults=True,
                )
            )
            if turn_detection
            else None
        )

        # initial session update
        self.send_event(
            SessionUpdateEvent(
                type=&#34;session.update&#34;,
                # Using model_construct since OpenAI restricts voices to those defined in the BaseModel.  # noqa: E501
                # Other providers support different voices, so we need to accommodate that.
                session=session_update_event.Session.model_construct(
                    model=self._realtime_model._opts.model,
                    voice=self._realtime_model._opts.voice,
                    input_audio_format=&#34;pcm16&#34;,
                    output_audio_format=&#34;pcm16&#34;,
                    modalities=[&#34;text&#34;, &#34;audio&#34;],
                    turn_detection=turn_detection,
                    input_audio_transcription=input_audio_transcription,
                    temperature=self._realtime_model._opts.temperature,
                ),
                event_id=utils.shortuuid(&#34;session_update_&#34;),
            )
        )

    @property
    def chat_ctx(self) -&gt; llm.ChatContext:
        return self._remote_chat_ctx.to_chat_ctx()

    @property
    def tools(self) -&gt; llm.ToolContext:
        return self._tools.copy()

    def update_options(
        self,
        *,
        tool_choice: NotGivenOr[llm.ToolChoice | None] = NOT_GIVEN,
        voice: NotGivenOr[str] = NOT_GIVEN,
        temperature: NotGivenOr[float] = NOT_GIVEN,
        turn_detection: NotGivenOr[TurnDetection | None] = NOT_GIVEN,
    ) -&gt; None:
        kwargs = {}

        if is_given(tool_choice):
            oai_tool_choice = tool_choice
            if isinstance(tool_choice, dict) and tool_choice[&#34;type&#34;] == &#34;function&#34;:
                oai_tool_choice = tool_choice[&#34;function&#34;]
            if oai_tool_choice is None:
                oai_tool_choice = DEFAULT_TOOL_CHOICE

            kwargs[&#34;tool_choice&#34;] = oai_tool_choice

        if is_given(voice):
            kwargs[&#34;voice&#34;] = voice

        if is_given(temperature):
            kwargs[&#34;temperature&#34;] = temperature

        if is_given(turn_detection):
            kwargs[&#34;turn_detection&#34;] = turn_detection

        if kwargs:
            self.send_event(
                SessionUpdateEvent(
                    type=&#34;session.update&#34;,
                    session=session_update_event.Session.model_construct(**kwargs),
                    event_id=utils.shortuuid(&#34;options_update_&#34;),
                )
            )

    async def update_chat_ctx(
        self, chat_ctx: llm.ChatContext, *, _add_mock_audio: bool = False
    ) -&gt; None:
        chat_ctx = chat_ctx.copy()
        if _add_mock_audio:
            chat_ctx.items.append(_create_mock_audio_item())
        else:
            # clean up existing mock audio items
            chat_ctx.items[:] = [
                item for item in chat_ctx.items if not item.id.startswith(_MOCK_AUDIO_ID_PREFIX)
            ]

        async with self._update_chat_ctx_lock:
            diff_ops = llm.utils.compute_chat_ctx_diff(
                self._remote_chat_ctx.to_chat_ctx(), chat_ctx
            )

            futs = []

            for msg_id in diff_ops.to_remove:
                event_id = utils.shortuuid(&#34;chat_ctx_delete_&#34;)
                self.send_event(
                    ConversationItemDeleteEvent(
                        type=&#34;conversation.item.delete&#34;,
                        item_id=msg_id,
                        event_id=event_id,
                    )
                )
                futs.append(f := asyncio.Future())
                self._item_delete_future[msg_id] = f

            for previous_msg_id, msg_id in diff_ops.to_create:
                event_id = utils.shortuuid(&#34;chat_ctx_create_&#34;)
                chat_item = chat_ctx.get_by_id(msg_id)
                assert chat_item is not None

                self.send_event(
                    ConversationItemCreateEvent(
                        type=&#34;conversation.item.create&#34;,
                        item=_livekit_item_to_openai_item(chat_item),
                        previous_item_id=(&#34;root&#34; if previous_msg_id is None else previous_msg_id),
                        event_id=event_id,
                    )
                )
                futs.append(f := asyncio.Future())
                self._item_create_future[msg_id] = f

            try:
                await asyncio.wait_for(asyncio.gather(*futs, return_exceptions=True), timeout=5.0)
            except asyncio.TimeoutError:
                raise llm.RealtimeError(&#34;update_chat_ctx timed out.&#34;) from None

    async def update_tools(self, tools: list[llm.FunctionTool | llm.RawFunctionTool]) -&gt; None:
        async with self._update_fnc_ctx_lock:
            oai_tools: list[session_update_event.SessionTool] = []
            retained_tools: list[llm.FunctionTool | llm.RawFunctionTool] = []

            for tool in tools:
                if is_function_tool(tool):
                    tool_desc = llm.utils.build_legacy_openai_schema(tool, internally_tagged=True)
                elif is_raw_function_tool(tool):
                    tool_info = get_raw_function_info(tool)
                    tool_desc = tool_info.raw_schema
                    tool_desc[&#34;type&#34;] = &#34;function&#34;  # internally tagged
                else:
                    logger.error(
                        &#34;OpenAI Realtime API doesn&#39;t support this tool type&#34;, extra={&#34;tool&#34;: tool}
                    )
                    continue

                try:
                    session_tool = session_update_event.SessionTool.model_validate(tool_desc)
                    oai_tools.append(session_tool)
                    retained_tools.append(tool)
                except ValidationError:
                    logger.error(
                        &#34;OpenAI Realtime API doesn&#39;t support this tool&#34;,
                        extra={&#34;tool&#34;: tool_desc},
                    )
                    continue

            event_id = utils.shortuuid(&#34;tools_update_&#34;)
            # f = asyncio.Future()
            # self._response_futures[event_id] = f
            self.send_event(
                SessionUpdateEvent(
                    type=&#34;session.update&#34;,
                    session=session_update_event.Session.model_construct(
                        model=self._realtime_model._opts.model,
                        tools=oai_tools,
                    ),
                    event_id=event_id,
                )
            )

            self._tools = llm.ToolContext(retained_tools)

    async def update_instructions(self, instructions: str) -&gt; None:
        event_id = utils.shortuuid(&#34;instructions_update_&#34;)
        # f = asyncio.Future()
        # self._response_futures[event_id] = f
        self.send_event(
            SessionUpdateEvent(
                type=&#34;session.update&#34;,
                session=session_update_event.Session.model_construct(instructions=instructions),
                event_id=event_id,
            )
        )

    def push_audio(self, frame: rtc.AudioFrame) -&gt; None:
        for f in self._resample_audio(frame):
            data = f.data.tobytes()
            for nf in self._bstream.write(data):
                self.send_event(
                    InputAudioBufferAppendEvent(
                        type=&#34;input_audio_buffer.append&#34;,
                        audio=base64.b64encode(nf.data).decode(&#34;utf-8&#34;),
                    )
                )
                self._pushed_duration_s += nf.duration

    def push_video(self, frame: rtc.VideoFrame) -&gt; None:
        pass

    def commit_audio(self) -&gt; None:
        if self._pushed_duration_s &gt; 0.1:  # OpenAI requires at least 100ms of audio
            self.send_event(InputAudioBufferCommitEvent(type=&#34;input_audio_buffer.commit&#34;))
            self._pushed_duration_s = 0

    def clear_audio(self) -&gt; None:
        self.send_event(InputAudioBufferClearEvent(type=&#34;input_audio_buffer.clear&#34;))
        self._pushed_duration_s = 0

    def generate_reply(
        self, *, instructions: NotGivenOr[str] = NOT_GIVEN
    ) -&gt; asyncio.Future[llm.GenerationCreatedEvent]:
        handle = self._create_response(instructions=instructions, user_initiated=True)
        self._text_mode_recovery_retries = 0  # reset the counter
        return handle.done_fut

    def interrupt(self) -&gt; None:
        self.send_event(ResponseCancelEvent(type=&#34;response.cancel&#34;))

    def truncate(self, *, message_id: str, audio_end_ms: int) -&gt; None:
        self.send_event(
            ConversationItemTruncateEvent(
                type=&#34;conversation.item.truncate&#34;,
                content_index=0,
                item_id=message_id,
                audio_end_ms=audio_end_ms,
            )
        )

    async def aclose(self) -&gt; None:
        self._msg_ch.close()
        await self._main_atask

    def _resample_audio(self, frame: rtc.AudioFrame) -&gt; Iterator[rtc.AudioFrame]:
        if self._input_resampler:
            if frame.sample_rate != self._input_resampler._input_rate:
                # input audio changed to a different sample rate
                self._input_resampler = None

        if self._input_resampler is None and (
            frame.sample_rate != SAMPLE_RATE or frame.num_channels != NUM_CHANNELS
        ):
            self._input_resampler = rtc.AudioResampler(
                input_rate=frame.sample_rate,
                output_rate=SAMPLE_RATE,
                num_channels=NUM_CHANNELS,
            )

        if self._input_resampler:
            # TODO(long): flush the resampler when the input source is changed
            yield from self._input_resampler.push(frame)
        else:
            yield frame

    def _create_response(
        self,
        *,
        user_initiated: bool,
        instructions: NotGivenOr[str] = NOT_GIVEN,
        old_handle: _CreateResponseHandle | None = None,
    ) -&gt; _CreateResponseHandle:
        handle = old_handle or _CreateResponseHandle(
            instructions=instructions, done_fut=asyncio.Future()
        )
        if old_handle and utils.is_given(instructions):
            handle.instructions = instructions

        event_id = utils.shortuuid(&#34;response_create_&#34;)
        if user_initiated:
            self._response_created_futures[event_id] = handle

        self.send_event(
            ResponseCreateEvent(
                type=&#34;response.create&#34;,
                event_id=event_id,
                response=Response(
                    instructions=handle.instructions or None,
                    metadata={&#34;client_event_id&#34;: event_id} if user_initiated else None,
                ),
            )
        )
        if user_initiated:
            handle.timeout_start()
        return handle

    def _emit_generation_event(self, response_id: str) -&gt; None:
        # called when the generation is a function call or a audio message
        generation_ev = llm.GenerationCreatedEvent(
            message_stream=self._current_generation.message_ch,
            function_stream=self._current_generation.function_ch,
            user_initiated=False,
        )

        if handle := self._response_created_futures.pop(response_id, None):
            generation_ev.user_initiated = True
            try:
                handle.done_fut.set_result(generation_ev)
            except asyncio.InvalidStateError:
                # in case the generation comes after the reply timeout
                logger.warning(
                    &#34;response received after timeout&#34;, extra={&#34;response_id&#34;: response_id}
                )

        self.emit(&#34;generation_created&#34;, generation_ev)

    def _handle_input_audio_buffer_speech_started(
        self, _: InputAudioBufferSpeechStartedEvent
    ) -&gt; None:
        self.emit(&#34;input_speech_started&#34;, llm.InputSpeechStartedEvent())

    def _handle_input_audio_buffer_speech_stopped(
        self, _: InputAudioBufferSpeechStoppedEvent
    ) -&gt; None:
        user_transcription_enabled = (
            self._realtime_model._opts.input_audio_transcription is not None
        )
        self.emit(
            &#34;input_speech_stopped&#34;,
            llm.InputSpeechStoppedEvent(user_transcription_enabled=user_transcription_enabled),
        )

    def _handle_response_created(self, event: ResponseCreatedEvent) -&gt; None:
        assert event.response.id is not None, &#34;response.id is None&#34;

        self._current_generation = _ResponseGeneration(
            message_ch=utils.aio.Chan(),
            function_ch=utils.aio.Chan(),
            messages={},
        )

        if (
            isinstance(event.response.metadata, dict)
            and (client_event_id := event.response.metadata.get(&#34;client_event_id&#34;))
            and (handle := self._response_created_futures.pop(client_event_id, None))
        ):
            # set key to the response id
            self._response_created_futures[event.response.id] = handle

        # the generation_created event is emitted when
        # 1. the response is not a message on response.output_item.added event
        # 2. the content is audio on response.content_part.added event
        # will try to recover from text response on response.content_part.done event

    def _handle_response_output_item_added(self, event: ResponseOutputItemAddedEvent) -&gt; None:
        assert self._current_generation is not None, &#34;current_generation is None&#34;
        assert (item_type := event.item.type) is not None, &#34;item.type is None&#34;
        assert (response_id := event.response_id) is not None, &#34;response_id is None&#34;

        if item_type != &#34;message&#34;:
            # emit immediately if it&#39;s not a message, otherwise wait response.content_part.added
            self._emit_generation_event(response_id)
            self._text_mode_recovery_retries = 0

    def _handle_response_content_part_added(self, event: ResponseContentPartAddedEvent) -&gt; None:
        assert self._current_generation is not None, &#34;current_generation is None&#34;
        assert (item_id := event.item_id) is not None, &#34;item_id is None&#34;
        assert (item_type := event.part.type) is not None, &#34;part.type is None&#34;
        assert (response_id := event.response_id) is not None, &#34;response_id is None&#34;

        if item_type == &#34;audio&#34;:
            self._emit_generation_event(response_id)
            if self._text_mode_recovery_retries &gt; 0:
                logger.info(
                    &#34;recovered from text-only response&#34;,
                    extra={&#34;retried_times&#34;: self._text_mode_recovery_retries},
                )
                self._text_mode_recovery_retries = 0

            item_generation = _MessageGeneration(
                message_id=item_id,
                text_ch=utils.aio.Chan(),
                audio_ch=utils.aio.Chan(),
            )
            self._current_generation.message_ch.send_nowait(
                llm.MessageGeneration(
                    message_id=item_id,
                    text_stream=item_generation.text_ch,
                    audio_stream=item_generation.audio_ch,
                )
            )
            self._current_generation.messages[item_id] = item_generation
        else:
            self.interrupt()
            if self._text_mode_recovery_retries == 0:
                logger.warning(&#34;received text-only response from realtime API&#34;)

    def _handle_response_content_part_done(self, event: ResponseContentPartDoneEvent) -&gt; None:
        if event.part.type != &#34;text&#34;:
            return

        # try to recover from text-only response on response.content_part_done event
        assert self._current_generation is not None, &#34;current_generation is None&#34;
        assert (item_id := event.item_id) is not None, &#34;item_id is None&#34;
        assert (response_id := event.response_id) is not None, &#34;response_id is None&#34;

        async def _retry_generation(
            item_id: str, response_handle: _CreateResponseHandle | None
        ) -&gt; None:
            &#34;&#34;&#34;Recover from text-only response to audio mode.

            When chat history is loaded, OpenAI Realtime API may respond with text only.
            This method recovers by:
            1. Deleting the text response
            2. Creating an empty user audio message
            3. Requesting a new response to trigger audio mode
            &#34;&#34;&#34;

            # remove the text item
            chat_ctx = self.chat_ctx
            idx = chat_ctx.index_by_id(item_id)
            if idx is not None:
                chat_ctx.items.pop(idx)
            await self.update_chat_ctx(chat_ctx, _add_mock_audio=True)

            if response_handle and response_handle.done_fut.done():
                if response_handle.done_fut.exception() is not None:
                    logger.error(&#34;generate_reply timed out, cancel recovery&#34;)
                return

            self._create_response(
                old_handle=response_handle,
                user_initiated=response_handle is not None,
            )

        if self._text_mode_recovery_retries &gt;= 5:
            logger.error(
                &#34;failed to recover from text-only response&#34;,
                extra={&#34;retried_times&#34;: self._text_mode_recovery_retries},
            )
            self._text_mode_recovery_retries = 0
            return

        handle = self._response_created_futures.pop(response_id, None)
        if handle and handle.done_fut.done():
            if handle.done_fut.exception() is not None:
                logger.error(&#34;generate_reply timed out, cancel recovery&#34;)
            self._text_mode_recovery_retries = 0
            return

        self._text_mode_recovery_retries += 1
        logger.warning(
            &#34;trying to recover from text-only response&#34;,
            extra={&#34;retries&#34;: self._text_mode_recovery_retries},
        )

        if self._text_mode_recovery_atask and not self._text_mode_recovery_atask.done():
            self._text_mode_recovery_atask.cancel()
        self._text_mode_recovery_atask = asyncio.create_task(
            _retry_generation(item_id=item_id, response_handle=handle)
        )

    def _handle_conversion_item_created(self, event: ConversationItemCreatedEvent) -&gt; None:
        assert event.item.id is not None, &#34;item.id is None&#34;

        try:
            self._remote_chat_ctx.insert(
                event.previous_item_id, _openai_item_to_livekit_item(event.item)
            )
        except ValueError as e:
            logger.warning(
                f&#34;failed to insert item `{event.item.id}`: {str(e)}&#34;,
            )

        if fut := self._item_create_future.pop(event.item.id, None):
            fut.set_result(None)

    def _handle_conversion_item_deleted(self, event: ConversationItemDeletedEvent) -&gt; None:
        assert event.item_id is not None, &#34;item_id is None&#34;

        try:
            self._remote_chat_ctx.delete(event.item_id)
        except ValueError as e:
            logger.warning(
                f&#34;failed to delete item `{event.item_id}`: {str(e)}&#34;,
            )

        if fut := self._item_delete_future.pop(event.item_id, None):
            fut.set_result(None)

    def _handle_conversion_item_input_audio_transcription_completed(
        self, event: ConversationItemInputAudioTranscriptionCompletedEvent
    ) -&gt; None:
        if remote_item := self._remote_chat_ctx.get(event.item_id):
            assert isinstance(remote_item.item, llm.ChatMessage)
            remote_item.item.content.append(event.transcript)

        self.emit(
            &#34;input_audio_transcription_completed&#34;,
            llm.InputTranscriptionCompleted(item_id=event.item_id, transcript=event.transcript),
        )

    def _handle_conversion_item_input_audio_transcription_failed(
        self, event: ConversationItemInputAudioTranscriptionFailedEvent
    ) -&gt; None:
        logger.error(
            &#34;OpenAI Realtime API failed to transcribe input audio&#34;,
            extra={&#34;error&#34;: event.error},
        )

    def _handle_response_audio_transcript_delta(self, event: dict) -&gt; None:
        assert self._current_generation is not None, &#34;current_generation is None&#34;

        item_id = event[&#34;item_id&#34;]
        delta = event[&#34;delta&#34;]

        if (start_time := event.get(&#34;start_time&#34;)) is not None:
            delta = io.TimedString(delta, start_time=start_time)

        item_generation = self._current_generation.messages[item_id]
        item_generation.text_ch.send_nowait(delta)

    def _handle_response_audio_delta(self, event: ResponseAudioDeltaEvent) -&gt; None:
        assert self._current_generation is not None, &#34;current_generation is None&#34;
        item_generation = self._current_generation.messages[event.item_id]

        data = base64.b64decode(event.delta)
        item_generation.audio_ch.send_nowait(
            rtc.AudioFrame(
                data=data,
                sample_rate=SAMPLE_RATE,
                num_channels=NUM_CHANNELS,
                samples_per_channel=len(data) // 2,
            )
        )

    def _handle_response_audio_transcript_done(self, _: ResponseAudioTranscriptDoneEvent) -&gt; None:
        assert self._current_generation is not None, &#34;current_generation is None&#34;

    def _handle_response_audio_done(self, _: ResponseAudioDoneEvent) -&gt; None:
        assert self._current_generation is not None, &#34;current_generation is None&#34;

    def _handle_response_output_item_done(self, event: ResponseOutputItemDoneEvent) -&gt; None:
        assert self._current_generation is not None, &#34;current_generation is None&#34;
        assert (item_id := event.item.id) is not None, &#34;item.id is None&#34;
        assert (item_type := event.item.type) is not None, &#34;item.type is None&#34;

        if item_type == &#34;function_call&#34;:
            item = event.item
            assert item.call_id is not None, &#34;call_id is None&#34;
            assert item.name is not None, &#34;name is None&#34;
            assert item.arguments is not None, &#34;arguments is None&#34;

            self._current_generation.function_ch.send_nowait(
                llm.FunctionCall(
                    call_id=item.call_id,
                    name=item.name,
                    arguments=item.arguments,
                )
            )
        elif item_type == &#34;message&#34; and (
            item_generation := self._current_generation.messages.get(item_id)
        ):
            # text response doesn&#39;t have item_generation
            item_generation.text_ch.close()
            item_generation.audio_ch.close()

    def _handle_response_done(self, _: ResponseDoneEvent) -&gt; None:
        if self._current_generation is None:
            return  # OpenAI has a race condition where we could receive response.done without any previous response.created (This happens generally during interruption)  # noqa: E501

        assert self._current_generation is not None, &#34;current_generation is None&#34;
        for generation in self._current_generation.messages.values():
            # close all messages that haven&#39;t been closed yet
            if not generation.text_ch.closed:
                generation.text_ch.close()
            if not generation.audio_ch.closed:
                generation.audio_ch.close()

        self._current_generation.function_ch.close()
        self._current_generation.message_ch.close()
        self._current_generation = None

    def _handle_error(self, event: ErrorEvent) -&gt; None:
        if event.error.message.startswith(&#34;Cancellation failed&#34;):
            return

        logger.error(
            &#34;OpenAI Realtime API returned an error&#34;,
            extra={&#34;error&#34;: event.error},
        )
        self.emit(
            &#34;error&#34;,
            llm.RealtimeModelError(
                timestamp=time.time(),
                label=self._realtime_model._label,
                error=APIError(
                    message=&#34;OpenAI Realtime API returned an error&#34;,
                    body=event.error,
                    retryable=True,
                ),
                recoverable=True,
            ),
        )

        # if event.error.event_id:
        #     fut = self._response_futures.pop(event.error.event_id, None)
        #     if fut is not None and not fut.done():
        #         fut.set_exception(multimodal.RealtimeError(event.error.message))</code></pre>
</details>
<div class="desc"><p>A session for the OpenAI Realtime API.</p>
<p>This class is used to interact with the OpenAI Realtime API.
It is responsible for sending events to the OpenAI Realtime API and receiving events from it.</p>
<p>It exposes two more events:
- openai_server_event_received: expose the raw server events from the OpenAI Realtime API
- openai_client_event_queued: expose the raw client events sent to the OpenAI Realtime API</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="livekit.agents.llm.realtime.RealtimeSession" href="../../../agents/llm/realtime.html#livekit.agents.llm.realtime.RealtimeSession">RealtimeSession</a></li>
<li>abc.ABC</li>
<li><a title="livekit.rtc.event_emitter.EventEmitter" href="../../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter">EventEmitter</a></li>
<li>typing.Generic</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.chat_ctx"><code class="name">prop <span class="ident">chat_ctx</span> : llm.ChatContext</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def chat_ctx(self) -&gt; llm.ChatContext:
    return self._remote_chat_ctx.to_chat_ctx()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.tools"><code class="name">prop <span class="ident">tools</span> : llm.ToolContext</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def tools(self) -&gt; llm.ToolContext:
    return self._tools.copy()</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.aclose"><code class="name flex">
<span>async def <span class="ident">aclose</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def aclose(self) -&gt; None:
    self._msg_ch.close()
    await self._main_atask</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.clear_audio"><code class="name flex">
<span>def <span class="ident">clear_audio</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear_audio(self) -&gt; None:
    self.send_event(InputAudioBufferClearEvent(type=&#34;input_audio_buffer.clear&#34;))
    self._pushed_duration_s = 0</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.commit_audio"><code class="name flex">
<span>def <span class="ident">commit_audio</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def commit_audio(self) -&gt; None:
    if self._pushed_duration_s &gt; 0.1:  # OpenAI requires at least 100ms of audio
        self.send_event(InputAudioBufferCommitEvent(type=&#34;input_audio_buffer.commit&#34;))
        self._pushed_duration_s = 0</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.generate_reply"><code class="name flex">
<span>def <span class="ident">generate_reply</span></span>(<span>self, *, instructions: NotGivenOr[str] = NOT_GIVEN) ‑> _asyncio.Future[<a title="livekit.agents.llm.realtime.GenerationCreatedEvent" href="../../../agents/llm/realtime.html#livekit.agents.llm.realtime.GenerationCreatedEvent">GenerationCreatedEvent</a>]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_reply(
    self, *, instructions: NotGivenOr[str] = NOT_GIVEN
) -&gt; asyncio.Future[llm.GenerationCreatedEvent]:
    handle = self._create_response(instructions=instructions, user_initiated=True)
    self._text_mode_recovery_retries = 0  # reset the counter
    return handle.done_fut</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.interrupt"><code class="name flex">
<span>def <span class="ident">interrupt</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def interrupt(self) -&gt; None:
    self.send_event(ResponseCancelEvent(type=&#34;response.cancel&#34;))</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.push_audio"><code class="name flex">
<span>def <span class="ident">push_audio</span></span>(<span>self, frame: rtc.AudioFrame) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def push_audio(self, frame: rtc.AudioFrame) -&gt; None:
    for f in self._resample_audio(frame):
        data = f.data.tobytes()
        for nf in self._bstream.write(data):
            self.send_event(
                InputAudioBufferAppendEvent(
                    type=&#34;input_audio_buffer.append&#34;,
                    audio=base64.b64encode(nf.data).decode(&#34;utf-8&#34;),
                )
            )
            self._pushed_duration_s += nf.duration</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.push_video"><code class="name flex">
<span>def <span class="ident">push_video</span></span>(<span>self, frame: rtc.VideoFrame) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def push_video(self, frame: rtc.VideoFrame) -&gt; None:
    pass</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.send_event"><code class="name flex">
<span>def <span class="ident">send_event</span></span>(<span>self, event: RealtimeClientEvent | dict) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def send_event(self, event: RealtimeClientEvent | dict) -&gt; None:
    with contextlib.suppress(utils.aio.channel.ChanClosed):
        self._msg_ch.send_nowait(event)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.truncate"><code class="name flex">
<span>def <span class="ident">truncate</span></span>(<span>self, *, message_id: str, audio_end_ms: int) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def truncate(self, *, message_id: str, audio_end_ms: int) -&gt; None:
    self.send_event(
        ConversationItemTruncateEvent(
            type=&#34;conversation.item.truncate&#34;,
            content_index=0,
            item_id=message_id,
            audio_end_ms=audio_end_ms,
        )
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.update_chat_ctx"><code class="name flex">
<span>async def <span class="ident">update_chat_ctx</span></span>(<span>self, chat_ctx: llm.ChatContext) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def update_chat_ctx(
    self, chat_ctx: llm.ChatContext, *, _add_mock_audio: bool = False
) -&gt; None:
    chat_ctx = chat_ctx.copy()
    if _add_mock_audio:
        chat_ctx.items.append(_create_mock_audio_item())
    else:
        # clean up existing mock audio items
        chat_ctx.items[:] = [
            item for item in chat_ctx.items if not item.id.startswith(_MOCK_AUDIO_ID_PREFIX)
        ]

    async with self._update_chat_ctx_lock:
        diff_ops = llm.utils.compute_chat_ctx_diff(
            self._remote_chat_ctx.to_chat_ctx(), chat_ctx
        )

        futs = []

        for msg_id in diff_ops.to_remove:
            event_id = utils.shortuuid(&#34;chat_ctx_delete_&#34;)
            self.send_event(
                ConversationItemDeleteEvent(
                    type=&#34;conversation.item.delete&#34;,
                    item_id=msg_id,
                    event_id=event_id,
                )
            )
            futs.append(f := asyncio.Future())
            self._item_delete_future[msg_id] = f

        for previous_msg_id, msg_id in diff_ops.to_create:
            event_id = utils.shortuuid(&#34;chat_ctx_create_&#34;)
            chat_item = chat_ctx.get_by_id(msg_id)
            assert chat_item is not None

            self.send_event(
                ConversationItemCreateEvent(
                    type=&#34;conversation.item.create&#34;,
                    item=_livekit_item_to_openai_item(chat_item),
                    previous_item_id=(&#34;root&#34; if previous_msg_id is None else previous_msg_id),
                    event_id=event_id,
                )
            )
            futs.append(f := asyncio.Future())
            self._item_create_future[msg_id] = f

        try:
            await asyncio.wait_for(asyncio.gather(*futs, return_exceptions=True), timeout=5.0)
        except asyncio.TimeoutError:
            raise llm.RealtimeError(&#34;update_chat_ctx timed out.&#34;) from None</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.update_instructions"><code class="name flex">
<span>async def <span class="ident">update_instructions</span></span>(<span>self, instructions: str) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def update_instructions(self, instructions: str) -&gt; None:
    event_id = utils.shortuuid(&#34;instructions_update_&#34;)
    # f = asyncio.Future()
    # self._response_futures[event_id] = f
    self.send_event(
        SessionUpdateEvent(
            type=&#34;session.update&#34;,
            session=session_update_event.Session.model_construct(instructions=instructions),
            event_id=event_id,
        )
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.update_options"><code class="name flex">
<span>def <span class="ident">update_options</span></span>(<span>self,<br>*,<br>tool_choice: NotGivenOr[llm.ToolChoice | None] = NOT_GIVEN,<br>voice: NotGivenOr[str] = NOT_GIVEN,<br>temperature: NotGivenOr[float] = NOT_GIVEN,<br>turn_detection: NotGivenOr[TurnDetection | None] = NOT_GIVEN) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_options(
    self,
    *,
    tool_choice: NotGivenOr[llm.ToolChoice | None] = NOT_GIVEN,
    voice: NotGivenOr[str] = NOT_GIVEN,
    temperature: NotGivenOr[float] = NOT_GIVEN,
    turn_detection: NotGivenOr[TurnDetection | None] = NOT_GIVEN,
) -&gt; None:
    kwargs = {}

    if is_given(tool_choice):
        oai_tool_choice = tool_choice
        if isinstance(tool_choice, dict) and tool_choice[&#34;type&#34;] == &#34;function&#34;:
            oai_tool_choice = tool_choice[&#34;function&#34;]
        if oai_tool_choice is None:
            oai_tool_choice = DEFAULT_TOOL_CHOICE

        kwargs[&#34;tool_choice&#34;] = oai_tool_choice

    if is_given(voice):
        kwargs[&#34;voice&#34;] = voice

    if is_given(temperature):
        kwargs[&#34;temperature&#34;] = temperature

    if is_given(turn_detection):
        kwargs[&#34;turn_detection&#34;] = turn_detection

    if kwargs:
        self.send_event(
            SessionUpdateEvent(
                type=&#34;session.update&#34;,
                session=session_update_event.Session.model_construct(**kwargs),
                event_id=utils.shortuuid(&#34;options_update_&#34;),
            )
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.update_tools"><code class="name flex">
<span>async def <span class="ident">update_tools</span></span>(<span>self, tools: list[llm.FunctionTool | llm.RawFunctionTool]) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def update_tools(self, tools: list[llm.FunctionTool | llm.RawFunctionTool]) -&gt; None:
    async with self._update_fnc_ctx_lock:
        oai_tools: list[session_update_event.SessionTool] = []
        retained_tools: list[llm.FunctionTool | llm.RawFunctionTool] = []

        for tool in tools:
            if is_function_tool(tool):
                tool_desc = llm.utils.build_legacy_openai_schema(tool, internally_tagged=True)
            elif is_raw_function_tool(tool):
                tool_info = get_raw_function_info(tool)
                tool_desc = tool_info.raw_schema
                tool_desc[&#34;type&#34;] = &#34;function&#34;  # internally tagged
            else:
                logger.error(
                    &#34;OpenAI Realtime API doesn&#39;t support this tool type&#34;, extra={&#34;tool&#34;: tool}
                )
                continue

            try:
                session_tool = session_update_event.SessionTool.model_validate(tool_desc)
                oai_tools.append(session_tool)
                retained_tools.append(tool)
            except ValidationError:
                logger.error(
                    &#34;OpenAI Realtime API doesn&#39;t support this tool&#34;,
                    extra={&#34;tool&#34;: tool_desc},
                )
                continue

        event_id = utils.shortuuid(&#34;tools_update_&#34;)
        # f = asyncio.Future()
        # self._response_futures[event_id] = f
        self.send_event(
            SessionUpdateEvent(
                type=&#34;session.update&#34;,
                session=session_update_event.Session.model_construct(
                    model=self._realtime_model._opts.model,
                    tools=oai_tools,
                ),
                event_id=event_id,
            )
        )

        self._tools = llm.ToolContext(retained_tools)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="livekit.agents.llm.realtime.RealtimeSession" href="../../../agents/llm/realtime.html#livekit.agents.llm.realtime.RealtimeSession">RealtimeSession</a></b></code>:
<ul class="hlist">
<li><code><a title="livekit.agents.llm.realtime.RealtimeSession.emit" href="../../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.emit">emit</a></code></li>
<li><code><a title="livekit.agents.llm.realtime.RealtimeSession.off" href="../../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.off">off</a></code></li>
<li><code><a title="livekit.agents.llm.realtime.RealtimeSession.on" href="../../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.on">on</a></code></li>
<li><code><a title="livekit.agents.llm.realtime.RealtimeSession.once" href="../../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.once">once</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="livekit.plugins.openai.realtime" href="index.html">livekit.plugins.openai.realtime</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.process_base_url" href="#livekit.plugins.openai.realtime.realtime_model.process_base_url">process_base_url</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeModel" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeModel">RealtimeModel</a></code></h4>
<ul class="">
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeModel.aclose" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeModel.aclose">aclose</a></code></li>
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeModel.session" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeModel.session">session</a></code></li>
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeModel.update_options" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeModel.update_options">update_options</a></code></li>
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeModel.with_azure" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeModel.with_azure">with_azure</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeSession" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeSession">RealtimeSession</a></code></h4>
<ul class="two-column">
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.aclose" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeSession.aclose">aclose</a></code></li>
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.chat_ctx" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeSession.chat_ctx">chat_ctx</a></code></li>
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.clear_audio" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeSession.clear_audio">clear_audio</a></code></li>
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.commit_audio" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeSession.commit_audio">commit_audio</a></code></li>
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.generate_reply" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeSession.generate_reply">generate_reply</a></code></li>
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.interrupt" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeSession.interrupt">interrupt</a></code></li>
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.push_audio" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeSession.push_audio">push_audio</a></code></li>
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.push_video" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeSession.push_video">push_video</a></code></li>
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.send_event" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeSession.send_event">send_event</a></code></li>
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.tools" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeSession.tools">tools</a></code></li>
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.truncate" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeSession.truncate">truncate</a></code></li>
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.update_chat_ctx" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeSession.update_chat_ctx">update_chat_ctx</a></code></li>
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.update_instructions" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeSession.update_instructions">update_instructions</a></code></li>
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.update_options" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeSession.update_options">update_options</a></code></li>
<li><code><a title="livekit.plugins.openai.realtime.realtime_model.RealtimeSession.update_tools" href="#livekit.plugins.openai.realtime.realtime_model.RealtimeSession.update_tools">update_tools</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
