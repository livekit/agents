<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>livekit.plugins.deepgram API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>livekit.plugins.deepgram</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="livekit.plugins.deepgram.AudioEnergyFilter"><code class="flex name class">
<span>class <span class="ident">AudioEnergyFilter</span></span>
<span>(</span><span>*, min_silence: float = 1.5, rms_threshold: float = 1.6e-05)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AudioEnergyFilter:
    class State(Enum):
        START = 0
        SPEAKING = 1
        SILENCE = 2
        END = 3

    def __init__(self, *, min_silence: float = 1.5, rms_threshold: float = MAGIC_NUMBER_THRESHOLD):
        self._cooldown_seconds = min_silence
        self._cooldown = min_silence
        self._state = self.State.SILENCE
        self._rms_threshold = rms_threshold

    def update(self, frame: rtc.AudioFrame) -&gt; State:
        arr = np.frombuffer(frame.data, dtype=np.int16)
        float_arr = arr.astype(np.float32) / 32768.0
        rms = np.mean(np.square(float_arr))

        if rms &gt; self._rms_threshold:
            self._cooldown = self._cooldown_seconds
            if self._state in (self.State.SILENCE, self.State.END):
                self._state = self.State.START
            else:
                self._state = self.State.SPEAKING
        else:
            if self._cooldown &lt;= 0:
                if self._state in (self.State.SPEAKING, self.State.START):
                    self._state = self.State.END
                elif self._state == self.State.END:
                    self._state = self.State.SILENCE
            else:
                # keep speaking during cooldown
                self._cooldown -= frame.duration
                self._state = self.State.SPEAKING

        return self._state</code></pre>
</details>
<div class="desc"></div>
<h3>Class variables</h3>
<dl>
<dt id="livekit.plugins.deepgram.AudioEnergyFilter.State"><code class="name">var <span class="ident">State</span></code></dt>
<dd>
<div class="desc"><p>Create a collection of name/value pairs.</p>
<p>Example enumeration:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; class Color(Enum):
...     RED = 1
...     BLUE = 2
...     GREEN = 3
</code></pre>
<p>Access them by:</p>
<ul>
<li>attribute access:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color.RED
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li>value lookup:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color(1)
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li>name lookup:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color['RED']
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<p>Enumerations can be iterated over, and know how many members they have:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; len(Color)
3
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; list(Color)
[&lt;Color.RED: 1&gt;, &lt;Color.BLUE: 2&gt;, &lt;Color.GREEN: 3&gt;]
</code></pre>
<p>Methods can be added to enumerations, and members can have their own
attributes &ndash; see the documentation for details.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="livekit.plugins.deepgram.AudioEnergyFilter.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, frame: rtc.AudioFrame) ‑> State</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self, frame: rtc.AudioFrame) -&gt; State:
    arr = np.frombuffer(frame.data, dtype=np.int16)
    float_arr = arr.astype(np.float32) / 32768.0
    rms = np.mean(np.square(float_arr))

    if rms &gt; self._rms_threshold:
        self._cooldown = self._cooldown_seconds
        if self._state in (self.State.SILENCE, self.State.END):
            self._state = self.State.START
        else:
            self._state = self.State.SPEAKING
    else:
        if self._cooldown &lt;= 0:
            if self._state in (self.State.SPEAKING, self.State.START):
                self._state = self.State.END
            elif self._state == self.State.END:
                self._state = self.State.SILENCE
        else:
            # keep speaking during cooldown
            self._cooldown -= frame.duration
            self._state = self.State.SPEAKING

    return self._state</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="livekit.plugins.deepgram.STT"><code class="flex name class">
<span>class <span class="ident">STT</span></span>
<span>(</span><span>*,<br>model: DeepgramModels | str = 'nova-3',<br>language: DeepgramLanguages | str = 'en-US',<br>detect_language: bool = False,<br>interim_results: bool = True,<br>punctuate: bool = True,<br>smart_format: bool = True,<br>sample_rate: int = 16000,<br>no_delay: bool = True,<br>endpointing_ms: int = 25,<br>filler_words: bool = True,<br>keywords: NotGivenOr[list[tuple[str, float]]] = NOT_GIVEN,<br>keyterms: NotGivenOr[list[str]] = NOT_GIVEN,<br>tags: NotGivenOr[list[str]] = NOT_GIVEN,<br>profanity_filter: bool = False,<br>api_key: NotGivenOr[str] = NOT_GIVEN,<br>http_session: aiohttp.ClientSession | None = None,<br>base_url: str = 'https://api.deepgram.com/v1/listen',<br>energy_filter: <a title="livekit.plugins.deepgram.AudioEnergyFilter" href="#livekit.plugins.deepgram.AudioEnergyFilter">AudioEnergyFilter</a> | bool = False,<br>numerals: bool = False,<br>mip_opt_out: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class STT(stt.STT):
    def __init__(
        self,
        *,
        model: DeepgramModels | str = &#34;nova-3&#34;,
        language: DeepgramLanguages | str = &#34;en-US&#34;,
        detect_language: bool = False,
        interim_results: bool = True,
        punctuate: bool = True,
        smart_format: bool = True,
        sample_rate: int = 16000,
        no_delay: bool = True,
        endpointing_ms: int = 25,
        # enable filler words by default to improve turn detector accuracy
        filler_words: bool = True,
        keywords: NotGivenOr[list[tuple[str, float]]] = NOT_GIVEN,
        keyterms: NotGivenOr[list[str]] = NOT_GIVEN,
        tags: NotGivenOr[list[str]] = NOT_GIVEN,
        profanity_filter: bool = False,
        api_key: NotGivenOr[str] = NOT_GIVEN,
        http_session: aiohttp.ClientSession | None = None,
        base_url: str = BASE_URL,
        energy_filter: AudioEnergyFilter | bool = False,
        numerals: bool = False,
        mip_opt_out: bool = False,
    ) -&gt; None:
        &#34;&#34;&#34;Create a new instance of Deepgram STT.

        Args:
            model: The Deepgram model to use for speech recognition. Defaults to &#34;nova-2-general&#34;.
            language: The language code for recognition. Defaults to &#34;en-US&#34;.
            detect_language: Whether to enable automatic language detection. Defaults to False.
            interim_results: Whether to return interim (non-final) transcription results. Defaults to True.
            punctuate: Whether to add punctuations to the transcription. Defaults to True. Turn detector will work better with punctuations.
            smart_format: Whether to apply smart formatting to numbers, dates, etc. Defaults to True.
            sample_rate: The sample rate of the audio in Hz. Defaults to 16000.
            no_delay: When smart_format is used, ensures it does not wait for sequence to be complete before returning results. Defaults to True.
            endpointing_ms: Time in milliseconds of silence to consider end of speech. Set to 0 to disable. Defaults to 25.
            filler_words: Whether to include filler words (um, uh, etc.) in transcription. Defaults to True.
            keywords: List of tuples containing keywords and their boost values for improved recognition.
                     Each tuple should be (keyword: str, boost: float). Defaults to None.
                     `keywords` does not work with Nova-3 models. Use `keyterms` instead.
            keyterms: List of key terms to improve recognition accuracy. Defaults to None.
                     `keyterms` is supported by Nova-3 models.
            tags: List of tags to add to the requests for usage reporting. Defaults to NOT_GIVEN.
            profanity_filter: Whether to filter profanity from the transcription. Defaults to False.
            api_key: Your Deepgram API key. If not provided, will look for DEEPGRAM_API_KEY environment variable.
            http_session: Optional aiohttp ClientSession to use for requests.
            base_url: The base URL for Deepgram API. Defaults to &#34;https://api.deepgram.com/v1/listen&#34;.
            energy_filter: Audio energy filter configuration for voice activity detection.
                         Can be a boolean or AudioEnergyFilter instance. Defaults to False.
            numerals: Whether to include numerals in the transcription. Defaults to False.
            mip_opt_out: Whether to take part in the model improvement program

        Raises:
            ValueError: If no API key is provided or found in environment variables.

        Note:
            The api_key must be set either through the constructor argument or by setting
            the DEEPGRAM_API_KEY environmental variable.
        &#34;&#34;&#34;  # noqa: E501

        super().__init__(
            capabilities=stt.STTCapabilities(streaming=True, interim_results=interim_results)
        )
        self._base_url = base_url

        self._api_key = api_key if is_given(api_key) else os.environ.get(&#34;DEEPGRAM_API_KEY&#34;)
        if not self._api_key:
            raise ValueError(&#34;Deepgram API key is required&#34;)

        model = _validate_model(model, language)
        _validate_keyterms(model, language, keyterms, keywords)

        self._opts = STTOptions(
            language=language,
            detect_language=detect_language,
            interim_results=interim_results,
            punctuate=punctuate,
            model=model,
            smart_format=smart_format,
            no_delay=no_delay,
            endpointing_ms=endpointing_ms,
            filler_words=filler_words,
            sample_rate=sample_rate,
            num_channels=1,
            keywords=keywords if is_given(keywords) else [],
            keyterms=keyterms if is_given(keyterms) else [],
            profanity_filter=profanity_filter,
            energy_filter=energy_filter,
            numerals=numerals,
            mip_opt_out=mip_opt_out,
            tags=_validate_tags(tags) if is_given(tags) else [],
        )
        self._session = http_session
        self._streams = weakref.WeakSet[SpeechStream]()

    def _ensure_session(self) -&gt; aiohttp.ClientSession:
        if not self._session:
            self._session = utils.http_context.http_session()

        return self._session

    async def _recognize_impl(
        self,
        buffer: AudioBuffer,
        *,
        language: NotGivenOr[DeepgramLanguages | str] = NOT_GIVEN,
        conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
    ) -&gt; stt.SpeechEvent:
        config = self._sanitize_options(language=language)

        recognize_config = {
            &#34;model&#34;: str(config.model),
            &#34;punctuate&#34;: config.punctuate,
            &#34;detect_language&#34;: config.detect_language,
            &#34;smart_format&#34;: config.smart_format,
            &#34;keywords&#34;: self._opts.keywords,
            &#34;profanity_filter&#34;: config.profanity_filter,
            &#34;numerals&#34;: config.numerals,
        }
        if config.language:
            recognize_config[&#34;language&#34;] = config.language

        try:
            async with self._ensure_session().post(
                url=_to_deepgram_url(recognize_config, self._base_url, websocket=False),
                data=rtc.combine_audio_frames(buffer).to_wav_bytes(),
                headers={
                    &#34;Authorization&#34;: f&#34;Token {self._api_key}&#34;,
                    &#34;Accept&#34;: &#34;application/json&#34;,
                    &#34;Content-Type&#34;: &#34;audio/wav&#34;,
                },
                timeout=aiohttp.ClientTimeout(
                    total=30,
                    sock_connect=conn_options.timeout,
                ),
            ) as res:
                return prerecorded_transcription_to_speech_event(
                    config.language,
                    await res.json(),
                )

        except asyncio.TimeoutError as e:
            raise APITimeoutError() from e
        except aiohttp.ClientResponseError as e:
            raise APIStatusError(
                message=e.message,
                status_code=e.status,
                request_id=None,
                body=None,
            ) from e
        except Exception as e:
            raise APIConnectionError() from e

    def stream(
        self,
        *,
        language: NotGivenOr[DeepgramLanguages | str] = NOT_GIVEN,
        conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
    ) -&gt; SpeechStream:
        config = self._sanitize_options(language=language)
        stream = SpeechStream(
            stt=self,
            conn_options=conn_options,
            opts=config,
            api_key=self._api_key,
            http_session=self._ensure_session(),
            base_url=self._base_url,
        )
        self._streams.add(stream)
        return stream

    def update_options(
        self,
        *,
        language: NotGivenOr[DeepgramLanguages | str] = NOT_GIVEN,
        model: NotGivenOr[DeepgramModels | str] = NOT_GIVEN,
        interim_results: NotGivenOr[bool] = NOT_GIVEN,
        punctuate: NotGivenOr[bool] = NOT_GIVEN,
        smart_format: NotGivenOr[bool] = NOT_GIVEN,
        sample_rate: NotGivenOr[int] = NOT_GIVEN,
        no_delay: NotGivenOr[bool] = NOT_GIVEN,
        endpointing_ms: NotGivenOr[int] = NOT_GIVEN,
        filler_words: NotGivenOr[bool] = NOT_GIVEN,
        keywords: NotGivenOr[list[tuple[str, float]]] = NOT_GIVEN,
        keyterms: NotGivenOr[list[str]] = NOT_GIVEN,
        profanity_filter: NotGivenOr[bool] = NOT_GIVEN,
        numerals: NotGivenOr[bool] = NOT_GIVEN,
        mip_opt_out: NotGivenOr[bool] = NOT_GIVEN,
        tags: NotGivenOr[list[str]] = NOT_GIVEN,
    ):
        if is_given(language):
            self._opts.language = language
        if is_given(model):
            self._opts.model = _validate_model(model, language)
        if is_given(interim_results):
            self._opts.interim_results = interim_results
        if is_given(punctuate):
            self._opts.punctuate = punctuate
        if is_given(smart_format):
            self._opts.smart_format = smart_format
        if is_given(sample_rate):
            self._opts.sample_rate = sample_rate
        if is_given(no_delay):
            self._opts.no_delay = no_delay
        if is_given(endpointing_ms):
            self._opts.endpointing_ms = endpointing_ms
        if is_given(filler_words):
            self._opts.filler_words = filler_words
        if is_given(keywords):
            self._opts.keywords = keywords
        if is_given(keyterms):
            self._opts.keyterms = keyterms
        if is_given(profanity_filter):
            self._opts.profanity_filter = profanity_filter
        if is_given(numerals):
            self._opts.numerals = numerals
        if is_given(mip_opt_out):
            self._opts.mip_opt_out = mip_opt_out
        if is_given(tags):
            self._opts.tags = _validate_tags(tags)

        for stream in self._streams:
            stream.update_options(
                language=language,
                model=model,
                interim_results=interim_results,
                punctuate=punctuate,
                smart_format=smart_format,
                sample_rate=sample_rate,
                no_delay=no_delay,
                endpointing_ms=endpointing_ms,
                filler_words=filler_words,
                keywords=keywords,
                keyterms=keyterms,
                profanity_filter=profanity_filter,
                numerals=numerals,
                mip_opt_out=mip_opt_out,
            )

    def _sanitize_options(
        self, *, language: NotGivenOr[DeepgramLanguages | str] = NOT_GIVEN
    ) -&gt; STTOptions:
        config = dataclasses.replace(self._opts)
        if is_given(language):
            config.language = language

        if config.detect_language:
            config.language = None

        return config</code></pre>
</details>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Create a new instance of Deepgram STT.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong></dt>
<dd>The Deepgram model to use for speech recognition. Defaults to "nova-2-general".</dd>
<dt><strong><code>language</code></strong></dt>
<dd>The language code for recognition. Defaults to "en-US".</dd>
<dt><strong><code>detect_language</code></strong></dt>
<dd>Whether to enable automatic language detection. Defaults to False.</dd>
<dt><strong><code>interim_results</code></strong></dt>
<dd>Whether to return interim (non-final) transcription results. Defaults to True.</dd>
<dt><strong><code>punctuate</code></strong></dt>
<dd>Whether to add punctuations to the transcription. Defaults to True. Turn detector will work better with punctuations.</dd>
<dt><strong><code>smart_format</code></strong></dt>
<dd>Whether to apply smart formatting to numbers, dates, etc. Defaults to True.</dd>
<dt><strong><code>sample_rate</code></strong></dt>
<dd>The sample rate of the audio in Hz. Defaults to 16000.</dd>
<dt><strong><code>no_delay</code></strong></dt>
<dd>When smart_format is used, ensures it does not wait for sequence to be complete before returning results. Defaults to True.</dd>
<dt><strong><code>endpointing_ms</code></strong></dt>
<dd>Time in milliseconds of silence to consider end of speech. Set to 0 to disable. Defaults to 25.</dd>
<dt><strong><code>filler_words</code></strong></dt>
<dd>Whether to include filler words (um, uh, etc.) in transcription. Defaults to True.</dd>
<dt><strong><code>keywords</code></strong></dt>
<dd>List of tuples containing keywords and their boost values for improved recognition.
Each tuple should be (keyword: str, boost: float). Defaults to None.
<code>keywords</code> does not work with Nova-3 models. Use <code>keyterms</code> instead.</dd>
<dt><strong><code>keyterms</code></strong></dt>
<dd>List of key terms to improve recognition accuracy. Defaults to None.
<code>keyterms</code> is supported by Nova-3 models.</dd>
<dt><strong><code>tags</code></strong></dt>
<dd>List of tags to add to the requests for usage reporting. Defaults to NOT_GIVEN.</dd>
<dt><strong><code>profanity_filter</code></strong></dt>
<dd>Whether to filter profanity from the transcription. Defaults to False.</dd>
<dt><strong><code>api_key</code></strong></dt>
<dd>Your Deepgram API key. If not provided, will look for DEEPGRAM_API_KEY environment variable.</dd>
<dt><strong><code>http_session</code></strong></dt>
<dd>Optional aiohttp ClientSession to use for requests.</dd>
<dt><strong><code>base_url</code></strong></dt>
<dd>The base URL for Deepgram API. Defaults to "https://api.deepgram.com/v1/listen".</dd>
<dt><strong><code>energy_filter</code></strong></dt>
<dd>Audio energy filter configuration for voice activity detection.
Can be a boolean or AudioEnergyFilter instance. Defaults to False.</dd>
<dt><strong><code>numerals</code></strong></dt>
<dd>Whether to include numerals in the transcription. Defaults to False.</dd>
<dt><strong><code>mip_opt_out</code></strong></dt>
<dd>Whether to take part in the model improvement program</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If no API key is provided or found in environment variables.</dd>
</dl>
<h2 id="note">Note</h2>
<p>The api_key must be set either through the constructor argument or by setting
the DEEPGRAM_API_KEY environmental variable.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="livekit.agents.stt.stt.STT" href="../../agents/stt/stt.html#livekit.agents.stt.stt.STT">STT</a></li>
<li>abc.ABC</li>
<li><a title="livekit.rtc.event_emitter.EventEmitter" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter">EventEmitter</a></li>
<li>typing.Generic</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="livekit.plugins.deepgram.STT.stream"><code class="name flex">
<span>def <span class="ident">stream</span></span>(<span>self,<br>*,<br>language: NotGivenOr[DeepgramLanguages | str] = NOT_GIVEN,<br>conn_options: APIConnectOptions = APIConnectOptions(max_retry=3, retry_interval=2.0, timeout=10.0)) ‑> livekit.plugins.deepgram.stt.SpeechStream</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stream(
    self,
    *,
    language: NotGivenOr[DeepgramLanguages | str] = NOT_GIVEN,
    conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
) -&gt; SpeechStream:
    config = self._sanitize_options(language=language)
    stream = SpeechStream(
        stt=self,
        conn_options=conn_options,
        opts=config,
        api_key=self._api_key,
        http_session=self._ensure_session(),
        base_url=self._base_url,
    )
    self._streams.add(stream)
    return stream</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.deepgram.STT.update_options"><code class="name flex">
<span>def <span class="ident">update_options</span></span>(<span>self,<br>*,<br>language: NotGivenOr[DeepgramLanguages | str] = NOT_GIVEN,<br>model: NotGivenOr[DeepgramModels | str] = NOT_GIVEN,<br>interim_results: NotGivenOr[bool] = NOT_GIVEN,<br>punctuate: NotGivenOr[bool] = NOT_GIVEN,<br>smart_format: NotGivenOr[bool] = NOT_GIVEN,<br>sample_rate: NotGivenOr[int] = NOT_GIVEN,<br>no_delay: NotGivenOr[bool] = NOT_GIVEN,<br>endpointing_ms: NotGivenOr[int] = NOT_GIVEN,<br>filler_words: NotGivenOr[bool] = NOT_GIVEN,<br>keywords: NotGivenOr[list[tuple[str, float]]] = NOT_GIVEN,<br>keyterms: NotGivenOr[list[str]] = NOT_GIVEN,<br>profanity_filter: NotGivenOr[bool] = NOT_GIVEN,<br>numerals: NotGivenOr[bool] = NOT_GIVEN,<br>mip_opt_out: NotGivenOr[bool] = NOT_GIVEN,<br>tags: NotGivenOr[list[str]] = NOT_GIVEN)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_options(
    self,
    *,
    language: NotGivenOr[DeepgramLanguages | str] = NOT_GIVEN,
    model: NotGivenOr[DeepgramModels | str] = NOT_GIVEN,
    interim_results: NotGivenOr[bool] = NOT_GIVEN,
    punctuate: NotGivenOr[bool] = NOT_GIVEN,
    smart_format: NotGivenOr[bool] = NOT_GIVEN,
    sample_rate: NotGivenOr[int] = NOT_GIVEN,
    no_delay: NotGivenOr[bool] = NOT_GIVEN,
    endpointing_ms: NotGivenOr[int] = NOT_GIVEN,
    filler_words: NotGivenOr[bool] = NOT_GIVEN,
    keywords: NotGivenOr[list[tuple[str, float]]] = NOT_GIVEN,
    keyterms: NotGivenOr[list[str]] = NOT_GIVEN,
    profanity_filter: NotGivenOr[bool] = NOT_GIVEN,
    numerals: NotGivenOr[bool] = NOT_GIVEN,
    mip_opt_out: NotGivenOr[bool] = NOT_GIVEN,
    tags: NotGivenOr[list[str]] = NOT_GIVEN,
):
    if is_given(language):
        self._opts.language = language
    if is_given(model):
        self._opts.model = _validate_model(model, language)
    if is_given(interim_results):
        self._opts.interim_results = interim_results
    if is_given(punctuate):
        self._opts.punctuate = punctuate
    if is_given(smart_format):
        self._opts.smart_format = smart_format
    if is_given(sample_rate):
        self._opts.sample_rate = sample_rate
    if is_given(no_delay):
        self._opts.no_delay = no_delay
    if is_given(endpointing_ms):
        self._opts.endpointing_ms = endpointing_ms
    if is_given(filler_words):
        self._opts.filler_words = filler_words
    if is_given(keywords):
        self._opts.keywords = keywords
    if is_given(keyterms):
        self._opts.keyterms = keyterms
    if is_given(profanity_filter):
        self._opts.profanity_filter = profanity_filter
    if is_given(numerals):
        self._opts.numerals = numerals
    if is_given(mip_opt_out):
        self._opts.mip_opt_out = mip_opt_out
    if is_given(tags):
        self._opts.tags = _validate_tags(tags)

    for stream in self._streams:
        stream.update_options(
            language=language,
            model=model,
            interim_results=interim_results,
            punctuate=punctuate,
            smart_format=smart_format,
            sample_rate=sample_rate,
            no_delay=no_delay,
            endpointing_ms=endpointing_ms,
            filler_words=filler_words,
            keywords=keywords,
            keyterms=keyterms,
            profanity_filter=profanity_filter,
            numerals=numerals,
            mip_opt_out=mip_opt_out,
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="livekit.agents.stt.stt.STT" href="../../agents/stt/stt.html#livekit.agents.stt.stt.STT">STT</a></b></code>:
<ul class="hlist">
<li><code><a title="livekit.agents.stt.stt.STT.aclose" href="../../agents/stt/stt.html#livekit.agents.stt.stt.STT.aclose">aclose</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STT.emit" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.emit">emit</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STT.off" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.off">off</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STT.on" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.on">on</a></code></li>
<li><code><a title="livekit.agents.stt.stt.STT.once" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.once">once</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="livekit.plugins.deepgram.SpeechStream"><code class="flex name class">
<span>class <span class="ident">SpeechStream</span></span>
<span>(</span><span>*,<br>stt: <a title="livekit.plugins.deepgram.STT" href="#livekit.plugins.deepgram.STT">STT</a>,<br>opts: STTOptions,<br>conn_options: APIConnectOptions,<br>api_key: str,<br>http_session: aiohttp.ClientSession,<br>base_url: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SpeechStream(stt.SpeechStream):
    _KEEPALIVE_MSG: str = json.dumps({&#34;type&#34;: &#34;KeepAlive&#34;})
    _CLOSE_MSG: str = json.dumps({&#34;type&#34;: &#34;CloseStream&#34;})
    _FINALIZE_MSG: str = json.dumps({&#34;type&#34;: &#34;Finalize&#34;})

    def __init__(
        self,
        *,
        stt: STT,
        opts: STTOptions,
        conn_options: APIConnectOptions,
        api_key: str,
        http_session: aiohttp.ClientSession,
        base_url: str,
    ) -&gt; None:
        super().__init__(stt=stt, conn_options=conn_options, sample_rate=opts.sample_rate)

        if opts.detect_language or opts.language is None:
            raise ValueError(
                &#34;language detection is not supported in streaming mode, &#34;
                &#34;please disable it and specify a language&#34;
            )

        self._opts = opts
        self._api_key = api_key
        self._session = http_session
        self._base_url = base_url
        self._speaking = False
        self._audio_duration_collector = PeriodicCollector(
            callback=self._on_audio_duration_report,
            duration=5.0,
        )

        self._audio_energy_filter: AudioEnergyFilter | None = None
        if opts.energy_filter:
            if isinstance(opts.energy_filter, AudioEnergyFilter):
                self._audio_energy_filter = opts.energy_filter
            else:
                self._audio_energy_filter = AudioEnergyFilter()

        self._request_id = &#34;&#34;
        self._reconnect_event = asyncio.Event()

    def update_options(
        self,
        *,
        language: NotGivenOr[DeepgramLanguages | str] = NOT_GIVEN,
        model: NotGivenOr[DeepgramModels | str] = NOT_GIVEN,
        interim_results: NotGivenOr[bool] = NOT_GIVEN,
        punctuate: NotGivenOr[bool] = NOT_GIVEN,
        smart_format: NotGivenOr[bool] = NOT_GIVEN,
        sample_rate: NotGivenOr[int] = NOT_GIVEN,
        no_delay: NotGivenOr[bool] = NOT_GIVEN,
        endpointing_ms: NotGivenOr[int] = NOT_GIVEN,
        filler_words: NotGivenOr[bool] = NOT_GIVEN,
        keywords: NotGivenOr[list[tuple[str, float]]] = NOT_GIVEN,
        keyterms: NotGivenOr[list[str]] = NOT_GIVEN,
        profanity_filter: NotGivenOr[bool] = NOT_GIVEN,
        numerals: NotGivenOr[bool] = NOT_GIVEN,
        mip_opt_out: NotGivenOr[bool] = NOT_GIVEN,
        tags: NotGivenOr[list[str]] = NOT_GIVEN,
    ):
        if is_given(language):
            self._opts.language = language
        if is_given(model):
            self._opts.model = _validate_model(model, language)
        if is_given(interim_results):
            self._opts.interim_results = interim_results
        if is_given(punctuate):
            self._opts.punctuate = punctuate
        if is_given(smart_format):
            self._opts.smart_format = smart_format
        if is_given(sample_rate):
            self._opts.sample_rate = sample_rate
        if is_given(no_delay):
            self._opts.no_delay = no_delay
        if is_given(endpointing_ms):
            self._opts.endpointing_ms = endpointing_ms
        if is_given(filler_words):
            self._opts.filler_words = filler_words
        if is_given(keywords):
            self._opts.keywords = keywords
        if is_given(keyterms):
            self._opts.keyterms = keyterms
        if is_given(profanity_filter):
            self._opts.profanity_filter = profanity_filter
        if is_given(numerals):
            self._opts.numerals = numerals
        if is_given(mip_opt_out):
            self._opts.mip_opt_out = mip_opt_out
        if is_given(tags):
            self._opts.tags = _validate_tags(tags)

        self._reconnect_event.set()

    async def _run(self) -&gt; None:
        closing_ws = False

        async def keepalive_task(ws: aiohttp.ClientWebSocketResponse):
            # if we want to keep the connection alive even if no audio is sent,
            # Deepgram expects a keepalive message.
            # https://developers.deepgram.com/reference/listen-live#stream-keepalive
            try:
                while True:
                    await ws.send_str(SpeechStream._KEEPALIVE_MSG)
                    await asyncio.sleep(5)
            except Exception:
                return

        @utils.log_exceptions(logger=logger)
        async def send_task(ws: aiohttp.ClientWebSocketResponse):
            nonlocal closing_ws

            # forward audio to deepgram in chunks of 50ms
            samples_50ms = self._opts.sample_rate // 20
            audio_bstream = utils.audio.AudioByteStream(
                sample_rate=self._opts.sample_rate,
                num_channels=self._opts.num_channels,
                samples_per_channel=samples_50ms,
            )

            has_ended = False
            last_frame: rtc.AudioFrame | None = None
            async for data in self._input_ch:
                frames: list[rtc.AudioFrame] = []
                if isinstance(data, rtc.AudioFrame):
                    state = self._check_energy_state(data)
                    if state in (
                        AudioEnergyFilter.State.START,
                        AudioEnergyFilter.State.SPEAKING,
                    ):
                        if last_frame:
                            frames.extend(audio_bstream.write(last_frame.data.tobytes()))
                            last_frame = None
                        frames.extend(audio_bstream.write(data.data.tobytes()))
                    elif state == AudioEnergyFilter.State.END:
                        # no need to buffer as we have cooldown period
                        frames.extend(audio_bstream.flush())
                        has_ended = True
                    elif state == AudioEnergyFilter.State.SILENCE:
                        # buffer the last silence frame, since it could contain beginning of speech
                        # TODO: improve accuracy by using a ring buffer with longer window
                        last_frame = data
                elif isinstance(data, self._FlushSentinel):
                    frames.extend(audio_bstream.flush())
                    has_ended = True

                for frame in frames:
                    self._audio_duration_collector.push(frame.duration)
                    await ws.send_bytes(frame.data.tobytes())

                    if has_ended:
                        self._audio_duration_collector.flush()
                        await ws.send_str(SpeechStream._FINALIZE_MSG)
                        has_ended = False

            # tell deepgram we are done sending audio/inputs
            closing_ws = True
            await ws.send_str(SpeechStream._CLOSE_MSG)

        @utils.log_exceptions(logger=logger)
        async def recv_task(ws: aiohttp.ClientWebSocketResponse):
            nonlocal closing_ws
            while True:
                msg = await ws.receive()
                if msg.type in (
                    aiohttp.WSMsgType.CLOSED,
                    aiohttp.WSMsgType.CLOSE,
                    aiohttp.WSMsgType.CLOSING,
                ):
                    # close is expected, see SpeechStream.aclose
                    # or when the agent session ends, the http session is closed
                    if closing_ws or self._session.closed:
                        return

                    # this will trigger a reconnection, see the _run loop
                    raise APIStatusError(message=&#34;deepgram connection closed unexpectedly&#34;)

                if msg.type != aiohttp.WSMsgType.TEXT:
                    logger.warning(&#34;unexpected deepgram message type %s&#34;, msg.type)
                    continue

                try:
                    self._process_stream_event(json.loads(msg.data))
                except Exception:
                    logger.exception(&#34;failed to process deepgram message&#34;)

        ws: aiohttp.ClientWebSocketResponse | None = None

        while True:
            try:
                ws = await self._connect_ws()
                tasks = [
                    asyncio.create_task(send_task(ws)),
                    asyncio.create_task(recv_task(ws)),
                    asyncio.create_task(keepalive_task(ws)),
                ]
                wait_reconnect_task = asyncio.create_task(self._reconnect_event.wait())
                try:
                    done, _ = await asyncio.wait(
                        [asyncio.gather(*tasks), wait_reconnect_task],
                        return_when=asyncio.FIRST_COMPLETED,
                    )  # type: ignore

                    # propagate exceptions from completed tasks
                    for task in done:
                        if task != wait_reconnect_task:
                            task.result()

                    if wait_reconnect_task not in done:
                        break

                    self._reconnect_event.clear()
                finally:
                    await utils.aio.gracefully_cancel(*tasks, wait_reconnect_task)
            finally:
                if ws is not None:
                    await ws.close()

    async def _connect_ws(self) -&gt; aiohttp.ClientWebSocketResponse:
        live_config: dict[str, Any] = {
            &#34;model&#34;: self._opts.model,
            &#34;punctuate&#34;: self._opts.punctuate,
            &#34;smart_format&#34;: self._opts.smart_format,
            &#34;no_delay&#34;: self._opts.no_delay,
            &#34;interim_results&#34;: self._opts.interim_results,
            &#34;encoding&#34;: &#34;linear16&#34;,
            &#34;vad_events&#34;: True,
            &#34;sample_rate&#34;: self._opts.sample_rate,
            &#34;channels&#34;: self._opts.num_channels,
            &#34;endpointing&#34;: False if self._opts.endpointing_ms == 0 else self._opts.endpointing_ms,
            &#34;filler_words&#34;: self._opts.filler_words,
            &#34;profanity_filter&#34;: self._opts.profanity_filter,
            &#34;numerals&#34;: self._opts.numerals,
            &#34;mip_opt_out&#34;: self._opts.mip_opt_out,
        }
        if self._opts.keywords:
            live_config[&#34;keywords&#34;] = self._opts.keywords
        if self._opts.keyterms:
            # the query param is `keyterm`
            # See: https://developers.deepgram.com/docs/keyterm
            live_config[&#34;keyterm&#34;] = self._opts.keyterms

        if self._opts.language:
            live_config[&#34;language&#34;] = self._opts.language

        if self._opts.tags:
            live_config[&#34;tag&#34;] = self._opts.tags

        ws = await asyncio.wait_for(
            self._session.ws_connect(
                _to_deepgram_url(live_config, base_url=self._base_url, websocket=True),
                headers={&#34;Authorization&#34;: f&#34;Token {self._api_key}&#34;},
            ),
            self._conn_options.timeout,
        )
        return ws

    def _check_energy_state(self, frame: rtc.AudioFrame) -&gt; AudioEnergyFilter.State:
        if self._audio_energy_filter:
            return self._audio_energy_filter.update(frame)
        return AudioEnergyFilter.State.SPEAKING

    def _on_audio_duration_report(self, duration: float) -&gt; None:
        usage_event = stt.SpeechEvent(
            type=stt.SpeechEventType.RECOGNITION_USAGE,
            request_id=self._request_id,
            alternatives=[],
            recognition_usage=stt.RecognitionUsage(audio_duration=duration),
        )
        self._event_ch.send_nowait(usage_event)

    def _process_stream_event(self, data: dict) -&gt; None:
        assert self._opts.language is not None

        if data[&#34;type&#34;] == &#34;SpeechStarted&#34;:
            # This is a normal case. Deepgram&#39;s SpeechStarted events
            # are not correlated with speech_final or utterance end.
            # It&#39;s possible that we receive two in a row without an endpoint
            # It&#39;s also possible we receive a transcript without a SpeechStarted event.
            if self._speaking:
                return

            self._speaking = True
            start_event = stt.SpeechEvent(type=stt.SpeechEventType.START_OF_SPEECH)
            self._event_ch.send_nowait(start_event)

        # see this page:
        # https://developers.deepgram.com/docs/understand-endpointing-interim-results#using-endpointing-speech_final
        # for more information about the different types of events
        elif data[&#34;type&#34;] == &#34;Results&#34;:
            metadata = data[&#34;metadata&#34;]
            request_id = metadata[&#34;request_id&#34;]
            is_final_transcript = data[&#34;is_final&#34;]
            is_endpoint = data[&#34;speech_final&#34;]
            self._request_id = request_id

            alts = live_transcription_to_speech_data(self._opts.language, data)
            # If, for some reason, we didn&#39;t get a SpeechStarted event but we got
            # a transcript with text, we should start speaking. It&#39;s rare but has
            # been observed.
            if len(alts) &gt; 0 and alts[0].text:
                if not self._speaking:
                    self._speaking = True
                    start_event = stt.SpeechEvent(type=stt.SpeechEventType.START_OF_SPEECH)
                    self._event_ch.send_nowait(start_event)

                if is_final_transcript:
                    final_event = stt.SpeechEvent(
                        type=stt.SpeechEventType.FINAL_TRANSCRIPT,
                        request_id=request_id,
                        alternatives=alts,
                    )
                    self._event_ch.send_nowait(final_event)
                else:
                    interim_event = stt.SpeechEvent(
                        type=stt.SpeechEventType.INTERIM_TRANSCRIPT,
                        request_id=request_id,
                        alternatives=alts,
                    )
                    self._event_ch.send_nowait(interim_event)

            # if we receive an endpoint, only end the speech if
            # we either had a SpeechStarted event or we have a seen
            # a non-empty transcript (deepgram doesn&#39;t have a SpeechEnded event)
            if is_endpoint and self._speaking:
                self._speaking = False
                self._event_ch.send_nowait(stt.SpeechEvent(type=stt.SpeechEventType.END_OF_SPEECH))

        elif data[&#34;type&#34;] == &#34;Metadata&#34;:
            pass  # metadata is too noisy
        else:
            logger.warning(&#34;received unexpected message from deepgram %s&#34;, data)</code></pre>
</details>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Args:
sample_rate : int or None, optional
The desired sample rate for the audio input.
If specified, the audio input will be automatically resampled to match
the given sample rate before being processed for Speech-to-Text.
If not provided (None), the input will retain its original sample rate.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="livekit.agents.stt.stt.RecognizeStream" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream">RecognizeStream</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="livekit.plugins.deepgram.SpeechStream.update_options"><code class="name flex">
<span>def <span class="ident">update_options</span></span>(<span>self,<br>*,<br>language: NotGivenOr[DeepgramLanguages | str] = NOT_GIVEN,<br>model: NotGivenOr[DeepgramModels | str] = NOT_GIVEN,<br>interim_results: NotGivenOr[bool] = NOT_GIVEN,<br>punctuate: NotGivenOr[bool] = NOT_GIVEN,<br>smart_format: NotGivenOr[bool] = NOT_GIVEN,<br>sample_rate: NotGivenOr[int] = NOT_GIVEN,<br>no_delay: NotGivenOr[bool] = NOT_GIVEN,<br>endpointing_ms: NotGivenOr[int] = NOT_GIVEN,<br>filler_words: NotGivenOr[bool] = NOT_GIVEN,<br>keywords: NotGivenOr[list[tuple[str, float]]] = NOT_GIVEN,<br>keyterms: NotGivenOr[list[str]] = NOT_GIVEN,<br>profanity_filter: NotGivenOr[bool] = NOT_GIVEN,<br>numerals: NotGivenOr[bool] = NOT_GIVEN,<br>mip_opt_out: NotGivenOr[bool] = NOT_GIVEN,<br>tags: NotGivenOr[list[str]] = NOT_GIVEN)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_options(
    self,
    *,
    language: NotGivenOr[DeepgramLanguages | str] = NOT_GIVEN,
    model: NotGivenOr[DeepgramModels | str] = NOT_GIVEN,
    interim_results: NotGivenOr[bool] = NOT_GIVEN,
    punctuate: NotGivenOr[bool] = NOT_GIVEN,
    smart_format: NotGivenOr[bool] = NOT_GIVEN,
    sample_rate: NotGivenOr[int] = NOT_GIVEN,
    no_delay: NotGivenOr[bool] = NOT_GIVEN,
    endpointing_ms: NotGivenOr[int] = NOT_GIVEN,
    filler_words: NotGivenOr[bool] = NOT_GIVEN,
    keywords: NotGivenOr[list[tuple[str, float]]] = NOT_GIVEN,
    keyterms: NotGivenOr[list[str]] = NOT_GIVEN,
    profanity_filter: NotGivenOr[bool] = NOT_GIVEN,
    numerals: NotGivenOr[bool] = NOT_GIVEN,
    mip_opt_out: NotGivenOr[bool] = NOT_GIVEN,
    tags: NotGivenOr[list[str]] = NOT_GIVEN,
):
    if is_given(language):
        self._opts.language = language
    if is_given(model):
        self._opts.model = _validate_model(model, language)
    if is_given(interim_results):
        self._opts.interim_results = interim_results
    if is_given(punctuate):
        self._opts.punctuate = punctuate
    if is_given(smart_format):
        self._opts.smart_format = smart_format
    if is_given(sample_rate):
        self._opts.sample_rate = sample_rate
    if is_given(no_delay):
        self._opts.no_delay = no_delay
    if is_given(endpointing_ms):
        self._opts.endpointing_ms = endpointing_ms
    if is_given(filler_words):
        self._opts.filler_words = filler_words
    if is_given(keywords):
        self._opts.keywords = keywords
    if is_given(keyterms):
        self._opts.keyterms = keyterms
    if is_given(profanity_filter):
        self._opts.profanity_filter = profanity_filter
    if is_given(numerals):
        self._opts.numerals = numerals
    if is_given(mip_opt_out):
        self._opts.mip_opt_out = mip_opt_out
    if is_given(tags):
        self._opts.tags = _validate_tags(tags)

    self._reconnect_event.set()</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="livekit.agents.stt.stt.RecognizeStream" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream">RecognizeStream</a></b></code>:
<ul class="hlist">
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.aclose" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream.aclose">aclose</a></code></li>
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.end_input" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream.end_input">end_input</a></code></li>
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.flush" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream.flush">flush</a></code></li>
<li><code><a title="livekit.agents.stt.stt.RecognizeStream.push_frame" href="../../agents/stt/stt.html#livekit.agents.stt.stt.RecognizeStream.push_frame">push_frame</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="livekit.plugins.deepgram.TTS"><code class="flex name class">
<span>class <span class="ident">TTS</span></span>
<span>(</span><span>*,<br>model: str = 'aura-asteria-en',<br>encoding: str = 'linear16',<br>sample_rate: int = 24000,<br>api_key: NotGivenOr[str] = NOT_GIVEN,<br>base_url: str = 'https://api.deepgram.com/v1/speak',<br>word_tokenizer: NotGivenOr[tokenize.WordTokenizer] = NOT_GIVEN,<br>http_session: aiohttp.ClientSession | None = None,<br>mip_opt_out: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TTS(tts.TTS):
    def __init__(
        self,
        *,
        model: str = &#34;aura-asteria-en&#34;,
        encoding: str = &#34;linear16&#34;,
        sample_rate: int = 24000,
        api_key: NotGivenOr[str] = NOT_GIVEN,
        base_url: str = BASE_URL,
        word_tokenizer: NotGivenOr[tokenize.WordTokenizer] = NOT_GIVEN,
        http_session: aiohttp.ClientSession | None = None,
        mip_opt_out: bool = False,
    ) -&gt; None:
        &#34;&#34;&#34;
        Create a new instance of Deepgram TTS.

        Args:
            model (str): TTS model to use. Defaults to &#34;aura-asteria-en&#34;.
            encoding (str): Audio encoding to use. Defaults to &#34;linear16&#34;.
            sample_rate (int): Sample rate of audio. Defaults to 24000.
            api_key (str): Deepgram API key. If not provided, will look for DEEPGRAM_API_KEY in environment.
            base_url (str): Base URL for Deepgram TTS API. Defaults to &#34;https://api.deepgram.com/v1/speak&#34;
            word_tokenizer (tokenize.WordTokenizer): Tokenizer for processing text. Defaults to basic WordTokenizer.
            http_session (aiohttp.ClientSession): Optional aiohttp session to use for requests.

        &#34;&#34;&#34;  # noqa: E501
        super().__init__(
            capabilities=tts.TTSCapabilities(streaming=True),
            sample_rate=sample_rate,
            num_channels=NUM_CHANNELS,
        )

        self._api_key = api_key if is_given(api_key) else os.environ.get(&#34;DEEPGRAM_API_KEY&#34;)
        if not self._api_key:
            raise ValueError(&#34;Deepgram API key required. Set DEEPGRAM_API_KEY or provide api_key.&#34;)

        if not is_given(word_tokenizer):
            word_tokenizer = tokenize.basic.WordTokenizer(ignore_punctuation=False)

        self._opts = _TTSOptions(
            model=model,
            encoding=encoding,
            sample_rate=sample_rate,
            word_tokenizer=word_tokenizer,
            mip_opt_out=mip_opt_out,
        )
        self._session = http_session
        self._base_url = base_url
        self._streams = weakref.WeakSet[SynthesizeStream]()
        self._pool = utils.ConnectionPool[aiohttp.ClientWebSocketResponse](
            connect_cb=self._connect_ws,
            close_cb=self._close_ws,
            max_session_duration=3600,  # 1 hour
            mark_refreshed_on_get=False,
        )

    async def _connect_ws(self) -&gt; aiohttp.ClientWebSocketResponse:
        session = self._ensure_session()
        config = {
            &#34;encoding&#34;: self._opts.encoding,
            &#34;model&#34;: self._opts.model,
            &#34;sample_rate&#34;: self._opts.sample_rate,
            &#34;mip_opt_out&#34;: self._opts.mip_opt_out,
        }
        return await asyncio.wait_for(
            session.ws_connect(
                _to_deepgram_url(config, self._base_url, websocket=True),
                headers={&#34;Authorization&#34;: f&#34;Token {self._api_key}&#34;},
            ),
            self._conn_options.timeout,
        )

    async def _close_ws(self, ws: aiohttp.ClientWebSocketResponse):
        await ws.close()

    def _ensure_session(self) -&gt; aiohttp.ClientSession:
        if not self._session:
            self._session = utils.http_context.http_session()
        return self._session

    def update_options(
        self,
        *,
        model: NotGivenOr[str] = NOT_GIVEN,
        sample_rate: NotGivenOr[int] = NOT_GIVEN,
    ) -&gt; None:
        &#34;&#34;&#34;
        args:
            model (str): TTS model to use.
            sample_rate (int): Sample rate of audio.
        &#34;&#34;&#34;
        if is_given(model):
            self._opts.model = model
        if is_given(sample_rate):
            self._opts.sample_rate = sample_rate
        for stream in self._streams:
            stream.update_options(
                model=model,
                sample_rate=sample_rate,
            )

    def synthesize(
        self,
        text: str,
        *,
        conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
    ) -&gt; ChunkedStream:
        return ChunkedStream(
            tts=self,
            input_text=text,
            base_url=self._base_url,
            api_key=self._api_key,
            conn_options=conn_options,
            opts=self._opts,
            session=self._ensure_session(),
        )

    def stream(
        self, *, conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS
    ) -&gt; SynthesizeStream:
        stream = SynthesizeStream(
            tts=self,
            conn_options=conn_options,
            base_url=self._base_url,
            api_key=self._api_key,
            opts=self._opts,
            session=self._ensure_session(),
        )
        self._streams.add(stream)
        return stream

    def prewarm(self) -&gt; None:
        self._pool.prewarm()

    async def aclose(self) -&gt; None:
        for stream in list(self._streams):
            await stream.aclose()
        self._streams.clear()
        await self._pool.aclose()
        await super().aclose()</code></pre>
</details>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Create a new instance of Deepgram TTS.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>str</code></dt>
<dd>TTS model to use. Defaults to "aura-asteria-en".</dd>
<dt><strong><code>encoding</code></strong> :&ensp;<code>str</code></dt>
<dd>Audio encoding to use. Defaults to "linear16".</dd>
<dt><strong><code>sample_rate</code></strong> :&ensp;<code>int</code></dt>
<dd>Sample rate of audio. Defaults to 24000.</dd>
<dt><strong><code>api_key</code></strong> :&ensp;<code>str</code></dt>
<dd>Deepgram API key. If not provided, will look for DEEPGRAM_API_KEY in environment.</dd>
<dt><strong><code>base_url</code></strong> :&ensp;<code>str</code></dt>
<dd>Base URL for Deepgram TTS API. Defaults to "https://api.deepgram.com/v1/speak"</dd>
<dt><strong><code>word_tokenizer</code></strong> :&ensp;<code>tokenize.WordTokenizer</code></dt>
<dd>Tokenizer for processing text. Defaults to basic WordTokenizer.</dd>
<dt><strong><code>http_session</code></strong> :&ensp;<code>aiohttp.ClientSession</code></dt>
<dd>Optional aiohttp session to use for requests.</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="livekit.agents.tts.tts.TTS" href="../../agents/tts/tts.html#livekit.agents.tts.tts.TTS">TTS</a></li>
<li>abc.ABC</li>
<li><a title="livekit.rtc.event_emitter.EventEmitter" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter">EventEmitter</a></li>
<li>typing.Generic</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="livekit.plugins.deepgram.TTS.aclose"><code class="name flex">
<span>async def <span class="ident">aclose</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def aclose(self) -&gt; None:
    for stream in list(self._streams):
        await stream.aclose()
    self._streams.clear()
    await self._pool.aclose()
    await super().aclose()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.deepgram.TTS.stream"><code class="name flex">
<span>def <span class="ident">stream</span></span>(<span>self,<br>*,<br>conn_options: APIConnectOptions = APIConnectOptions(max_retry=3, retry_interval=2.0, timeout=10.0)) ‑> livekit.plugins.deepgram.tts.SynthesizeStream</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stream(
    self, *, conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS
) -&gt; SynthesizeStream:
    stream = SynthesizeStream(
        tts=self,
        conn_options=conn_options,
        base_url=self._base_url,
        api_key=self._api_key,
        opts=self._opts,
        session=self._ensure_session(),
    )
    self._streams.add(stream)
    return stream</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.deepgram.TTS.synthesize"><code class="name flex">
<span>def <span class="ident">synthesize</span></span>(<span>self,<br>text: str,<br>*,<br>conn_options: APIConnectOptions = APIConnectOptions(max_retry=3, retry_interval=2.0, timeout=10.0)) ‑> livekit.plugins.deepgram.tts.ChunkedStream</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def synthesize(
    self,
    text: str,
    *,
    conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
) -&gt; ChunkedStream:
    return ChunkedStream(
        tts=self,
        input_text=text,
        base_url=self._base_url,
        api_key=self._api_key,
        conn_options=conn_options,
        opts=self._opts,
        session=self._ensure_session(),
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="livekit.plugins.deepgram.TTS.update_options"><code class="name flex">
<span>def <span class="ident">update_options</span></span>(<span>self,<br>*,<br>model: NotGivenOr[str] = NOT_GIVEN,<br>sample_rate: NotGivenOr[int] = NOT_GIVEN) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_options(
    self,
    *,
    model: NotGivenOr[str] = NOT_GIVEN,
    sample_rate: NotGivenOr[int] = NOT_GIVEN,
) -&gt; None:
    &#34;&#34;&#34;
    args:
        model (str): TTS model to use.
        sample_rate (int): Sample rate of audio.
    &#34;&#34;&#34;
    if is_given(model):
        self._opts.model = model
    if is_given(sample_rate):
        self._opts.sample_rate = sample_rate
    for stream in self._streams:
        stream.update_options(
            model=model,
            sample_rate=sample_rate,
        )</code></pre>
</details>
<div class="desc"><p>args:
model (str): TTS model to use.
sample_rate (int): Sample rate of audio.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="livekit.agents.tts.tts.TTS" href="../../agents/tts/tts.html#livekit.agents.tts.tts.TTS">TTS</a></b></code>:
<ul class="hlist">
<li><code><a title="livekit.agents.tts.tts.TTS.emit" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.emit">emit</a></code></li>
<li><code><a title="livekit.agents.tts.tts.TTS.off" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.off">off</a></code></li>
<li><code><a title="livekit.agents.tts.tts.TTS.on" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.on">on</a></code></li>
<li><code><a title="livekit.agents.tts.tts.TTS.once" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.once">once</a></code></li>
<li><code><a title="livekit.agents.tts.tts.TTS.prewarm" href="../../agents/tts/tts.html#livekit.agents.tts.tts.TTS.prewarm">prewarm</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="livekit.plugins" href="../index.html">livekit.plugins</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="livekit.plugins.deepgram.AudioEnergyFilter" href="#livekit.plugins.deepgram.AudioEnergyFilter">AudioEnergyFilter</a></code></h4>
<ul class="">
<li><code><a title="livekit.plugins.deepgram.AudioEnergyFilter.State" href="#livekit.plugins.deepgram.AudioEnergyFilter.State">State</a></code></li>
<li><code><a title="livekit.plugins.deepgram.AudioEnergyFilter.update" href="#livekit.plugins.deepgram.AudioEnergyFilter.update">update</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.plugins.deepgram.STT" href="#livekit.plugins.deepgram.STT">STT</a></code></h4>
<ul class="">
<li><code><a title="livekit.plugins.deepgram.STT.stream" href="#livekit.plugins.deepgram.STT.stream">stream</a></code></li>
<li><code><a title="livekit.plugins.deepgram.STT.update_options" href="#livekit.plugins.deepgram.STT.update_options">update_options</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.plugins.deepgram.SpeechStream" href="#livekit.plugins.deepgram.SpeechStream">SpeechStream</a></code></h4>
<ul class="">
<li><code><a title="livekit.plugins.deepgram.SpeechStream.update_options" href="#livekit.plugins.deepgram.SpeechStream.update_options">update_options</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.plugins.deepgram.TTS" href="#livekit.plugins.deepgram.TTS">TTS</a></code></h4>
<ul class="">
<li><code><a title="livekit.plugins.deepgram.TTS.aclose" href="#livekit.plugins.deepgram.TTS.aclose">aclose</a></code></li>
<li><code><a title="livekit.plugins.deepgram.TTS.stream" href="#livekit.plugins.deepgram.TTS.stream">stream</a></code></li>
<li><code><a title="livekit.plugins.deepgram.TTS.synthesize" href="#livekit.plugins.deepgram.TTS.synthesize">synthesize</a></code></li>
<li><code><a title="livekit.plugins.deepgram.TTS.update_options" href="#livekit.plugins.deepgram.TTS.update_options">update_options</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
