<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>livekit.plugins.silero API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>livekit.plugins.silero</code></h1>
</header>
<section id="section-intro">
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="livekit.plugins.silero.resources" href="resources/index.html">livekit.plugins.silero.resources</a></code></dt>
<dd>
<div class="desc"><p>Used by importlib.resources and setuptools</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="livekit.plugins.silero.VAD"><code class="flex name class">
<span>class <span class="ident">VAD</span></span>
<span>(</span><span>*, session: onnxruntime.InferenceSession, opts: _VADOptions)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VAD(agents.vad.VAD):
    &#34;&#34;&#34;
    Silero Voice Activity Detection (VAD) class.

    This class provides functionality to detect speech segments within audio data using the Silero VAD model.
    &#34;&#34;&#34;  # noqa: E501

    @classmethod
    def load(
        cls,
        *,
        min_speech_duration: float = 0.05,
        min_silence_duration: float = 0.55,
        prefix_padding_duration: float = 0.5,
        max_buffered_speech: float = 60.0,
        activation_threshold: float = 0.5,
        sample_rate: Literal[8000, 16000] = 16000,
        force_cpu: bool = True,
        # deprecated
        padding_duration: NotGivenOr[float] = NOT_GIVEN,
    ) -&gt; VAD:
        &#34;&#34;&#34;
        Load and initialize the Silero VAD model.

        This method loads the ONNX model and prepares it for inference. When options are not provided,
        sane defaults are used.

        **Note:**
            This method is blocking and may take time to load the model into memory.
            It is recommended to call this method inside your prewarm mechanism.

        **Example:**

            ```python
            def prewarm(proc: JobProcess):
                proc.userdata[&#34;vad&#34;] = silero.VAD.load()


            async def entrypoint(ctx: JobContext):
                vad = (ctx.proc.userdata[&#34;vad&#34;],)
                # your agent logic...


            if __name__ == &#34;__main__&#34;:
                cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint, prewarm_fnc=prewarm))
            ```

        Args:
            min_speech_duration (float): Minimum duration of speech to start a new speech chunk.
            min_silence_duration (float): At the end of each speech, wait this duration before ending the speech.
            prefix_padding_duration (float): Duration of padding to add to the beginning of each speech chunk.
            max_buffered_speech (float): Maximum duration of speech to keep in the buffer (in seconds).
            activation_threshold (float): Threshold to consider a frame as speech.
            sample_rate (Literal[8000, 16000]): Sample rate for the inference (only 8KHz and 16KHz are supported).
            force_cpu (bool): Force the use of CPU for inference.
            padding_duration (float | None): **Deprecated**. Use `prefix_padding_duration` instead.

        Returns:
            VAD: An instance of the VAD class ready for streaming.

        Raises:
            ValueError: If an unsupported sample rate is provided.
        &#34;&#34;&#34;  # noqa: E501
        if sample_rate not in onnx_model.SUPPORTED_SAMPLE_RATES:
            raise ValueError(&#34;Silero VAD only supports 8KHz and 16KHz sample rates&#34;)

        if is_given(padding_duration):
            logger.warning(
                &#34;padding_duration is deprecated and will be removed in 1.5.0, use prefix_padding_duration instead&#34;,  # noqa: E501
            )
            prefix_padding_duration = padding_duration

        session = onnx_model.new_inference_session(force_cpu)
        opts = _VADOptions(
            min_speech_duration=min_speech_duration,
            min_silence_duration=min_silence_duration,
            prefix_padding_duration=prefix_padding_duration,
            max_buffered_speech=max_buffered_speech,
            activation_threshold=activation_threshold,
            sample_rate=sample_rate,
        )
        return cls(session=session, opts=opts)

    def __init__(
        self,
        *,
        session: onnxruntime.InferenceSession,
        opts: _VADOptions,
    ) -&gt; None:
        super().__init__(capabilities=agents.vad.VADCapabilities(update_interval=0.032))
        self._onnx_session = session
        self._opts = opts
        self._streams = weakref.WeakSet[VADStream]()

    def stream(self) -&gt; VADStream:
        &#34;&#34;&#34;
        Create a new VADStream for processing audio data.

        Returns:
            VADStream: A stream object for processing audio input and detecting speech.
        &#34;&#34;&#34;
        stream = VADStream(
            self,
            self._opts,
            onnx_model.OnnxModel(
                onnx_session=self._onnx_session, sample_rate=self._opts.sample_rate
            ),
        )
        self._streams.add(stream)
        return stream

    def update_options(
        self,
        *,
        min_speech_duration: NotGivenOr[float] = NOT_GIVEN,
        min_silence_duration: NotGivenOr[float] = NOT_GIVEN,
        prefix_padding_duration: NotGivenOr[float] = NOT_GIVEN,
        max_buffered_speech: NotGivenOr[float] = NOT_GIVEN,
        activation_threshold: NotGivenOr[float] = NOT_GIVEN,
    ) -&gt; None:
        &#34;&#34;&#34;
        Update the VAD options.

        This method allows you to update the VAD options after the VAD object has been created.

        Args:
            min_speech_duration (float): Minimum duration of speech to start a new speech chunk.
            min_silence_duration (float): At the end of each speech, wait this duration before ending the speech.
            prefix_padding_duration (float): Duration of padding to add to the beginning of each speech chunk.
            max_buffered_speech (float): Maximum duration of speech to keep in the buffer (in seconds).
            activation_threshold (float): Threshold to consider a frame as speech.
        &#34;&#34;&#34;  # noqa: E501
        if is_given(min_speech_duration):
            self._opts.min_speech_duration = min_speech_duration
        if is_given(min_silence_duration):
            self._opts.min_silence_duration = min_silence_duration
        if is_given(prefix_padding_duration):
            self._opts.prefix_padding_duration = prefix_padding_duration
        if is_given(max_buffered_speech):
            self._opts.max_buffered_speech = max_buffered_speech
        if is_given(activation_threshold):
            self._opts.activation_threshold = activation_threshold

        for stream in self._streams:
            stream.update_options(
                min_speech_duration=min_speech_duration,
                min_silence_duration=min_silence_duration,
                prefix_padding_duration=prefix_padding_duration,
                max_buffered_speech=max_buffered_speech,
                activation_threshold=activation_threshold,
            )</code></pre>
</details>
<div class="desc"><p>Silero Voice Activity Detection (VAD) class.</p>
<p>This class provides functionality to detect speech segments within audio data using the Silero VAD model.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="livekit.agents.vad.VAD" href="../../agents/vad.html#livekit.agents.vad.VAD">VAD</a></li>
<li>abc.ABC</li>
<li><a title="livekit.rtc.event_emitter.EventEmitter" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter">EventEmitter</a></li>
<li>typing.Generic</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="livekit.plugins.silero.VAD.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>*,<br>min_speech_duration: float = 0.05,<br>min_silence_duration: float = 0.55,<br>prefix_padding_duration: float = 0.5,<br>max_buffered_speech: float = 60.0,<br>activation_threshold: float = 0.5,<br>sample_rate: Literal[8000, 16000] = 16000,<br>force_cpu: bool = True,<br>padding_duration: NotGivenOr[float] = NOT_GIVEN) ‑> livekit.plugins.silero.vad.VAD</span>
</code></dt>
<dd>
<div class="desc"><p>Load and initialize the Silero VAD model.</p>
<p>This method loads the ONNX model and prepares it for inference. When options are not provided,
sane defaults are used.</p>
<p><strong>Note:</strong>
This method is blocking and may take time to load the model into memory.
It is recommended to call this method inside your prewarm mechanism.</p>
<p><strong>Example:</strong></p>
<pre><code>```python
def prewarm(proc: JobProcess):
    proc.userdata["vad"] = silero.VAD.load()


async def entrypoint(ctx: JobContext):
    vad = (ctx.proc.userdata["vad"],)
    # your agent logic...


if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint, prewarm_fnc=prewarm))
```
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>min_speech_duration</code></strong> :&ensp;<code>float</code></dt>
<dd>Minimum duration of speech to start a new speech chunk.</dd>
<dt><strong><code>min_silence_duration</code></strong> :&ensp;<code>float</code></dt>
<dd>At the end of each speech, wait this duration before ending the speech.</dd>
<dt><strong><code>prefix_padding_duration</code></strong> :&ensp;<code>float</code></dt>
<dd>Duration of padding to add to the beginning of each speech chunk.</dd>
<dt><strong><code>max_buffered_speech</code></strong> :&ensp;<code>float</code></dt>
<dd>Maximum duration of speech to keep in the buffer (in seconds).</dd>
<dt><strong><code>activation_threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Threshold to consider a frame as speech.</dd>
<dt><strong><code>sample_rate</code></strong> :&ensp;<code>Literal[8000, 16000]</code></dt>
<dd>Sample rate for the inference (only 8KHz and 16KHz are supported).</dd>
<dt><strong><code>force_cpu</code></strong> :&ensp;<code>bool</code></dt>
<dd>Force the use of CPU for inference.</dd>
<dt><strong><code>padding_duration</code></strong> :&ensp;<code>float | None</code></dt>
<dd><strong>Deprecated</strong>. Use <code>prefix_padding_duration</code> instead.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="livekit.plugins.silero.VAD" href="#livekit.plugins.silero.VAD">VAD</a></code></dt>
<dd>An instance of the VAD class ready for streaming.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If an unsupported sample rate is provided.</dd>
</dl></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="livekit.plugins.silero.VAD.stream"><code class="name flex">
<span>def <span class="ident">stream</span></span>(<span>self) ‑> livekit.plugins.silero.vad.VADStream</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stream(self) -&gt; VADStream:
    &#34;&#34;&#34;
    Create a new VADStream for processing audio data.

    Returns:
        VADStream: A stream object for processing audio input and detecting speech.
    &#34;&#34;&#34;
    stream = VADStream(
        self,
        self._opts,
        onnx_model.OnnxModel(
            onnx_session=self._onnx_session, sample_rate=self._opts.sample_rate
        ),
    )
    self._streams.add(stream)
    return stream</code></pre>
</details>
<div class="desc"><p>Create a new VADStream for processing audio data.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="livekit.plugins.silero.VADStream" href="#livekit.plugins.silero.VADStream">VADStream</a></code></dt>
<dd>A stream object for processing audio input and detecting speech.</dd>
</dl></div>
</dd>
<dt id="livekit.plugins.silero.VAD.update_options"><code class="name flex">
<span>def <span class="ident">update_options</span></span>(<span>self,<br>*,<br>min_speech_duration: NotGivenOr[float] = NOT_GIVEN,<br>min_silence_duration: NotGivenOr[float] = NOT_GIVEN,<br>prefix_padding_duration: NotGivenOr[float] = NOT_GIVEN,<br>max_buffered_speech: NotGivenOr[float] = NOT_GIVEN,<br>activation_threshold: NotGivenOr[float] = NOT_GIVEN) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_options(
    self,
    *,
    min_speech_duration: NotGivenOr[float] = NOT_GIVEN,
    min_silence_duration: NotGivenOr[float] = NOT_GIVEN,
    prefix_padding_duration: NotGivenOr[float] = NOT_GIVEN,
    max_buffered_speech: NotGivenOr[float] = NOT_GIVEN,
    activation_threshold: NotGivenOr[float] = NOT_GIVEN,
) -&gt; None:
    &#34;&#34;&#34;
    Update the VAD options.

    This method allows you to update the VAD options after the VAD object has been created.

    Args:
        min_speech_duration (float): Minimum duration of speech to start a new speech chunk.
        min_silence_duration (float): At the end of each speech, wait this duration before ending the speech.
        prefix_padding_duration (float): Duration of padding to add to the beginning of each speech chunk.
        max_buffered_speech (float): Maximum duration of speech to keep in the buffer (in seconds).
        activation_threshold (float): Threshold to consider a frame as speech.
    &#34;&#34;&#34;  # noqa: E501
    if is_given(min_speech_duration):
        self._opts.min_speech_duration = min_speech_duration
    if is_given(min_silence_duration):
        self._opts.min_silence_duration = min_silence_duration
    if is_given(prefix_padding_duration):
        self._opts.prefix_padding_duration = prefix_padding_duration
    if is_given(max_buffered_speech):
        self._opts.max_buffered_speech = max_buffered_speech
    if is_given(activation_threshold):
        self._opts.activation_threshold = activation_threshold

    for stream in self._streams:
        stream.update_options(
            min_speech_duration=min_speech_duration,
            min_silence_duration=min_silence_duration,
            prefix_padding_duration=prefix_padding_duration,
            max_buffered_speech=max_buffered_speech,
            activation_threshold=activation_threshold,
        )</code></pre>
</details>
<div class="desc"><p>Update the VAD options.</p>
<p>This method allows you to update the VAD options after the VAD object has been created.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>min_speech_duration</code></strong> :&ensp;<code>float</code></dt>
<dd>Minimum duration of speech to start a new speech chunk.</dd>
<dt><strong><code>min_silence_duration</code></strong> :&ensp;<code>float</code></dt>
<dd>At the end of each speech, wait this duration before ending the speech.</dd>
<dt><strong><code>prefix_padding_duration</code></strong> :&ensp;<code>float</code></dt>
<dd>Duration of padding to add to the beginning of each speech chunk.</dd>
<dt><strong><code>max_buffered_speech</code></strong> :&ensp;<code>float</code></dt>
<dd>Maximum duration of speech to keep in the buffer (in seconds).</dd>
<dt><strong><code>activation_threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Threshold to consider a frame as speech.</dd>
</dl></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="livekit.agents.vad.VAD" href="../../agents/vad.html#livekit.agents.vad.VAD">VAD</a></b></code>:
<ul class="hlist">
<li><code><a title="livekit.agents.vad.VAD.emit" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.emit">emit</a></code></li>
<li><code><a title="livekit.agents.vad.VAD.off" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.off">off</a></code></li>
<li><code><a title="livekit.agents.vad.VAD.on" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.on">on</a></code></li>
<li><code><a title="livekit.agents.vad.VAD.once" href="../../rtc/event_emitter.html#livekit.rtc.event_emitter.EventEmitter.once">once</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="livekit.plugins.silero.VADStream"><code class="flex name class">
<span>class <span class="ident">VADStream</span></span>
<span>(</span><span>vad: <a title="livekit.plugins.silero.VAD" href="#livekit.plugins.silero.VAD">VAD</a>,<br>opts: _VADOptions,<br>model: onnx_model.OnnxModel)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VADStream(agents.vad.VADStream):
    def __init__(self, vad: VAD, opts: _VADOptions, model: onnx_model.OnnxModel) -&gt; None:
        super().__init__(vad)
        self._opts, self._model = opts, model
        self._loop = asyncio.get_event_loop()

        self._executor = ThreadPoolExecutor(max_workers=1)
        self._task.add_done_callback(lambda _: self._executor.shutdown(wait=False))
        self._exp_filter = utils.ExpFilter(alpha=0.35)

        self._input_sample_rate = 0
        self._speech_buffer: np.ndarray | None = None
        self._speech_buffer_max_reached = False
        self._prefix_padding_samples = 0  # (input_sample_rate)

    def update_options(
        self,
        *,
        min_speech_duration: NotGivenOr[float] = NOT_GIVEN,
        min_silence_duration: NotGivenOr[float] = NOT_GIVEN,
        prefix_padding_duration: NotGivenOr[float] = NOT_GIVEN,
        max_buffered_speech: NotGivenOr[float] = NOT_GIVEN,
        activation_threshold: NotGivenOr[float] = NOT_GIVEN,
    ) -&gt; None:
        &#34;&#34;&#34;
        Update the VAD options.

        This method allows you to update the VAD options after the VAD object has been created.

        Args:
            min_speech_duration (float): Minimum duration of speech to start a new speech chunk.
            min_silence_duration (float): At the end of each speech, wait this duration before ending the speech.
            prefix_padding_duration (float): Duration of padding to add to the beginning of each speech chunk.
            max_buffered_speech (float): Maximum duration of speech to keep in the buffer (in seconds).
            activation_threshold (float): Threshold to consider a frame as speech.
        &#34;&#34;&#34;  # noqa: E501
        old_max_buffered_speech = self._opts.max_buffered_speech

        if is_given(min_speech_duration):
            self._opts.min_speech_duration = min_speech_duration
        if is_given(min_silence_duration):
            self._opts.min_silence_duration = min_silence_duration
        if is_given(prefix_padding_duration):
            self._opts.prefix_padding_duration = prefix_padding_duration
        if is_given(max_buffered_speech):
            self._opts.max_buffered_speech = max_buffered_speech
        if is_given(activation_threshold):
            self._opts.activation_threshold = activation_threshold

        if self._input_sample_rate:
            assert self._speech_buffer is not None

            self._prefix_padding_samples = int(
                self._opts.prefix_padding_duration * self._input_sample_rate
            )

            self._speech_buffer.resize(
                int(self._opts.max_buffered_speech * self._input_sample_rate)
                + self._prefix_padding_samples
            )

            if self._opts.max_buffered_speech &gt; old_max_buffered_speech:
                self._speech_buffer_max_reached = False

    @agents.utils.log_exceptions(logger=logger)
    async def _main_task(self):
        inference_f32_data = np.empty(self._model.window_size_samples, dtype=np.float32)
        speech_buffer_index: int = 0

        # &#34;pub_&#34; means public, these values are exposed to the users through events
        pub_speaking = False
        pub_speech_duration = 0.0
        pub_silence_duration = 0.0
        pub_current_sample = 0
        pub_timestamp = 0.0

        speech_threshold_duration = 0.0
        silence_threshold_duration = 0.0

        input_frames = []
        inference_frames = []
        resampler: rtc.AudioResampler | None = None

        # used to avoid drift when the sample_rate ratio is not an integer
        input_copy_remaining_fract = 0.0

        extra_inference_time = 0.0

        async for input_frame in self._input_ch:
            if not isinstance(input_frame, rtc.AudioFrame):
                continue  # ignore flush sentinel for now

            if not self._input_sample_rate:
                self._input_sample_rate = input_frame.sample_rate

                # alloc the buffers now that we know the input sample rate
                self._prefix_padding_samples = int(
                    self._opts.prefix_padding_duration * self._input_sample_rate
                )

                self._speech_buffer = np.empty(
                    int(self._opts.max_buffered_speech * self._input_sample_rate)
                    + self._prefix_padding_samples,
                    dtype=np.int16,
                )

                if self._input_sample_rate != self._opts.sample_rate:
                    # resampling needed: the input sample rate isn&#39;t the same as the model&#39;s
                    # sample rate used for inference
                    resampler = rtc.AudioResampler(
                        input_rate=self._input_sample_rate,
                        output_rate=self._opts.sample_rate,
                        quality=rtc.AudioResamplerQuality.QUICK,  # VAD doesn&#39;t need high quality
                    )

            elif self._input_sample_rate != input_frame.sample_rate:
                logger.error(&#34;a frame with another sample rate was already pushed&#34;)
                continue

            assert self._speech_buffer is not None

            input_frames.append(input_frame)
            if resampler is not None:
                # the resampler may have a bit of latency, but it is OK to ignore since it should be
                # negligible
                inference_frames.extend(resampler.push(input_frame))
            else:
                inference_frames.append(input_frame)

            while True:
                start_time = time.perf_counter()

                available_inference_samples = sum(
                    [frame.samples_per_channel for frame in inference_frames]
                )
                if available_inference_samples &lt; self._model.window_size_samples:
                    break  # not enough samples to run inference

                input_frame = utils.combine_frames(input_frames)
                inference_frame = utils.combine_frames(inference_frames)

                # convert data to f32
                np.divide(
                    inference_frame.data[: self._model.window_size_samples],
                    np.iinfo(np.int16).max,
                    out=inference_f32_data,
                    dtype=np.float32,
                )

                # run the inference
                p = await self._loop.run_in_executor(
                    self._executor, self._model, inference_f32_data
                )
                p = self._exp_filter.apply(exp=1.0, sample=p)

                window_duration = self._model.window_size_samples / self._opts.sample_rate

                pub_current_sample += self._model.window_size_samples
                pub_timestamp += window_duration

                resampling_ratio = self._input_sample_rate / self._model.sample_rate
                to_copy = (
                    self._model.window_size_samples * resampling_ratio + input_copy_remaining_fract
                )
                to_copy_int = int(to_copy)
                input_copy_remaining_fract = to_copy - to_copy_int

                # copy the inference window to the speech buffer
                available_space = len(self._speech_buffer) - speech_buffer_index
                to_copy_buffer = min(to_copy_int, available_space)
                if to_copy_buffer &gt; 0:
                    self._speech_buffer[
                        speech_buffer_index : speech_buffer_index + to_copy_buffer
                    ] = input_frame.data[:to_copy_buffer]
                    speech_buffer_index += to_copy_buffer
                elif not self._speech_buffer_max_reached:
                    # reached self._opts.max_buffered_speech (padding is included)
                    speech_buffer_max_reached = True
                    logger.warning(
                        &#34;max_buffered_speech reached, ignoring further data for the current speech input&#34;  # noqa: E501
                    )

                inference_duration = time.perf_counter() - start_time
                extra_inference_time = max(
                    0.0,
                    extra_inference_time + inference_duration - window_duration,
                )
                if inference_duration &gt; SLOW_INFERENCE_THRESHOLD:
                    logger.warning(
                        &#34;inference is slower than realtime&#34;,
                        extra={&#34;delay&#34;: extra_inference_time},
                    )

                def _reset_write_cursor():
                    nonlocal speech_buffer_index, speech_buffer_max_reached
                    assert self._speech_buffer is not None

                    if speech_buffer_index &lt;= self._prefix_padding_samples:
                        return

                    padding_data = self._speech_buffer[
                        speech_buffer_index - self._prefix_padding_samples : speech_buffer_index
                    ]

                    self._speech_buffer_max_reached = False
                    self._speech_buffer[: self._prefix_padding_samples] = padding_data
                    speech_buffer_index = self._prefix_padding_samples

                def _copy_speech_buffer() -&gt; rtc.AudioFrame:
                    # copy the data from speech_buffer
                    assert self._speech_buffer is not None
                    speech_data = self._speech_buffer[:speech_buffer_index].tobytes()  # noqa: B023

                    return rtc.AudioFrame(
                        sample_rate=self._input_sample_rate,
                        num_channels=1,
                        samples_per_channel=speech_buffer_index,  # noqa: B023
                        data=speech_data,
                    )

                if pub_speaking:
                    pub_speech_duration += window_duration
                else:
                    pub_silence_duration += window_duration

                self._event_ch.send_nowait(
                    agents.vad.VADEvent(
                        type=agents.vad.VADEventType.INFERENCE_DONE,
                        samples_index=pub_current_sample,
                        timestamp=pub_timestamp,
                        silence_duration=pub_silence_duration,
                        speech_duration=pub_speech_duration,
                        probability=p,
                        inference_duration=inference_duration,
                        frames=[
                            rtc.AudioFrame(
                                data=input_frame.data[:to_copy_int].tobytes(),
                                sample_rate=self._input_sample_rate,
                                num_channels=1,
                                samples_per_channel=to_copy_int,
                            )
                        ],
                        speaking=pub_speaking,
                        raw_accumulated_silence=silence_threshold_duration,
                        raw_accumulated_speech=speech_threshold_duration,
                    )
                )

                if p &gt;= self._opts.activation_threshold:
                    speech_threshold_duration += window_duration
                    silence_threshold_duration = 0.0

                    if not pub_speaking:
                        if speech_threshold_duration &gt;= self._opts.min_speech_duration:
                            pub_speaking = True
                            pub_silence_duration = 0.0
                            pub_speech_duration = speech_threshold_duration

                            self._event_ch.send_nowait(
                                agents.vad.VADEvent(
                                    type=agents.vad.VADEventType.START_OF_SPEECH,
                                    samples_index=pub_current_sample,
                                    timestamp=pub_timestamp,
                                    silence_duration=pub_silence_duration,
                                    speech_duration=pub_speech_duration,
                                    frames=[_copy_speech_buffer()],
                                    speaking=True,
                                )
                            )

                else:
                    silence_threshold_duration += window_duration
                    speech_threshold_duration = 0.0

                    if not pub_speaking:
                        _reset_write_cursor()

                    if (
                        pub_speaking
                        and silence_threshold_duration &gt;= self._opts.min_silence_duration
                    ):
                        pub_speaking = False
                        pub_speech_duration = 0.0
                        pub_silence_duration = silence_threshold_duration

                        self._event_ch.send_nowait(
                            agents.vad.VADEvent(
                                type=agents.vad.VADEventType.END_OF_SPEECH,
                                samples_index=pub_current_sample,
                                timestamp=pub_timestamp,
                                silence_duration=pub_silence_duration,
                                speech_duration=pub_speech_duration,
                                frames=[_copy_speech_buffer()],
                                speaking=False,
                            )
                        )

                        _reset_write_cursor()

                # remove the frames that were used for inference from the input and inference frames
                input_frames = []
                inference_frames = []

                # add the remaining data
                if len(input_frame.data) - to_copy_int &gt; 0:
                    data = input_frame.data[to_copy_int:].tobytes()
                    input_frames.append(
                        rtc.AudioFrame(
                            data=data,
                            sample_rate=self._input_sample_rate,
                            num_channels=1,
                            samples_per_channel=len(data) // 2,
                        )
                    )

                if len(inference_frame.data) - self._model.window_size_samples &gt; 0:
                    data = inference_frame.data[self._model.window_size_samples :].tobytes()
                    inference_frames.append(
                        rtc.AudioFrame(
                            data=data,
                            sample_rate=self._opts.sample_rate,
                            num_channels=1,
                            samples_per_channel=len(data) // 2,
                        )
                    )</code></pre>
</details>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="livekit.agents.vad.VADStream" href="../../agents/vad.html#livekit.agents.vad.VADStream">VADStream</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="livekit.plugins.silero.VADStream.update_options"><code class="name flex">
<span>def <span class="ident">update_options</span></span>(<span>self,<br>*,<br>min_speech_duration: NotGivenOr[float] = NOT_GIVEN,<br>min_silence_duration: NotGivenOr[float] = NOT_GIVEN,<br>prefix_padding_duration: NotGivenOr[float] = NOT_GIVEN,<br>max_buffered_speech: NotGivenOr[float] = NOT_GIVEN,<br>activation_threshold: NotGivenOr[float] = NOT_GIVEN) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_options(
    self,
    *,
    min_speech_duration: NotGivenOr[float] = NOT_GIVEN,
    min_silence_duration: NotGivenOr[float] = NOT_GIVEN,
    prefix_padding_duration: NotGivenOr[float] = NOT_GIVEN,
    max_buffered_speech: NotGivenOr[float] = NOT_GIVEN,
    activation_threshold: NotGivenOr[float] = NOT_GIVEN,
) -&gt; None:
    &#34;&#34;&#34;
    Update the VAD options.

    This method allows you to update the VAD options after the VAD object has been created.

    Args:
        min_speech_duration (float): Minimum duration of speech to start a new speech chunk.
        min_silence_duration (float): At the end of each speech, wait this duration before ending the speech.
        prefix_padding_duration (float): Duration of padding to add to the beginning of each speech chunk.
        max_buffered_speech (float): Maximum duration of speech to keep in the buffer (in seconds).
        activation_threshold (float): Threshold to consider a frame as speech.
    &#34;&#34;&#34;  # noqa: E501
    old_max_buffered_speech = self._opts.max_buffered_speech

    if is_given(min_speech_duration):
        self._opts.min_speech_duration = min_speech_duration
    if is_given(min_silence_duration):
        self._opts.min_silence_duration = min_silence_duration
    if is_given(prefix_padding_duration):
        self._opts.prefix_padding_duration = prefix_padding_duration
    if is_given(max_buffered_speech):
        self._opts.max_buffered_speech = max_buffered_speech
    if is_given(activation_threshold):
        self._opts.activation_threshold = activation_threshold

    if self._input_sample_rate:
        assert self._speech_buffer is not None

        self._prefix_padding_samples = int(
            self._opts.prefix_padding_duration * self._input_sample_rate
        )

        self._speech_buffer.resize(
            int(self._opts.max_buffered_speech * self._input_sample_rate)
            + self._prefix_padding_samples
        )

        if self._opts.max_buffered_speech &gt; old_max_buffered_speech:
            self._speech_buffer_max_reached = False</code></pre>
</details>
<div class="desc"><p>Update the VAD options.</p>
<p>This method allows you to update the VAD options after the VAD object has been created.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>min_speech_duration</code></strong> :&ensp;<code>float</code></dt>
<dd>Minimum duration of speech to start a new speech chunk.</dd>
<dt><strong><code>min_silence_duration</code></strong> :&ensp;<code>float</code></dt>
<dd>At the end of each speech, wait this duration before ending the speech.</dd>
<dt><strong><code>prefix_padding_duration</code></strong> :&ensp;<code>float</code></dt>
<dd>Duration of padding to add to the beginning of each speech chunk.</dd>
<dt><strong><code>max_buffered_speech</code></strong> :&ensp;<code>float</code></dt>
<dd>Maximum duration of speech to keep in the buffer (in seconds).</dd>
<dt><strong><code>activation_threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Threshold to consider a frame as speech.</dd>
</dl></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="livekit.agents.vad.VADStream" href="../../agents/vad.html#livekit.agents.vad.VADStream">VADStream</a></b></code>:
<ul class="hlist">
<li><code><a title="livekit.agents.vad.VADStream.aclose" href="../../agents/vad.html#livekit.agents.vad.VADStream.aclose">aclose</a></code></li>
<li><code><a title="livekit.agents.vad.VADStream.end_input" href="../../agents/vad.html#livekit.agents.vad.VADStream.end_input">end_input</a></code></li>
<li><code><a title="livekit.agents.vad.VADStream.flush" href="../../agents/vad.html#livekit.agents.vad.VADStream.flush">flush</a></code></li>
<li><code><a title="livekit.agents.vad.VADStream.push_frame" href="../../agents/vad.html#livekit.agents.vad.VADStream.push_frame">push_frame</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="livekit.plugins" href="../index.html">livekit.plugins</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="livekit.plugins.silero.resources" href="resources/index.html">livekit.plugins.silero.resources</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="livekit.plugins.silero.VAD" href="#livekit.plugins.silero.VAD">VAD</a></code></h4>
<ul class="">
<li><code><a title="livekit.plugins.silero.VAD.load" href="#livekit.plugins.silero.VAD.load">load</a></code></li>
<li><code><a title="livekit.plugins.silero.VAD.stream" href="#livekit.plugins.silero.VAD.stream">stream</a></code></li>
<li><code><a title="livekit.plugins.silero.VAD.update_options" href="#livekit.plugins.silero.VAD.update_options">update_options</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="livekit.plugins.silero.VADStream" href="#livekit.plugins.silero.VADStream">VADStream</a></code></h4>
<ul class="">
<li><code><a title="livekit.plugins.silero.VADStream.update_options" href="#livekit.plugins.silero.VADStream.update_options">update_options</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
